[{"id":0,"href":"/en/geekdoc/usage/getting-started/","title":"Getting Started","parent":"Usage","content":"This page tells you how to get started with the Geekdoc theme, including installation and basic configuration.\nInstall requirements Using the theme Option 1: Download pre-build release bundle Option 2: Clone the GitHub repository Deployments Netlify Use a Makefile Chain required commands Subdirectories Known Limitations Minify HTML results in spacing issues Install requirements You need a recent version of Hugo for local builds and previews of sites that use Geekdoc. As we are using webpack as pre-processor, the normal version of Hugo is sufficient. If you prefer the extended version of Hugo anyway this will work as well. For comprehensive Hugo documentation, see gohugo.io.\nIf you want to use the theme from a cloned branch instead of a release tarball you\u0026rsquo;ll need to install webpack locally and run the build script once to create all required assets.\n# install required packages from package.json npm install # run the build script to build required assets npm run build # build release tarball npm run pack Using the theme To prepare your new site environment just a few steps are required:\nCreate a new empty Hugo site.\nhugo new site demosite Switch to the root of the new site.\ncd demosite Install the Geekdoc theme from a release bundle (recommended) or from Git branch.\nCreate the minimal required Hugo configuration config.toml. For all configuration options take a look at the configuration page.\nbaseURL = \u0026#34;http://localhost\u0026#34; title = \u0026#34;Geekdocs\u0026#34; theme = \u0026#34;hugo-geekdoc\u0026#34; pluralizeListTitles = false # Geekdoc required configuration pygmentsUseClasses = true pygmentsCodeFences = true disablePathToLower = true # Required if you want to render robots.txt template enableRobotsTXT = true # Needed for mermaid shortcodes [markup] [markup.goldmark.renderer] # Needed for mermaid shortcode or when nesting shortcodes (e.g. img within # columns or tabs) unsafe = true [markup.tableOfContents] startLevel = 1 endLevel = 9 [taxonomies] tag = \u0026#34;tags\u0026#34; Test your site.\nhugo server -D The -D or --buildDrafts option is used to include content marked as draft during the build. It is used because content pages created with the hugo new content command have the draft flag set by default and this can lead to build errors in newly created projects. For projects with a production-ready content structure, this flag is not required in most cases and can be omitted.\nOption 1: Download pre-build release bundle Download and extract the latest release bundle into the theme directory.\nmkdir -p themes/hugo-geekdoc/ curl -L https://github.com/thegeeklab/hugo-geekdoc/releases/latest/download/hugo-geekdoc.tar.gz | tar -xz -C themes/hugo-geekdoc/ --strip-components=1 Option 2: Clone the GitHub repository Keep in mind this method is not recommended and needs some extra steps to get it working. If you want to use the Theme as submodule keep in mind that your build process need to run the described steps as well. Clone the Geekdoc git repository.\ngit clone https://github.com/thegeeklab/hugo-geekdoc.git themes/hugo-geekdoc Build required theme assets e.g. CSS files and SVG sprites.\nnpm install npm run build Deployments Netlify There are several ways to deploy your site with this theme on Netlify. Regardless of which solution you choose, the main goal is to ensure that the prebuilt theme release tarball is used or to run the required commands to prepare the theme assets before running the Hugo build command.\nHere are some possible solutions:\nUse a Makefile Add a Makefile to your repository to bundle the required steps.\nThe Makefile is only an example. Depending on your project structure, BASEDIR or THEMEDIR may need to be adapted. # Please change the theme version to the latest release version. THEME_VERSION := v0.44.1 THEME := hugo-geekdoc BASEDIR := docs THEMEDIR := $(BASEDIR)/themes .PHONY: doc doc: doc-assets doc-build .PHONY: doc-assets doc-assets: mkdir -p $(THEMEDIR)/$(THEME)/ ; \\ curl -sSL \u0026#34;https://github.com/thegeeklab/$(THEME)/releases/download/${THEME_VERSION}/$(THEME).tar.gz\u0026#34; | tar -xz -C $(THEMEDIR)/$(THEME)/ --strip-components=1 .PHONY: doc-build doc-build: cd $(BASEDIR); hugo .PHONY: clean clean: rm -rf $(THEMEDIR) \u0026amp;\u0026amp; \\ rm -rf $(BASEDIR)/public This Makefile can be used in your netlify.toml, take a look at the Netlify example for more information:\n[build] publish = \u0026#34;docs/public\u0026#34; command = \u0026#34;make doc\u0026#34; Chain required commands Chain all required commands to prepare the theme and build your site on the command option in your netlify.toml like this:\n[build] publish = \u0026#34;docs/public\u0026#34; command = \u0026#34;command1 \u0026amp;\u0026amp; command 2 \u0026amp;\u0026amp; command3 \u0026amp;\u0026amp; hugo\u0026#34; Subdirectories As deploying Hugo sites on subdirectories is not as robust as on subdomains, we do not recommend this. If you have a choice, using a domain/subdomain should always be the preferred solution! If you want to deploy your site to a subdirectory of your domain, some extra steps are required:\nConfigure your Hugo base URL e.g. baseURL = http://localhost/demo/. Don\u0026rsquo;t use relativeURLs: false nor canonifyURLs: true as is can cause unwanted side effects! There are two ways to get Markdown links or images working:\nUse the absolute path including your subdirectory e.g. [testlink](/demo/example-site) Overwrite the HTML base in your site configuration with geekdocOverwriteHTMLBase = true and use the relative path e.g. [testlink](example-site) But there is another special case if you use geekdocOverwriteHTMLBase = true. If you use anchors in your Markdown links you have to ensure to always include the page path. As an example [testlink](#some-anchor) will resolve to http://localhost/demo/#some-anchor and not automatically include the current page!\nKnown Limitations Minify HTML results in spacing issues Using hugo --minify without further configuration or using other minify tools that also minify HTML files might result in spacing issues in the theme and is not supported.\nAfter some testing we decided to not spend effort to fix this issue for now as the benefit is very low. There are some parts of the theme where spaces between HTML elements matters but were stripped by minify tools. Some of these issues are related to gohugoio/hugo#6892. While recommendation like \u0026ldquo;don\u0026rsquo;t depend on whitespace in your layout\u0026rdquo; sounds reasonable, it seems to be not that straight forward especially for something like embedded icons into the text flow.\nIf you still want to use Hugo\u0026rsquo;s minify flag you should at least exclude HTML files in your site configuration as described in the Hugo documentation:\n[minify] disableHTML = true ","description":"This page tells you how to get started with the Geekdoc theme, including installation and basic configuration.\n"},{"id":1,"href":"/en/geekdoc/usage/","title":"Usage","parent":"geekdoc","content":"","description":""},{"id":2,"href":"/en/geekdoc/features/","title":"Features","parent":"geekdoc","content":"","description":""},{"id":3,"href":"/en/geekdoc/usage/configuration/","title":"Configuration","parent":"Usage","content":" Site configuration Page configuration Site configuration TOML baseURL = \u0026#34;http://localhost\u0026#34; title = \u0026#34;Geekdocs\u0026#34; theme = \u0026#34;hugo-geekdoc\u0026#34; # Required to get well formatted code blocks pygmentsUseClasses = true pygmentsCodeFences = true disablePathToLower = true enableGitInfo = true # Required if you want to render robots.txt template enableRobotsTXT = true [markup] [markup.goldmark.renderer] # Needed for mermaid shortcode or when nesting shortcodes (e.g. img within # columns or tabs) unsafe = true [markup.tableOfContents] startLevel = 1 endLevel = 9 [taxonomies] tag = \u0026#34;tags\u0026#34; [params] # (Optional, default 6) Set how many table of contents levels to be showed on page. # Use false to hide ToC, note that 0 will default to 6 (https://gohugo.io/functions/default/) # You can also specify this parameter per page in front matter. geekdocToC = 3 # (Optional, default static/brand.svg) Set the path to a logo for the Geekdoc # relative to your \u0026#39;static/\u0026#39; folder. geekdocLogo = \u0026#34;logo.png\u0026#34; # (Optional, default false) Render menu from data file in \u0026#39;data/menu/main.yaml\u0026#39;. # See also https://geekdocs.de/usage/menus/#bundle-menu. geekdocMenuBundle = true # (Optional, default false) Collapse all menu entries, can not be overwritten # per page if enabled. Can be enabled per page via \u0026#39;geekdocCollapseSection\u0026#39;. geekdocCollapseAllSections = true # (Optional, default true) Show page navigation links at the bottom of each docs page. geekdocNextPrev = false # (Optional, default true) Show a breadcrumb navigation bar at the top of each docs page. # You can also specify this parameter per page in front matter. geekdocBreadcrumb = false # (Optional, default none) Set source repository location. Used for \u0026#39;Edit page\u0026#39; links. # You can also specify this parameter per page in front matter. geekdocRepo = \u0026#34;https://github.com/thegeeklab/hugo\u0026#34; # (Optional, default none) Enable \u0026#39;Edit page\u0026#39; links. Requires \u0026#39;geekdocRepo\u0026#39; param # and the path must point to the parent directory of the \u0026#39;content\u0026#39; folder. # You can also specify this parameter per page in front matter. geekdocEditPath = \u0026#34;edit/main/exampleSite\u0026#34; # (Optional, default true) Enables search function with flexsearch. # Index is built on the fly and might slow down your website. geekdocSearch = false # (Optional, default false) Display search results with the parent folder as prefix. This # option allows you to distinguish between files with the same name in different folders. # NOTE: This parameter only applies when \u0026#39;geekdocSearch = true\u0026#39;. geekdocSearchShowParent = true # (Optional, default none) Add a link to your Legal Notice page to the site footer. # It can be either a remote url or a local file path relative to your content directory. geekdocLegalNotice = \u0026#34;https://blog.example.com/legal\u0026#34; # (Optional, default none) Add a link to your Privacy Policy page to the site footer. # It can be either a remote url or a local file path relative to your content directory. geekdocPrivacyPolicy = \u0026#34;/privacy\u0026#34; # (Optional, default true) Add an anchor link to headlines. geekdocAnchor = true # (Optional, default true) Copy anchor url to clipboard on click. geekdocAnchorCopy = true # (Optional, default true) Enable or disable image lazy loading for images rendered # by the \u0026#39;img\u0026#39; shortcode. geekdocImageLazyLoading = true # (Optional, default false) Set HTMl \u0026lt;base\u0026gt; to .Site.Home.Permalink if enabled. It might be required # if a subdirectory is used within Hugos BaseURL. # See https://developer.mozilla.org/de/docs/Web/HTML/Element/base. geekdocOverwriteHTMLBase = false # (Optional, default true) Enable or disable the JavaScript based color theme toggle switch. The CSS based # user preference mode still works. geekdocDarkModeToggle = false # (Optional, default false) Auto-decrease brightness of images and add a slightly grayscale to avoid # bright spots while using the dark mode. geekdocDarkModeDim = false # (Optional, default false) Enforce code blocks to always use the dark color theme. geekdocDarkModeCode = false # (Optional, default true) Display a \u0026#34;Back to top\u0026#34; link in the site footer. geekdocBackToTop = true # (Optional, default false) Enable or disable adding tags for post pages automatically to the navigation sidebar. geekdocTagsToMenu = true # (Optional, default \u0026#39;title\u0026#39;) Configure how to sort file-tree menu entries. Possible options are \u0026#39;title\u0026#39;, \u0026#39;linktitle\u0026#39;, # \u0026#39;date\u0026#39;, \u0026#39;publishdate\u0026#39;, \u0026#39;expirydate\u0026#39; or \u0026#39;lastmod\u0026#39;. Every option can be used with a reverse modifier as well # e.g. \u0026#39;title_reverse\u0026#39;. geekdocFileTreeSortBy = \u0026#34;title\u0026#34; # (Optional, default none) Adds a \u0026#34;Content licensed under \u0026lt;license\u0026gt;\u0026#34; line to the footer. # Could be used if you want to define a default license for your content. [params.geekdocContentLicense] name = \u0026#34;CC BY-SA 4.0\u0026#34; link = \u0026#34;https://creativecommons.org/licenses/by-sa/4.0/\u0026#34; YAML --- baseURL: \u0026#34;http://localhost\u0026#34; title: \u0026#34;Geekdocs\u0026#34; theme: \u0026#34;hugo-geekdoc\u0026#34; # Required to get well formatted code blocks pygmentsUseClasses: true pygmentsCodeFences: true disablePathToLower: true enableGitInfo: true # Required if you want to render robots.txt template enableRobotsTXT: true markup: goldmark: # Needed for mermaid shortcode or when nesting shortcodes (e.g. img within # columns or tabs) renderer: unsafe: true tableOfContents: startLevel: 1 endLevel: 9 taxonomies: tag: tags params: # (Optional, default 6) Set how many table of contents levels to be showed on page. # Use false to hide ToC, note that 0 will default to 6 (https://gohugo.io/functions/default/) # You can also specify this parameter per page in front matter. geekdocToC: 3 # (Optional, default static/brand.svg) Set the path to a logo for the Geekdoc # relative to your \u0026#39;static/\u0026#39; folder. geekdocLogo: logo.png # (Optional, default false) Render menu from data file in \u0026#39;data/menu/main.yaml\u0026#39;. # See also https://geekdocs.de/usage/menus/#bundle-menu. geekdocMenuBundle: true # (Optional, default false) Collapse all menu entries, can not be overwritten # per page if enabled. Can be enabled per page via \u0026#39;geekdocCollapseSection\u0026#39;. geekdocCollapseAllSections: true # (Optional, default true) Show page navigation links at the bottom of each docs page. geekdocNextPrev: false # (Optional, default true) Show a breadcrumb navigation bar at the top of each docs page. # You can also specify this parameter per page in front matter. geekdocBreadcrumb: false # (Optional, default none) Set source repository location. Used for \u0026#39;Edit page\u0026#39; links. # You can also specify this parameter per page in front matter. geekdocRepo: \u0026#34;https://github.com/thegeeklab/hugo-geekdoc\u0026#34; # (Optional, default none) Enable \u0026#39;Edit page\u0026#39; links. Requires \u0026#39;geekdocRepo\u0026#39; param # and the path must point to the parent directory of the \u0026#39;content\u0026#39; folder. # You can also specify this parameter per page in front matter. geekdocEditPath: edit/main/exampleSite # (Optional, default true) Enables search function with flexsearch. # Index is built on the fly and might slow down your website. geekdocSearch: false # (Optional, default false) Display search results with the parent folder as prefix. This # option allows you to distinguish between files with the same name in different folders. # NOTE: This parameter only applies when \u0026#39;geekdocSearch: true\u0026#39;. geekdocSearchShowParent: true # (Optional, default none) Add a link to your Legal Notice page to the site footer. # It can be either a remote url or a local file path relative to your content directory. geekdocLegalNotice: \u0026#34;https://blog.example.com/legal\u0026#34; # (Optional, default none) Add a link to your Privacy Policy page to the site footer. # It can be either a remote url or a local file path relative to your content directory. geekdocPrivacyPolicy: \u0026#34;/privacy\u0026#34; # (Optional, default true) Add an anchor link to headlines. geekdocAnchor: true # (Optional, default true) Copy anchor url to clipboard on click. geekdocAnchorCopy: true # (Optional, default true) Enable or disable image lazy loading for images rendered # by the \u0026#39;img\u0026#39; shortcode. geekdocImageLazyLoading: true # (Optional, default false) Set HTMl \u0026lt;base\u0026gt; to .Site.Home.Permalink if enabled. It might be required # if a subdirectory is used within Hugos BaseURL. # See https://developer.mozilla.org/de/docs/Web/HTML/Element/base. geekdocOverwriteHTMLBase: false # (Optional, default true) Enable or disable the JavaScript based color theme toggle switch. The CSS based # user preference mode still works. geekdocDarkModeToggle: false # (Optional, default false) Auto-decrease brightness of images and add a slightly grayscale to avoid # bright spots while using the dark mode. geekdocDarkModeDim: false # (Optional, default false) Enforce code blocks to always use the dark color theme. geekdocDarkModeCode: false # (Optional, default true) Display a \u0026#34;Back to top\u0026#34; link in the site footer. geekdocBackToTop: true # (Optional, default false) Enable or disable adding tags for post pages automatically to the navigation sidebar. geekdocTagsToMenu: true # (Optional, default \u0026#39;title\u0026#39;) Configure how to sort file-tree menu entries. Possible options are \u0026#39;title\u0026#39;, \u0026#39;linktitle\u0026#39;, # \u0026#39;date\u0026#39;, \u0026#39;publishdate\u0026#39;, \u0026#39;expirydate\u0026#39; or \u0026#39;lastmod\u0026#39;. Every option can be used with a reverse modifier as well # e.g. \u0026#39;title_reverse\u0026#39;. geekdocFileTreeSortBy: \u0026#34;title\u0026#34; # (Optional, default none) Adds a \u0026#34;Content licensed under \u0026lt;license\u0026gt;\u0026#34; line to the footer. # Could be used if you want to define a default license for your content. geekdocContentLicense: name: CC BY-SA 4.0 link: https://creativecommons.org/licenses/by-sa/4.0/ Page configuration TOML # Set type to \u0026#39;posts\u0026#39; if you want to render page as blogpost type = \u0026#34;posts\u0026#34; # Hugo predefined front matter variable, to re-arrange items in file-tree menu # See weights section of # https://gohugo.io/content-management/front-matter/#predefined weight = 10 # Set how many table of contents levels to be showed on page. geekdocToC = 3 # Set a description for the current page. This will be shown in toc-trees objects. geekdocDescription = # Set false to hide the whole left navigation sidebar. Beware that it will make # navigation pretty hard without adding some kind of on-page navigation. geekdocNav = true # Show a breadcrumb navigation bar at the top of each docs page. geekdocBreadcrumb = false # Set source repository location. geekdocRepo = \u0026#34;https://github.com/thegeeklab/hugo-geekdoc\u0026#34; # Enable \u0026#39;Edit page\u0026#39; links. Requires \u0026#39;geekdocRepo\u0026#39; param and the path must point to # the parent directory of the \u0026#39;content\u0026#39; folder. geekdocEditPath = \u0026#34;edit/main/exampleSite\u0026#34; # Used for \u0026#39;Edit page\u0026#39; link, set to \u0026#39;.File.Path\u0026#39; by default. # Can be overwritten by a path relative to \u0026#39;geekdocEditPath\u0026#39; geekdocFilePath = # Set to mark page as flat section (file-tree menu only). geekdocFlatSection = true # Set true to hide page or section from side menu (file-tree menu only). geekdocHidden = true # Set false to show this page as a file-tree menu entry when you want it to be hidden in the sidebar. # NOTE: Only applies when \u0026#39;geekdocHidden = true\u0026#39;. geekdocHiddenTocTree = true # Set to true to make a section foldable in side menu. geekdocCollapseSection = true # Add an anchor link to headlines. geekdocAnchor = true # If you have protected some pages with e.g. basic authentication you may want to exclude these pages # from data file, otherwise information may be leaked. Setting this parameter to \u0026#39;true\u0026#39; will exclude the # page from search data, feeds, etc. # WARNING: Consider hosting a standalone, fully auth-protected static page for secret information instead! geekdocProtected = false # Set \u0026#39;left\u0026#39; (default), \u0026#39;center\u0026#39; or \u0026#39;right\u0026#39; to configure the text align of a page. geekdocAlign = \u0026#34;left\u0026#34; YAML # Set type to \u0026#39;posts\u0026#39; if you want to render page as blogpost. type: \u0026#34;posts\u0026#34; # Set page weight to re-arrange items in file-tree menu. weight: 10 # Set how many table of contents levels to be showed on page. geekdocToC: 3 # Set a description for the current page. This will be shown in toc-trees objects. geekdocDescription: # Set false to hide the whole left navigation sidebar. Beware that it will make # navigation pretty hard without adding some kind of on-page navigation. geekdocNav: true # Show a breadcrumb navigation bar at the top of each docs page. geekdocBreadcrumb: false # Set source repository location. geekdocRepo: \u0026#34;https://github.com/thegeeklab/hugo-geekdoc\u0026#34; # Enable \u0026#39;Edit page\u0026#39; links. Requires \u0026#39;geekdocRepo\u0026#39; param and the path must point to # the parent directory of the \u0026#39;content\u0026#39; folder. geekdocEditPath: \u0026#34;edit/main/exampleSite\u0026#34; # Used for \u0026#39;Edit page\u0026#39; link, set to \u0026#39;.File.Path\u0026#39; by default. # Can be overwritten by a path relative to \u0026#39;geekdocEditPath\u0026#39; geekdocFilePath: # Set to mark page as flat section (file-tree menu only). geekdocFlatSection: true # Set true to hide page or section from side menu (file-tree menu only). geekdocHidden: true # Set false to show this page as a file-tree menu entry when you want it to be hidden in the sidebar. # NOTE: Only applies when \u0026#39;geekdocHidden: true\u0026#39;. geekdocHiddenTocTree: true # Set to true to make a section foldable in side menu. geekdocCollapseSection: true # Add an anchor link to headlines. geekdocAnchor: true # If you have protected some pages with e.g. basic authentication you may want to exclude these pages # from data file, otherwise information may be leaked. Setting this parameter to \u0026#39;true\u0026#39; will exclude the # page from search data, feeds, etc. # WARNING: Consider hosting a standalone, fully auth-protected static page for secret information instead! geekdocProtected: false # Set \u0026#39;left\u0026#39; (default), \u0026#39;center\u0026#39; or \u0026#39;right\u0026#39; to configure the text align of a page. geekdocAlign: \u0026#34;left\u0026#34; ","description":"Site configuration Page configuration Site configuration TOML baseURL = \u0026#34;http://localhost\u0026#34; title = \u0026#34;Geekdocs\u0026#34; theme = \u0026#34;hugo-geekdoc\u0026#34; # Required to get well formatted code blocks pygmentsUseClasses = true pygmentsCodeFences = true disablePathToLower = true enableGitInfo = true # Required if you want to render robots.txt template enableRobotsTXT = true [markup] [markup.goldmark.renderer] # Needed for mermaid shortcode or when nesting shortcodes (e.g. img within # columns or tabs) unsafe = true [markup."},{"id":4,"href":"/en/geekdoc/shortcodes/","title":"Shortcodes","parent":"geekdoc","content":"","description":""},{"id":5,"href":"/en/hub/Databases/relational-database/","title":"relational database","parent":"Databases","content":"","description":""},{"id":6,"href":"/en/hub/Concepts/languages/SQL/","title":"SQL","parent":"languages","content":"SQL stands for Structured Query Language and is used for retrieving and manipulating data in a relational database management system (RDBMS). SQL is a declarative language meaning you don\u0026rsquo;t tell the computer how to return the data, you tell it what you want the results to look like and the database figures out the best way to deliver those results.\nSQL Variants There are multiple variants or dialects of SQL built by different companies which are built on top of the SQL Standard. They are roughly 95% similar in terms of syntax but each database may interpret them differently leading to optimization differences between databases. It\u0026rsquo;s typically not important to learn more than one, especially when you are just getting started. However, when you get more advanced it will be helpful to know the differences as you will likely start using different types of databases for different use cases.\nThe most common variants are:\n[[T-SQL]] ([[Microsoft SQL Server]]) [[PostgreSQL]] ([[PostgreSQL]]) [[MySQL]] ([[MySQL]]) ![[Learning Resources#SQL Learning Resources]]\nRecent Posts ","description":"SQL stands for Structured Query Language and is used for retrieving and manipulating data in a relational database management system (RDBMS). SQL is a declarative language meaning you don\u0026rsquo;t tell the computer how to return the data, you tell it what you want the results to look like and the database figures out the best way to deliver those results.\nSQL Variants There are multiple variants or dialects of SQL built by different companies which are built on top of the SQL Standard."},{"id":7,"href":"/en/hub/Concepts/languages/Python/","title":"Python","parent":"languages","content":"![[Assets/python_logo.svg|100]]\nPython is a high-level general-purpose programming language. It\u0026rsquo;s main philosophy revolves around code readability and object-oriented design to help programmers write and read clear, logical code. In Data Engineering, it\u0026rsquo;s commonly used to transform data and incorporate business logic in [[Data Pipeline|data pipelines]].\nOfficial Documentation https://docs.python.org/\nAdvantages Easy to learn, read and write Requires less code to complete a task compared to most other languages Can run on any platform with the same code (portable) Extensive 3rd party libraries Large active community Disadvantages Code is not compiled so it\u0026rsquo;s slower than compiled languages Increased probability of runtime errors due to dynamic typing Memory intensive Database access is weaker compared to JDBC and ODBC ![[Learning Resources#Python Learning Resources]]\nRecent Python Posts in the Community ","description":"![[Assets/python_logo.svg|100]]\nPython is a high-level general-purpose programming language. It\u0026rsquo;s main philosophy revolves around code readability and object-oriented design to help programmers write and read clear, logical code. In Data Engineering, it\u0026rsquo;s commonly used to transform data and incorporate business logic in [[Data Pipeline|data pipelines]].\nOfficial Documentation https://docs.python.org/\nAdvantages Easy to learn, read and write Requires less code to complete a task compared to most other languages Can run on any platform with the same code (portable) Extensive 3rd party libraries Large active community Disadvantages Code is not compiled so it\u0026rsquo;s slower than compiled languages Increased probability of runtime errors due to dynamic typing Memory intensive Database access is weaker compared to JDBC and ODBC !"},{"id":8,"href":"/en/hub/Data-Governance/Choosing-your-optimal-messaging-service/","title":"Choosing your optimal messaging service","parent":"Data Governance","content":" Overview A short guide on choosing which messaging service(s) to use.\nAWS %%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph TD A((Start)) --\u0026gt; B{Fan-out} B --\u0026gt;|Yes| C{Rate limit} C --\u0026gt;|Yes| D[SNS \u0026#43; SQS] C --\u0026gt;|No| E[SNS] B --\u0026gt;|No| F{Rate limit} F --\u0026gt;|Yes| G[SQS] F --\u0026gt;|No| H[Lambda Direct Invoke] class B internal-link; Source: AWS re:Invent 2020: Scalable serverless event-driven architectures with SNS, SQS \u0026amp; Lambda\nAzure #placeholder/description\nGCP #placeholder/description\n","description":"Overview A short guide on choosing which messaging service(s) to use.\nAWS %%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph TD A((Start)) --\u0026gt; B{Fan-out} B --\u0026gt;|Yes| C{Rate limit} C --\u0026gt;|Yes| D[SNS \u0026#43; SQS] C --\u0026gt;|No| E[SNS] B --\u0026gt;|No| F{Rate limit} F --\u0026gt;|Yes| G[SQS] F --\u0026gt;|No| H[Lambda Direct Invoke] class B internal-link; Source: AWS re:Invent 2020: Scalable serverless event-driven architectures with SNS, SQS \u0026amp; Lambda\nAzure #placeholder/description\nGCP #placeholder/description"},{"id":9,"href":"/en/hub/Concepts/languages/Java/","title":"Java","parent":"languages","content":"Java is a high-level, general-purpose, object-oriented programming language that is designed to have as few implementation dependencies as possible. Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture.\n","description":"Java is a high-level, general-purpose, object-oriented programming language that is designed to have as few implementation dependencies as possible. Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture."},{"id":10,"href":"/en/hub/Data-Governance/Cost-Optimization-in-the-Cloud/","title":"Cost Optimization in the Cloud","parent":"Data Governance","content":" Overview This guide provides general guidance for strategies to optimize various assets in the cloud. When talking about the cloud we will be using the most popular cloud providers as examples ([[Amazon Web Services|AWS]], [[Google Cloud Platform|GCP]], and [[Microsoft Azure|Azure]]).\nGeneral Compute General compute refers to servers that can be used to handle a large variety of general purpose work in the cloud. Typically, this kind of compute is used for transforming data or hosting a service. General compute services range from fully customizable to managed services where you have less control over the environment and settings.\nExamples of general compute services:\nAWS: EC2, Fargate, Batch Azure: Virtual Machine, Container Instances, Batch GCP: Compute Engine, Cloud Run, Batch on GKE Turn on metrics monitoring Before you can optimize anything, you need to turn on metrics to monitor the performance of your service. This monitoring is usually an additional expense but reasonable. If you don\u0026rsquo;t believe you\u0026rsquo;ll need it long term you can turn it on while you optimize and then turn it off later.\nExamples of metrics monitoring services:\nAWS: CloudWatch Azure Monitor GCP: Cloud Monitoring Datadog Once monitoring is turned on, focus on understanding your workload patterns and assessing whether your current usage is over-provisioned or under-provisioned. If you realize at this point that your workload is unpredictable, you may want to consider switching to a serverless service.\nRightsize resources Rightsizing is a term that means identifying and adjusting specific resources to increase resource utilization and potentially save costs. This adjustment usually happens when there\u0026rsquo;s an over-provisioning situation. Now that you\u0026rsquo;ve activated metric monitoring and gathered data on your resource usage, ensure that your instance size is suitable. This is the point where you\u0026rsquo;ll fine-tune the instance size to match the CPU and memory requirements of your workload.\nEnable Autoscaling After rightsizing your compute service, you can typically enable autoscaling to dynamically adjust resources up and down based on demand in your workload. This means that if demand is low, autoscaling will reduce the amount of resources provisioned allowing you to save money. Along with autoscaling, you will typically set high and low thresholds which should be based around your typical workload.\nSavings plans Finally, after exploring the above options, you can usually get significant savings by purchasing savings plans which are typically longer range commitments to use a predetermined amount of a resource. These are great when you know that your workload is relatively steady and predictable. Savings plans are a great high impact and low effort option for saving money.\nAWS Savings plans Azure savings plans for compute GCP Committed use discounts Databases #placeholder\n","description":"Overview This guide provides general guidance for strategies to optimize various assets in the cloud. When talking about the cloud we will be using the most popular cloud providers as examples ([[Amazon Web Services|AWS]], [[Google Cloud Platform|GCP]], and [[Microsoft Azure|Azure]]).\nGeneral Compute General compute refers to servers that can be used to handle a large variety of general purpose work in the cloud. Typically, this kind of compute is used for transforming data or hosting a service."},{"id":11,"href":"/en/hub/Concepts/languages/Scala/","title":"Scala","parent":"languages","content":"Scala combines object-oriented and functional programming in one concise, high-level language. Scala\u0026rsquo;s static types help avoid bugs in complex applications, and its JVM and JavaScript runtimes let you build high-performance systems with easy access to huge ecosystems of libraries.\n","description":"Scala combines object-oriented and functional programming in one concise, high-level language. Scala\u0026rsquo;s static types help avoid bugs in complex applications, and its JVM and JavaScript runtimes let you build high-performance systems with easy access to huge ecosystems of libraries."},{"id":12,"href":"/en/hub/Data-Governance/SQL-Guide/","title":"SQL Guide","parent":"Data Governance","content":" Overview This guide is intended to be a general [[SQL]] reference for data engineers. It is not specific to any particular [[SQL#SQL Variants|variant of SQL]]. It also does not cover every concept or feature of SQL - only the most important or commonly used ones in data engineering.\n[!info]- ### SQL Learning Resources ![[Learning Resources#SQL Learning Resources]]\n1. Beginner SQL Order of Operations SQL executes each clause in a query in a defined order.\nFROM, including JOINs WHERE GROUP BY HAVING WINDOW functions SELECT DISTINCT UNION ORDER BY LIMIT and OFFSET Basic Commands SELECT\nUsed to select data from a database The data returned is stored in a result table, called the result-set. FROM\nUsed to specify which table to select or delete data from. WHERE\nUsed to filter records. It is used to extract only those records that fulfill a specified condition. ORDER BY\nUsed to sort the result set in ascending or descending order. Sorts the result set in ascending order by default. To sort the records in descending order, use the DESC keyword. Joins INNER JOIN\nReturns only those records or rows that have matching values and is used to retrieve data that appears in both tables. LEFT JOIN\nGives the output of the matching rows between both tables. In case, no records match from the left table, it shows those records with null values. RIGHT JOIN\nGives the output of the matching rows between both tables. In case, no records match from the right table, it shows those records with null values. FULL (OUTER) JOIN\nWill retrieve not only the matching rows but also the unmatched rows as well. CROSS (CARTESIAN) JOIN\nJoins every row from the first table with every row from the second table and its result comprises all combinations of records in two tables. SELF JOIN\nJoins a table to itself. UNION vs UNION ALL UNION and UNION ALL are both used to retrieve records from multiple tables. Both UNION and UNION ALL are known as set operators. In SQL, set operators combine the results of two or more queries into a single result.\nThere is one major difference:\nUNION only returns unique UNION ALL returns all records, including duplicates. Example: The columns in both SELECT statements are of the same or matching data types.\nSELECT column_1, column_2 FROM table_1 [WHERE condition] UNION [ALL] SELECT column_1, column_2 FROM table_2 [WHERE condition] Filtering Data Filtering data with SQL is useful for returning desired reults from a dataset or table. Filtering can be accomplsihed with the WHERE clause.\nExamples:\nFilter with logical operators AND operator\n# this will return rows where the City column has a value of \u0026#34;London\u0026#34; and the Country column has a value of \u0026#34;UK\u0026#34; SELECT * FROM Customers WHERE City = \u0026#34;London\u0026#34; AND Country = \u0026#34;UK\u0026#34; OR operator\n# this will return rows where the City column has a value of either \u0026#34;London\u0026#34; or \u0026#34;Paris\u0026#34; SELECT * FROM Customers WHERE City = \u0026#34;London\u0026#34; OR City = \u0026#34;Paris\u0026#34; BETWEEN operator\n# this will return rows where the Price column has values that are between 50 and 60 SELECT * FROM Products WHERE Price BETWEEN 50 AND 60 LIKE operator\n# this will return rows where the City column has values that start with \u0026#39;S\u0026#39; with no character limit SELECT * FROM Customers WHERE City LIKE \u0026#39;S%\u0026#39; \u0026gt;\u0026gt; Santiago, Sydney, San Antonio # the \u0026#39;%\u0026#39; wildcard can placed any where in a string to try and find a match SELECT * FROM Customers WHERE City LIKE \u0026#39;%ar%\u0026#39; \u0026gt;\u0026gt; Paris, Barcelona, Jakarta # the \u0026#39;_\u0026#39; wildcard can also be used in conjuction with \u0026#39;%\u0026#39; to find a single value # this will return rows where \u0026#39;a\u0026#39; has to be the second character and the rest of the characters can be anything SELECT * FROM Customers WHERE City LIKE \u0026#39;_a%\u0026#39; \u0026gt;\u0026gt; Lagos, Manila, Cairo Limit Used to specify the number of records to return. Different database systems use their own syntax:\nSQL Server = SELECT TOP 3 * FROM Customers; MySQL = SELECT * FROM Customers LIMIT 3; Oracle = SELECT * FROM Customers WHERE ROWNUM \u0026lt;= 3 Where vs Having WHERE Introduces a condition on individual rows. Use before GROUP BY clause. HAVING Introduces a condition on aggregations, i.e. results of selection where a single result, such as COUNT(), SUM(), MAX(), MIN() has been produced from multiple rows. Use after GROUP BY clause. 2. Intermediate SQL CTE vs Subquery CTE\nCan be used multiple times in the body of a query. Allows for recursive queries. Generally more readable. Subquery\nCan only be used once in a query. Can be used to filter results in the WHERE clause. Can be used as a column in your query. Table vs View vs Materialized View Table View Materialized View A table contains records (rows) with data values for specified columns (fields) and is stored physically in a database. Tables can be joined together to create reports, manipulated through SQL queries, and changed directly by updating or deleting individual rows or columns. A view is a virtual table based on the results of a SQL statement. It does not physically exist in the database and the fields in the view are fields from one or more real tables in the database. A materialized view is a snapshot of a query saved as a physical object within the database. Materialized views can be used instead of tables and support all operations available to real tables with some drawbacks such as maintenance costs. Data physically stored in database Data not physically stored Data physically stored in database Faster to query Slower to query Faster to query If underlying table is dropped then view will no longer work Materialized view will continue to work if underlying table is dropped No additional maintenance required No additional maintenance required Needs to be updated as new/updated data arrive in underlying table Case Statements Case statements are SQL\u0026rsquo;s version of an if-else logic. The CASE expression goes through conditions and returns a value when the first condition is met. Once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the ELSE clause.\nExample:\n--Sytanx CASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 WHEN conditionN THEN resultN ELSE result END; --Example SELECT OrderID, Quantity, CASE WHEN Quantity \u0026gt; 30 THEN \u0026#39;The quantity is greater than 30\u0026#39; WHEN Quantity = 30 THEN \u0026#39;The quantity is 30\u0026#39; ELSE \u0026#39;The quantity is under 30\u0026#39; END AS QuantityText FROM OrderDetails; DML vs DDL DML\nDML is short name of Data Manipulation Language which deals with data manipulation, and includes most common SQL statements such SELECT, INSERT, UPDATE, DELETE etc, and it is used to store, modify, retrieve, delete and update data in database.\nSELECT – retrieve data from one or more tables. INSERT – insert data into a table. UPDATE – updates existing data within a table. DELETE – delete all records from a table. MERGE – UPSERT operation (insert or update) CALL – call a PL/SQL or Java subprogram. EXPLAIN PLAN – interpretation of the data access path. LOCK TABLE – concurrency control. DDL\nDDL is short name of Data Definition Language, which deals with database schemas and descriptions, of how the data should reside in the database.\nCREATE – to create database and its objects like (table, index, views, store procedure, function and triggers). ALTER – alters the structure of the existing database. DROP – delete objects from the database. TRUNCATE – remove all records from a table; also, all spaces allocated for the records are removed. COMMENT – add comments to the data dictionary. RENAME – rename an object. (Source: StackOverflow)\nAggregate Functions An SQL aggregate function calculates on a set of values and returns a single value. For example, the average function (AVG) takes a list of values and returns the average.\nOther basic aggregate functions include:\nCOUNT() – returns the number of items in a set. MAX() – returns the maximum value in a set. MIN() – returns the minimum value in a set SUM() – returns the sum of all or distinct values in a set Note that aggregate functions do not work within in a WHERE clause due to the order of evaluation of clauses. Instead, GROUP BY and HAVING clases are used in place of a WHERE clause.\n3. Advanced SQL Window Functions Online Reference\nWindow functions perform calculations on a set of rows that are related together, but, unlike aggregate functions, windowing functions do not collapse the result of the rows into a single value. Instead, all the rows maintain their original identity and the calculated result is returned for every row.\n![[Assets/window-vs-aggregate-function.png]]\nCorrelated Subqueries ![[Correlated Subquery]]\n","description":"Overview This guide is intended to be a general [[SQL]] reference for data engineers. It is not specific to any particular [[SQL#SQL Variants|variant of SQL]]. It also does not cover every concept or feature of SQL - only the most important or commonly used ones in data engineering.\n[!info]- ### SQL Learning Resources ![[Learning Resources#SQL Learning Resources]]\n1. Beginner SQL Order of Operations SQL executes each clause in a query in a defined order."},{"id":13,"href":"/en/hub/Data-Governance/Testing-Your-Data-Pipeline/","title":"Testing Your Data Pipeline","parent":"Data Governance","content":" Why Is Testing Important? Writing tests have a few key benefits for data engineers. They can help you sleep better at night knowing you\u0026rsquo;re less likely to get called into an emergency for a data issue. They can be used to help yourself and stakeholders get on the same page and understand the data you\u0026rsquo;re working with. And one of the most important reasons is they help build stakeholders\u0026rsquo; trust in the data. If stakeholders can\u0026rsquo;t trust the data they are using then they will likely stop using it and build their own solutions which may duplicate efforts or lead to more problems in the future.\nThe Different Types of Tests There are two main types of tests data engineers use to test their data pipelines: unit tests and data quality tests.\nUnit tests are used to test code, such as the functions and classes that make up a data pipeline. Unit tests can help ensure that your code is doing what it should be doing and that any changes you make don\u0026rsquo;t break existing functionality.\n[!example] Let\u0026rsquo;s say you have a function that you use to transform some data in Python. You would write a unit test with various inputs and expected outputs to ensure that it would always transform data in the same way even if you make a change.\nData quality tests, on the other hand, are used to check the accuracy of data flowing through your pipeline. These types of tests can help identify potential issues with incoming or outgoing data before they become a problem for downstream systems. Data quality tests also provide an extra layer of assurance that all parts of the system are working together correctly and producing reliable results.\n[!example] A data quality test for a data pipeline might involve comparing the results of a SQL query against the expected results. For example, if you have a dataset containing customer orders and you want to make sure that each order contains at least one item, your data quality test could check that there are no orders with zero items.\n[!tip] Typically, data quality tests are used with a Write-Audit-Publish pattern to make sure unexpected data doesn\u0026rsquo;t interfere with stakeholder-facing tools and diminish their trust in the data.\nCreating a Testing Plan Step 1: Identify your most important pipelines As a data engineer, you will constantly need to balance competing priorities which is why we first start by identifying pipelines to prioritize adding testing to. Here are a few questions you can ask yourself to identify these pipelines:\nIs it directly related to something that makes money or involves money (i.e. the Finance department)? Does it power a core feature of the business? Is the data used by high-level decision makers? These are the pipelines that should be prioritized and tested the most rigorously.\nStep 2: Understand the architecture and decide what/how to test Data pipelines can look very different depending on the company you\u0026rsquo;re at. At some companies, they are entirely no-code, at others everything is built in-house, but most are a combination of code mixed with SaaS or open-source tools.\nGenerally speaking, if you have any custom code that extracts, loads, or transforms data you should write unit tests that run every time you make a change to this code. Priority should be given to any business logic and custom connectors to ensure data isn\u0026rsquo;t corrupted early in the pipeline. Popular tools for this are unittest for Python and GitHub Actions to automate running the tests.\n[!tip] New to unit testing? Here is a short guide for beginners: https://www.dataquest.io/blog/unit-tests-python/\nNow that you\u0026rsquo;re testing your code, you also need to test your data quality to catch any \u0026ldquo;silent failures\u0026rdquo; and ensure the data arrives as expected. Silent failure in this case refers to when a data pipeline doesn\u0026rsquo;t produce an error but the data output is wrong or missing. These data quality tests are run as part of your data pipeline. Here are several popular [[Data Unit Test#Data Unit Testing Tools|data quality testing tools]].\nData quality tests should be triggered early on in the pipeline and test core business logic. You\u0026rsquo;ll need to work with the person/department who is the owner of the business logic to help you build appropriate tests. It would be a mistake to not talk to other departments and spend time writing tests that you later find out aren\u0026rsquo;t valuable.\nSome common data quality tests are:\nCheck the count of rows Check if a column contains nulls Check if the unique values in a column are expected values Check if a number value falls into an expected range Other kinds of tests may be more common depending on the use case. For example, if your pipeline feeds data into a machine learning model, then it\u0026rsquo;s common to check and make sure the data population hasn\u0026rsquo;t drifted.\nStep 3: Planning for failure When a unit test fails it usually fails before the code makes it to production. But what do you do if a data quality test fails?\nGoing back to the Write-Audit-Publish pattern mentioned earlier, you\u0026rsquo;ll want to write the batch of data that failed your tests somewhere so you can inspect it and figure out what happened. This also prevents bad data from being used downstream in important dashboards or data products. In a batch pipeline, this may be just a separate table in your data warehouse or it could be written to object storage like an S3 bucket. For streaming pipelines, you might send this bad data to a Dead-Letter Queue. Again, you should be able to inspect, fix, and then re-run the data through the pipeline from wherever you temporarily store the failed data.\nAnother common practice for data engineering is to create a runbook for each pipeline. A runbook is simply documentation that outlines the steps to take to resolve an issue or perform a task. You can read more about runbooks here.\nContinuous Monitoring and Alerting We won\u0026rsquo;t get into the details of monitoring and alerting because it\u0026rsquo;s a separate topic but it\u0026rsquo;s worth mentioning here that you will need to make sure that if tests fail, the appropriate person is alerted and can fix it. If you\u0026rsquo;re using a workflow orchestrator to run your pipeline then you can use the built-in email/alerting functionality to send an email if a data quality test fails.\n","description":"Why Is Testing Important? Writing tests have a few key benefits for data engineers. They can help you sleep better at night knowing you\u0026rsquo;re less likely to get called into an emergency for a data issue. They can be used to help yourself and stakeholders get on the same page and understand the data you\u0026rsquo;re working with. And one of the most important reasons is they help build stakeholders\u0026rsquo; trust in the data."},{"id":14,"href":"/en/hub/Data-Governance/Guides/","title":"Guides","parent":"Data Governance","content":"[[Getting Started With Data Engineering]]\n","description":"[[Getting Started With Data Engineering]]"},{"id":15,"href":"/en/hub/Data-Governance/Data-Governance-Guide/","title":"Data Governance Guide","parent":"Data Governance","content":"The emergence of [[data governance]] directly correlated with massively increased quantities of data that is now considered standard. Only a few decades ago, many companies managed a quantity of data that was reasonable to organize by a database administrator. But with the increased popularity of using different data sources and streaming data as well as the sheer quantity of data that is available nowadays, it is dangerous to leave data to a single person and expect them to properly handle everything.\nData governance is a philosophy of data management that focuses on establishing responsibility for data throughout the complete lifecycle of data. The important part here is that data governance is unfortunately not a \u0026ldquo;set it and forget it\u0026rdquo; system for managing data, at least during the beginning stages of implementation. Contrary to most of the philosophical cores of data engineering, completely automating data governance is not recommended. A quality data governance framework involves a system of rules, processes, procedures, and enforcement strategies to ensure that data is properly accounted for.\nDifferent kinds of data may require fine-tuned guidelines. For example, how one would manage HIPAA compliant data is very different than how one would handle a spreadsheet keeping track of lunch expenses for a team bowling party. Additionally, different teams and sub-organizations may have different cultures that necessitate the tweaking of any framework to ensure a customized fit. These challenges necessitate some manual guidance and buy-in from policymakers at the top of an organization. Without actual enforcement, data governance tends to be difficult to implement due to the natural high friction it tends to create.\nData use and management are particularly difficult when coordinating within and between different units to allow for better delivery on the business side and more accountability on the security side. There are typically multiple steps required to implement data governance effectively. A common five-pronged approach involves the following stages: inventory/mapping, planning, education, implementation, and enforcement.\nWhenever data governance is mentioned, a key piece that is misunderstood is what engaging in data governance entails. Even logging onto a computer or sending a work email is technically an example of data governance. The key is identifying what the end state of data governance looks like for a particular organization and then setting a formal practice for managing organizational information with consistency.\nInventory When compiling an inventory of assets, there are two pieces to keep in mind. A standard data inventory tends to be a completely documented repository of information resources that are owned by an organization, including the associated metadata. A data inventory is focused on understanding data and identifying risks that are posed due to any missing/broken dependencies or gaps. When tailoring an inventory for data governance, it can be helpful to have explicit data mappings associated with databases and data owners/subject matter experts. This allows for an open communication line between policymakers and gatekeepers of important data sets.\nThere are many use cases where data governance is being embraced by organizations of all different kinds. Larger companies often have different teams with completely different views on data. The analytics team might view it as a secret weapon while the security team might view it as a liability. This can lead to conflicts when it comes to how to best manage and assign ownership of data. Government institutions on the other hand traditionally deal with a high level of employee turnover, leading to entire databases and key institutional knowledge being lost in the offboarding process.\nDetailing, documenting, and accounting for risks and opportunities is a key piece of data inventory when preparing it for data governance.\nPlanning Once the data assets and personnel are identified for an organization, building trust with database owners, subject matter experts, and policymakers within the organization is key. Since this is the planning stage it is difficult to actually enforce data governance, and as such getting buy-in at this stage is crucial.\nA comprehensive plan requires interviews and meetings with key stakeholders and database owners/subject matter experts to understand what the needs and wants of all the sub-organizations require and how to reconcile these demands.\nSpecific processes that need to be ironed out should be included in this phase as well. For example, for a State level organization, data trusts tend to be very important. IT teams like to consolidate databases and implement uniform security.\nEducation In order for people in organizations to fully embrace data governance, a training and education program is necessary. Typically classroom-style lectures tend to not be effective. Instead, convincing business owners how data governance can bring value to the company and security admins that data governance increases the security and stability of the work.\nImplementation Actually implementing the process tends to be more of a practice of execution. As long as there\u0026rsquo;s been a quality plan and strategy produced and a training program that has started tackling some of the silos that have been built, progress will eventually be made. The biggest indicator of success for the implementation stage will be how interested and engaged key stakeholders are.\nThe main processes for data governance are the query engine, data catalog, and policy engine. Policymakers create a policy that is applied to the data catalog which is consumed through the query engine.\nEnforcement Enforcement is relatively simple if the infrastructure for the implementation is thorough. By applying the correct policies that restrict user roles, minimal enforcement is necessary. The main enforcement that is required during the process is making sure subject matter experts, policymakers, and stakeholders are all engaged. This is largely the job of Chief policymakers to create a plan of accountability.\nSummary Data governance is the practice of making data more of an asset rather than a liability. The ability to know and be able to search through all data assets an organization owns is transformative.\nData governance in modern architecture is centered around policymakers, data consumers, and subject matter experts. Subject matter experts own data resources, which must be mapped and consolidated under a data catalog with the associated metadata.\n","description":"The emergence of [[data governance]] directly correlated with massively increased quantities of data that is now considered standard. Only a few decades ago, many companies managed a quantity of data that was reasonable to organize by a database administrator. But with the increased popularity of using different data sources and streaming data as well as the sheer quantity of data that is available nowadays, it is dangerous to leave data to a single person and expect them to properly handle everything."},{"id":16,"href":"/en/hub/Data-Governance/Getting-Started-With-Data-Engineering/","title":"Getting Started With Data Engineering","parent":"Data Governance","content":" Overview This guide is intended for people who are new to Data Engineering and aren\u0026rsquo;t sure where to start. The purpose of this guide isn\u0026rsquo;t to cover every single thing you need to know, but rather give you the working knowledge and the intuition to find answers later on in your Data Engineering journey.\nStep 1: Read the FAQ ![[FAQ]]\nStep 2: Learn Data Engineering core concepts ![[Concepts#Core Concepts]]\nStep 3: Learn the core tools ![[Tools#Core Tools]]\n","description":"Overview This guide is intended for people who are new to Data Engineering and aren\u0026rsquo;t sure where to start. The purpose of this guide isn\u0026rsquo;t to cover every single thing you need to know, but rather give you the working knowledge and the intuition to find answers later on in your Data Engineering journey.\nStep 1: Read the FAQ ![[FAQ]]\nStep 2: Learn Data Engineering core concepts ![[Concepts#Core Concepts]]"},{"id":17,"href":"/en/hub/Data-Governance/Data-Pipeline-Best-Practices/","title":"Data Pipeline Best Practices","parent":"Data Governance","content":" Overview A best practice guide for data pipelines compiled from data engineers in the community. Follow this guide to help you build more robust, scalable, and more performant data pipelines. These best practices are in no particular order but we\u0026rsquo;ve done our best to categorize them.\nGeneral Best Practices Verify your assumptions about the data. Document your pipelines. Add proper logging to your pipelines to make debugging easier. Use code and version control (git) for pipelines. Make your pipelines [[Idempotence|idempotent]]. Understand the tradeoff between fast data and accurate data. Data quality takes time. Have separate environments for development, staging, and production ideally. If you have separate environments, color code them and label them clearly. Use templates whenever possible. If you\u0026rsquo;re writing custom code, try to make it generic/modular. Avoid ingesting data from manually created data sources (e.g. Google Sheet/Excel file). If you have to do so, require strict protections on what can change at the source. Design Use Docker for dependency management. Prepare for intermittent or temporary failures. Use exponential back-off and retry strategies. Use CI to deploy pipelines to staging and production environments. Set up alerting on failures and pipeline run times. Surface all parameters and use configuration files/environment variables to change pipeline behavior vs updating the code. Optimization Don\u0026rsquo;t let file sizes become too large or too small. Large files (\u0026gt;1 GB) can require more resources and many small files can create a large overhead to process. ~250 MB is a good size that allows for better parallel processing. Use the [[Claim Check Pattern|claim check pattern]] to pass large amounts of data between tasks in your pipeline. Security Save credentials in a secrets manager and access them in your pipeline programmatically. Ideally, have secrets rotated automatically. Avoid logging any sensitive information like credentials or PII data. Testing Test data quality with [[Data Unit Test|data unit tests]] regularly. Test data pipeline code with regular unit tests. Set up a local environment to test pipelines locally first. (see Docker above) Re-define pipeline failures. If pipeline fails x times but data is still delivered on time then it was successful. ","description":"Overview A best practice guide for data pipelines compiled from data engineers in the community. Follow this guide to help you build more robust, scalable, and more performant data pipelines. These best practices are in no particular order but we\u0026rsquo;ve done our best to categorize them.\nGeneral Best Practices Verify your assumptions about the data. Document your pipelines. Add proper logging to your pipelines to make debugging easier. Use code and version control (git) for pipelines."},{"id":18,"href":"/en/geekdoc/posts/","title":"News","parent":"geekdoc","content":"","description":""},{"id":19,"href":"/en/hub/Data-on-Cloud/AWS/AWS-dynanoDB/","title":"AWS DynamoDB","parent":"AWS","content":" Overview This tutorial will show you how to export a large MongoDB collection as a JSON file and upload it to AWS S3 using the AWS CLI. The AWS CLI is used for larger files because there is a file size limit when uploading files to S3 via the AWS management console.\nRequirements Install mongoexport Install the AWS CLI and configure it 1. Export the collection using mongoexport In the example below, replace the uri connection string with your own.\nmongoexport --uri=\u0026#34;mongodb+srv://username:password@example-mongodb.example.mongodb.net/\u0026#34; --db=example --collection=example --out=example.json You can also put your credentials into a separate configuration file and reference it.\nmongoexport --config=config.yaml --db=example --collection=example --type=json --out=example.json 2. Generate an md5 checksum After we have exported our file from MongoDB, we want to generate a hash which uniquely identifies the data. It will be used in the next step to verify that the data was uploaded correctly in S3.\nopenssl md5 -binary example.json | base64 The generated md5 hash should look similar to: t8oeOvMA7tKvxzZoEcYawQ==\n3. Initiate copy to S3 Using the S3 copy command below, specify your file, the destination you want to send it to in S3, and the md5 hash generated previously.\naws s3 cp example.json s3://example-bucket/example.json --metadata md5=\u0026#34;example\u0026#34; ","description":"Overview This tutorial will show you how to export a large MongoDB collection as a JSON file and upload it to AWS S3 using the AWS CLI. The AWS CLI is used for larger files because there is a file size limit when uploading files to S3 via the AWS management console.\nRequirements Install mongoexport Install the AWS CLI and configure it 1. Export the collection using mongoexport In the example below, replace the uri connection string with your own."},{"id":20,"href":"/en/hub/Data-Process/Apache-Spark-RDD-example/","title":"Apache Spark RDD example","parent":"Data Process","content":" Overview This tutorial will cover basic RDD operations that can be run on either Google Colab or Databricks Community Edition.\nOfficial Documentation https://spark.apache.org/docs/latest/rdd-programming-guide.html\nConfiguration While Databricks has Spark installed as a native module, Google Colab needs some previous configuration to set the environment for the RDD operations.\nWe will first install PySpark, a Python library that let us use Apache Spark:\n!pip install pyspark After the module is installed, we will set up Spark Configuration so we can use SparkContext:\nfrom pyspark import SparkContext, SparkConf conf = SparkConf().setAppName(\u0026#39;test\u0026#39;).setMaster(\u0026#39;local\u0026#39;) sc = SparkContext(conf=conf) First Steps In order to work with RDDs we need to understand how an RDD is created. We will execute sc.parallelize([your_data]) to create an RDD. By default, Spark admits lists and dictionaries on the parallelize argument. Now, if we want to see the content of an RDD, we must execute .collect().\nWe can see here an easy example:\nnums2 = sc.parallelize([3,2,1,4,5]) evens = nums2.filter(lambda elem: elem%2==0) odds = nums2.filter(lambda elem: elem%2!=0) order = pairs.union(impairs) order.takeOrdered(5) [1, 2, 3, 4, 5] Let\u0026rsquo;s explain this operation:\nWe first create our rdd with sc.parallelize() under the variable name \u0026ldquo;nums2\u0026rdquo; We use a transformation operator (filter) and set up a lambda algorithm that will search for even numbers. We will do the same but searching for odd numbers. We execute an action operator (union) to join even and odds on a single list. To end this operation we will execute a TakeOrdered so we get an ordered list. ","description":"Overview This tutorial will cover basic RDD operations that can be run on either Google Colab or Databricks Community Edition.\nOfficial Documentation https://spark.apache.org/docs/latest/rdd-programming-guide.html\nConfiguration While Databricks has Spark installed as a native module, Google Colab needs some previous configuration to set the environment for the RDD operations.\nWe will first install PySpark, a Python library that let us use Apache Spark:\n!pip install pyspark After the module is installed, we will set up Spark Configuration so we can use SparkContext:"},{"id":21,"href":"/en/hub/Data-Process/dbt-CLI-autocomplete-in-Docker/","title":"dbt CLI autocomplete in Docker","parent":"Data Process","content":" Overview By the end of this tutorial you will have autocompletion available with the dbt CLI (using bash) while running dbt locally via Docker Compose. With the autocomplete you can tab-complete model, tag, source, and package selectors to node selection flags like --select and --exclude.\nExample usage (using the redshift package):\n$ dbt run --model red\u0026lt;TAB\u0026gt; redshift.* redshift_admin_queries redshift_constraints redshift.base.* redshift_admin_table_stats redshift_cost redshift.introspection.* redshift_admin_users_schema_privileges redshift_sort_dist_keys redshift.views.* redshift_admin_users_table_view_privileges redshift_tables redshift_admin_dependencies redshift_columns For other command line instructions, please read the dbt-completion.bash README.\nNote that the dbt resources must be compiled before it will be available for tab completion. Please see notes and caveats for full details.\nThis tutorial will not cover the basics of Docker or dbt-core. It is assumed you already have a dbt-core project and want to add autocomplete functionality.\nRequirements Docker Desktop A dbt core project 1. Create dbt Dockerfile Inside your dbt project folder, add a Dockerfile if one doesn\u0026rsquo;t exist. In this tutorial, we have our dbt project in a top level folder named dbt.\nProject structure:\n. └── my_project/ ├── dbt/ │ ├── ... │ ├── Dockerfile │ └── requirements.txt ├── airflow/ │ └── ... └── docker-compose.yaml Dockerfile:\nFROM public.ecr.aws/docker/library/python:3.10-slim-buster COPY . /dbt # Update and install system packages RUN apt-get update -y \u0026amp;\u0026amp; \\ apt-get install --no-install-recommends -y -q \\ git libpq-dev python-dev curl \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* # dbt cli autocomplete RUN curl https://raw.githubusercontent.com/dbt-labs/dbt-completion.bash/master/dbt-completion.bash \u0026gt; ~/.dbt-completion.bash RUN echo \u0026#39;source ~/.dbt-completion.bash\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile # Install dbt RUN pip install -U pip RUN pip install -r dbt/requirements.txt # Install dbt dependencies WORKDIR /dbt RUN dbt deps # Specify profiles directory ENV DBT_PROFILES_DIR=.dbt Note line 8 installing curl and lines 13-14 that download the dbt-completion.bash script. You can also build your Dockerfile on top of the official dbt-core docker images if you\u0026rsquo;re familiar with Docker but that will be out of scope for this tutorial.\n2. Create Docker Compose file Next, we will be using Docker Compose to orchestrate the dbt service. It\u0026rsquo;s likely you\u0026rsquo;re already using Docker compose for local development like running airflow and a local db.\ndocker-compose.yaml:\nversion: \u0026#39;3\u0026#39; services: dbt: build: context: ./dbt dockerfile: Dockerfile container_name: dbt tty: true volumes: - ./dbt:/dbt env_file: ./dbt/.env ports: - 8080:8080 3. Run dbt service and compile Now you can start the dbt service and enter the dbt CLI. When attaching to the dbt service, it\u0026rsquo;s important to use the -l command because it sources the .bash_profile.\nDocker compose commands:\n# Build \u0026amp; start docker detached docker compose up -d # Enter dbt CLI docker exec -it dbt bash -l # Inside dbt service run dbt compile And now you should be able to use tab-complete with dbt!\n","description":"Overview By the end of this tutorial you will have autocompletion available with the dbt CLI (using bash) while running dbt locally via Docker Compose. With the autocomplete you can tab-complete model, tag, source, and package selectors to node selection flags like --select and --exclude.\nExample usage (using the redshift package):\n$ dbt run --model red\u0026lt;TAB\u0026gt; redshift.* redshift_admin_queries redshift_constraints redshift.base.* redshift_admin_table_stats redshift_cost redshift.introspection.* redshift_admin_users_schema_privileges redshift_sort_dist_keys redshift.views.* redshift_admin_users_table_view_privileges redshift_tables redshift_admin_dependencies redshift_columns For other command line instructions, please read the dbt-completion."},{"id":22,"href":"/en/hub/Data-on-Cloud/AWS/Amazon-MSK/","title":"Amazon MSK","parent":"AWS","content":"AWS Managed Streaming for Apache Kafka is a fully managed tool to securely stream data with\nAmazon MSK Official Documentation https://aws.amazon.com/msk/\n","description":"AWS Managed Streaming for Apache Kafka is a fully managed tool to securely stream data with\nAmazon MSK Official Documentation https://aws.amazon.com/msk/"},{"id":23,"href":"/en/hub/Data-on-Cloud/GCP/Google-Cloud-Platform/","title":"Google Cloud Platform","parent":"Google Cloud Platform","content":"GCP is Google\u0026rsquo;s cloud computing platform. It boasts advanced machine learning and analytics capabilities as well as the \u0026ldquo;cleanest cloud\u0026rdquo; in the industry (aka net carbon-neutral).\n","description":"GCP is Google\u0026rsquo;s cloud computing platform. It boasts advanced machine learning and analytics capabilities as well as the \u0026ldquo;cleanest cloud\u0026rdquo; in the industry (aka net carbon-neutral)."},{"id":24,"href":"/en/hub/Data-on-Cloud/Azure/Microsoft-Azure/","title":"Microsoft Azure","parent":"Azure","content":"Azure is Microsoft\u0026rsquo;s cloud computing platform that has over 200 products and cloud services. It is especially popular among businesses that already use Microsoft data products like [[Microsoft SQL Server]] as they move to a hybrid model of having on-prem servers and cloud servers.\n","description":"Azure is Microsoft\u0026rsquo;s cloud computing platform that has over 200 products and cloud services. It is especially popular among businesses that already use Microsoft data products like [[Microsoft SQL Server]] as they move to a hybrid model of having on-prem servers and cloud servers."},{"id":25,"href":"/en/hub/Data-on-Cloud/AWS/Benthos/","title":"Benthos","parent":"AWS","content":"![[Assets/benthos.png|100]]\nBenthos is a free and open source data streaming engine written entirely in Go and packaged as a static single-binary command line tool. It comes with a wide range of connectors and is totally data agnostic.\nData transformations can be expressed using a high-level DSL called Bloblang, or blobl for short, which is a language designed for mapping data of a wide variety of forms.\nIt also exposes a Go API which allows users to import Benthos as a library and extend it through custom plugins.\nBenthos Studio is an application that provides visual editing and testing capabilities for Benthos pipelines.\nThe Community page contains links to various places where you can reach out and ask for help with Benthos.\nBenthos Advantages Can be used for both realtime and batch processing use cases Provides a unified framework for building data streaming pipelines Simple and easy to deploy Well-maintained and organized codebase written in pure Go Stateless Popular use cases Reading to / Writing from [[Apache Kafka|Kafka]] and similar event streaming platforms Windowed Processing ","description":"![[Assets/benthos.png|100]]\nBenthos is a free and open source data streaming engine written entirely in Go and packaged as a static single-binary command line tool. It comes with a wide range of connectors and is totally data agnostic.\nData transformations can be expressed using a high-level DSL called Bloblang, or blobl for short, which is a language designed for mapping data of a wide variety of forms.\nIt also exposes a Go API which allows users to import Benthos as a library and extend it through custom plugins."},{"id":26,"href":"/en/hub/Data-on-Cloud/AWS/Amazon-Web-Services/","title":"Amazon Web Services","parent":"AWS","content":"AWS is a suite of cloud-based tools revolving around applications, networking, infrastructure, data processing, and data storage. One of it\u0026rsquo;s main attributes is serverless and managed products.\nDatabases [[Tools/Databases/Amazon Redshift|Amazon Redshift]] [[Tools/Databases/Amazon RDS|Amazon RDS]] [[Tools/Databases/Amazon DynamoDB|Amazon DynamoDB]] [[Tools/Databases/Amazon Aurora|Amazon Aurora]] [[Tools/Databases/Amazon DocumentDB|Amazon DocumentDB]] File Storage [[Amazon S3]] [[Amazon S3 Glacier]] Computation [[AWS Lambda]] [[Amazon EC2]] [[Amazon ECS]] [[AWS Fargate]] [[Amazon EMR]] [[AWS Batch]] Scheduling and Workflow Orchestration [[AWS Step Functions]] [[Amazon MWAA]] Data Migration and Streaming [[Amazon DMS]] [[Amazon MSK]] [[Amazon Kinesis]] ","description":"AWS is a suite of cloud-based tools revolving around applications, networking, infrastructure, data processing, and data storage. One of it\u0026rsquo;s main attributes is serverless and managed products.\nDatabases [[Tools/Databases/Amazon Redshift|Amazon Redshift]] [[Tools/Databases/Amazon RDS|Amazon RDS]] [[Tools/Databases/Amazon DynamoDB|Amazon DynamoDB]] [[Tools/Databases/Amazon Aurora|Amazon Aurora]] [[Tools/Databases/Amazon DocumentDB|Amazon DocumentDB]] File Storage [[Amazon S3]] [[Amazon S3 Glacier]] Computation [[AWS Lambda]] [[Amazon EC2]] [[Amazon ECS]] [[AWS Fargate]] [[Amazon EMR]] [[AWS Batch]] Scheduling and Workflow Orchestration [[AWS Step Functions]] [[Amazon MWAA]] Data Migration and Streaming [[Amazon DMS]] [[Amazon MSK]] [[Amazon Kinesis]] "},{"id":27,"href":"/en/hub/Data-Governance/data-quality/Soda/","title":"Soda","parent":"Data Quality","content":"Soda is an open-source data quality testing platform that makes it easy for you to test data quality early and often in development and production pipelines. Soda catches data problems far upstream, before they can impact your business.\nSoda Official Documentation https://docs.soda.io/\nSoda Advantages #placeholder/description\nSoda Disadvantages #placeholder/description\n","description":"Soda is an open-source data quality testing platform that makes it easy for you to test data quality early and often in development and production pipelines. Soda catches data problems far upstream, before they can impact your business.\nSoda Official Documentation https://docs.soda.io/\nSoda Advantages #placeholder/description\nSoda Disadvantages #placeholder/description"},{"id":28,"href":"/en/hub/Data-Governance/data-quality/Great-Expectations/","title":"Great Expectations","parent":"Data Quality","content":"Great Expectations is a [[Python]] library for creating [[Data Unit Test|data unit tests]] that can be used in your [[Data Pipeline|data pipelines]].\nSummary Great Expectations operates off of the principal that data engineering pipelines tend towards entropy over time, a term that they dub \u0026ldquo;pipeline debt\u0026rdquo;. Great Expectations aims to provide a testing and evaluation suite to help data engineering teams clean up their pipelines and increase their confidence in working on them in a collaborative setting.\nGreat Expectations calls each unit test segment an \u0026ldquo;Expectation\u0026rdquo;, due to expecting the output of a test to be a certain value. Rather than other testing and evaluation suites, Great Expectations encourage testing at batch time (when new data arrives). This is in contrast to compile or deploy times. By testing at batch time, teams can be confident that there is a safety net should code behave unexpectedly for new data, and pinpoint the root cause as soon as possible.\nWorkflow Introduce the expectation early into the process, perhaps even before you\u0026rsquo;ve built a pipeline. Show it to the stakeholder and have them validate the assumptions. Implement it into your pipeline. Continuously update tests as data changes by iterating with the stakeholder. ","description":"Great Expectations is a [[Python]] library for creating [[Data Unit Test|data unit tests]] that can be used in your [[Data Pipeline|data pipelines]].\nSummary Great Expectations operates off of the principal that data engineering pipelines tend towards entropy over time, a term that they dub \u0026ldquo;pipeline debt\u0026rdquo;. Great Expectations aims to provide a testing and evaluation suite to help data engineering teams clean up their pipelines and increase their confidence in working on them in a collaborative setting."},{"id":29,"href":"/en/hub/Data-Governance/data-quality/Monte-Carlo/","title":"Monte Carlo","parent":"Data Quality","content":"Monte Carlo is an end-to-end [[data observability]] tool that monitors and alerts you of data quality issues throughout the data lifecycle.\nMonte Carlo Official Documentation https://docs.getmontecarlo.com/\nMonte Carlo Advantages #placeholder/description\nMonte Carlo Disadvantages #placeholder/description\n","description":"Monte Carlo is an end-to-end [[data observability]] tool that monitors and alerts you of data quality issues throughout the data lifecycle.\nMonte Carlo Official Documentation https://docs.getmontecarlo.com/\nMonte Carlo Advantages #placeholder/description\nMonte Carlo Disadvantages #placeholder/description"},{"id":30,"href":"/en/hub/Data-Governance/data-quality/Deequ/","title":"Deequ","parent":"Data Quality","content":"Deequ is a library built on top of Apache Spark for defining \u0026ldquo;unit tests for data\u0026rdquo;, which measure data quality in large datasets. Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on GitHub, readthedocs, and PyPI.\nDeequ Official Documentation https://github.com/awslabs/deequ/tree/master\nDeequ Advantages Built on [[Apache Spark]] for large datasets Deequ Disadvantages #placeholder/description\n","description":"Deequ is a library built on top of Apache Spark for defining \u0026ldquo;unit tests for data\u0026rdquo;, which measure data quality in large datasets. Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on GitHub, readthedocs, and PyPI.\nDeequ Official Documentation https://github.com/awslabs/deequ/tree/master\nDeequ Advantages Built on [[Apache Spark]] for large datasets Deequ Disadvantages #placeholder/description"},{"id":31,"href":"/en/hub/Concepts/formats/JSON/","title":"JSON","parent":"formats","content":"JSON, or JavaScript Object Notation, is a lightweight data-interchange format. It is easy for humans to read and write, and for machines to parse and generate. JSON is a text-based format that uses curly braces ({}) to indicate objects, brackets ([]) to indicate arrays, and quotes (\u0026quot;\u0026quot;) around string values. It is often used when transferring data in APIs.\nExtension: .json\nJSON Advantages Human readable/writeable. Supports several data types (string, number, object, array, true/false, and null) and nested data structures. Flexible schema evolution. JSON Disadvantages Must read the entire file - can\u0026rsquo;t read line-by-line Not compressed Doesn\u0026rsquo;t support binary data There are other versions of JSON that address some of these disadvantages (see BSON and NDJSON.)\nJSON Editors/Viewers JSON Crack - Seamlessly visualize your JSON data instantly into graphs. VSCode: Prettier - Prettier is an opinionated code formatter. Supports JSON formatting among several other code languages. ","description":"JSON, or JavaScript Object Notation, is a lightweight data-interchange format. It is easy for humans to read and write, and for machines to parse and generate. JSON is a text-based format that uses curly braces ({}) to indicate objects, brackets ([]) to indicate arrays, and quotes (\u0026quot;\u0026quot;) around string values. It is often used when transferring data in APIs.\nExtension: .json\nJSON Advantages Human readable/writeable. Supports several data types (string, number, object, array, true/false, and null) and nested data structures."},{"id":32,"href":"/en/hub/Concepts/formats/CSV/","title":"CSV","parent":"formats","content":"A CSV file is a type of delimited text file that uses commas to separate values, with each line of the file being one data record. A record can have multiple fields, which are also separated by commas. Usually, a CSV files stores tabular data (numbers and text) in plain text form, meaning each line will include the same number of fields.\nExtension: .csv\nCSV Advantages Human readable/writeable Widely used/supported by most applications Can be read in a memory efficient way CSV Disadvantages Data is uncompressed Does not support binary data Unquoted fields can easily \u0026ldquo;break\u0026rdquo; the format if they contain the delimiter Lack of support for metadata ","description":"A CSV file is a type of delimited text file that uses commas to separate values, with each line of the file being one data record. A record can have multiple fields, which are also separated by commas. Usually, a CSV files stores tabular data (numbers and text) in plain text form, meaning each line will include the same number of fields.\nExtension: .csv\nCSV Advantages Human readable/writeable Widely used/supported by most applications Can be read in a memory efficient way CSV Disadvantages Data is uncompressed Does not support binary data Unquoted fields can easily \u0026ldquo;break\u0026rdquo; the format if they contain the delimiter Lack of support for metadata "},{"id":33,"href":"/en/hub/Concepts/formats/Parquet/","title":"Apache Parquet","parent":"formats","content":" Apache Parquet is an open source data file format that was designed to improve performance when handling [[Column-oriented Database|column-oriented data]] in bulk. Apache Parquet is able to provide efficient compression and encoding schemes with enhanced performance due to its design. This makes it a common interchange format for both batch and interactive workloads, similar to other available columnar-storage file formats in Hadoop like RCFile and ORC.\nExtension: .parquet\nApache Parquet Official Documentation https://parquet.apache.org/docs/\nApache Parquet Advantages Reduces IO operations. Column-based format makes it more efficient in terms of storage space but also speeds up analytics queries. Highly efficient data compression and decompression. Support type-specific encoding. Supports several data types and nested data structures. Apache Parquet Disadvantages Not human readable (binary). More memory required to read data vs row-based format. Can be slower to write than row-based file formats because of the metadata overhead. ","description":"Apache Parquet is an open source data file format that was designed to improve performance when handling [[Column-oriented Database|column-oriented data]] in bulk. Apache Parquet is able to provide efficient compression and encoding schemes with enhanced performance due to its design. This makes it a common interchange format for both batch and interactive workloads, similar to other available columnar-storage file formats in Hadoop like RCFile and ORC.\nExtension: .parquet\nApache Parquet Official Documentation https://parquet."},{"id":34,"href":"/en/hub/Concepts/formats/Delta-Lake/","title":"Delta Lake","parent":"formats","content":" Delta Lake is an open-source storage framework that enables building a\nLakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.\nDelta Lake is essentially a metadata layer on top of Parquet.\nThe file layout looks like:\n![[Assets/delta-lake-file-format.png|500]]\nDelta Lake Official Documentation\nhttps://docs.delta.io/latest/index.html\nDelta Lake Advantages (over plain [[Apache Parquet|Parquet]]) ACID transactions with optimistic concurrency control. Efficient streaming I/O. Caching. Time travel. Data layout optimization, e.g. Z-ordering. Schema enforcement \u0026amp; evolution. UPSERT \u0026amp; MERGE statements. Audit logging. Delta Lake Disadvantages Same Parquet disadvantages. Maintenance processes are required to maintain its performance, e.g. OPTIMIZE. There is a learning curve when using advanced features, e.g. VACUUM. ","description":"Delta Lake is an open-source storage framework that enables building a\nLakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.\nDelta Lake is essentially a metadata layer on top of Parquet.\nThe file layout looks like:\n![[Assets/delta-lake-file-format.png|500]]\nDelta Lake Official Documentation\nhttps://docs.delta.io/latest/index.html\nDelta Lake Advantages (over plain [[Apache Parquet|Parquet]]) ACID transactions with optimistic concurrency control. Efficient streaming I/O."},{"id":35,"href":"/en/hub/Concepts/formats/Protocol-Buffers/","title":"Protocol Buffers","parent":"formats","content":" Protocol buffers provide a serialization format for packets of typed, structured data that are up to a few megabytes in size. The format is suitable for both ephemeral network traffic and long-term data storage. Protocol buffers can be extended with new information without invalidating existing data or requiring code to be updated. They are the most commonly-used data format at Google.\nExtension: .proto\nProtocol Buffers Official Documentation https://developers.google.com/protocol-buffers/docs/overview\nProtocol Buffers Advantages Compact data storage Fast parsing Available in several programming languages Protocol Buffers Disadvantages Not suitable for data larger than a few megabytes Messages are not compressed. You can compress them but sometimes special-purpose compression algorithms (JPEG, PNG) will produce more optimal results. Not optimal for scientific and engineering use cases involving multi-dimensional arrays of floating point numbers. ","description":"Protocol buffers provide a serialization format for packets of typed, structured data that are up to a few megabytes in size. The format is suitable for both ephemeral network traffic and long-term data storage. Protocol buffers can be extended with new information without invalidating existing data or requiring code to be updated. They are the most commonly-used data format at Google.\nExtension: .proto\nProtocol Buffers Official Documentation https://developers.google.com/protocol-buffers/docs/overview\nProtocol Buffers Advantages Compact data storage Fast parsing Available in several programming languages Protocol Buffers Disadvantages Not suitable for data larger than a few megabytes Messages are not compressed."},{"id":36,"href":"/en/hub/Data-Ingestion/dlt/","title":"dlt","parent":"Data Ingestion","content":"dlt is an open-source library that enables you to create a data pipeline in a Python script, replacing your glue code with robust, declarative 1-liners with schema evolution. dlt is the first Python library in this space, enabling you to run pipelines anywhere python runs - no redundant orchestrators, etc.\ndlt Official Documentation dlt Advantages Can be run in unprecedented places like Airflow, Cloud Functions, GitHub Actions, and more. Removes complexity by integrating with your existing stack. Empowers data teams and collaboration by allowing you to discover or prototype in notebooks, run in cloud functions, and deploy to production. Rapid data exploration and prototyping with DuckDB. No vendor limits, dlt is forever free. dlt Disadvantages As a relatively new tool, the community is still growing and there may be fewer learning resources available compared to more established tools. dlt Learning Resources https://dlthub.com/docs/getting-started\ndlt Recent Posts ","description":"dlt is an open-source library that enables you to create a data pipeline in a Python script, replacing your glue code with robust, declarative 1-liners with schema evolution. dlt is the first Python library in this space, enabling you to run pipelines anywhere python runs - no redundant orchestrators, etc.\ndlt Official Documentation dlt Advantages Can be run in unprecedented places like Airflow, Cloud Functions, GitHub Actions, and more. Removes complexity by integrating with your existing stack."},{"id":37,"href":"/en/hub/Data-Ingestion/Stitch-Data/","title":"Stitch Data","parent":"Data Ingestion","content":"Stitch is a cloud-first, open source platform for rapidly moving data. Many of the open-source connectors it uses are built on top of the singer protocol.\nStitch Data Official Documentation https://www.stitchdata.com/docs/\nStitch Data Advantages #placeholder\nStitch Data Disadvantages Most connectors are maintained by the community which can lead to instability Stitch Data Recent Posts ","description":"Stitch is a cloud-first, open source platform for rapidly moving data. Many of the open-source connectors it uses are built on top of the singer protocol.\nStitch Data Official Documentation https://www.stitchdata.com/docs/\nStitch Data Advantages #placeholder\nStitch Data Disadvantages Most connectors are maintained by the community which can lead to instability Stitch Data Recent Posts "},{"id":38,"href":"/en/hub/Data-Ingestion/Amazon-DMS/","title":"Amazon DMS","parent":"Data Ingestion","content":"Amazon Database Migration Service (DMS) is a tool used to replicate data from a source database and a supported destination (e.g. PostgreSQL, S3, Kinesis, etc.). It is marketed as a way to migrate on-prem databases to a database in the cloud but is also often used to perform ongoing [[Change Data Capture|change data capture]].\nAmazon DMS Official Documentation https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\nAmazon DMS Advantages Simple to use Supports several data sources and destinations Migrate data with minimal downtime Cost effective Amazon DMS Disadvantages A replication instance can\u0026rsquo;t be scaled up or down A replication instance can\u0026rsquo;t be stopped, only deleted Each source and destination have their own specific limitations (see docs) Amazon DMS Recent Posts ","description":"Amazon Database Migration Service (DMS) is a tool used to replicate data from a source database and a supported destination (e.g. PostgreSQL, S3, Kinesis, etc.). It is marketed as a way to migrate on-prem databases to a database in the cloud but is also often used to perform ongoing [[Change Data Capture|change data capture]].\nAmazon DMS Official Documentation https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\nAmazon DMS Advantages Simple to use Supports several data sources and destinations Migrate data with minimal downtime Cost effective Amazon DMS Disadvantages A replication instance can\u0026rsquo;t be scaled up or down A replication instance can\u0026rsquo;t be stopped, only deleted Each source and destination have their own specific limitations (see docs) Amazon DMS Recent Posts "},{"id":39,"href":"/en/hub/Data-Ingestion/Matillion-Data-Loader/","title":"Matillion Data Loader","parent":"Data Ingestion","content":"Matillion Data Loader is a low-code data ingestion tool made by Matillion. It allows you to create batch and CDC pipelines from their cloud-based UI and currently has 150+ connectors.\nMatillion Data Loader Official Documentation https://docs.matillion.com/\nMatillion Data Loader Advantages Can create custom connectors Matillion Data Loader Disadvantages #placeholder\nMatillion Data Loader Learning Resources https://academy.matillion.com/certifications\nMatillion Data Loader Recent Posts ","description":"Matillion Data Loader is a low-code data ingestion tool made by Matillion. It allows you to create batch and CDC pipelines from their cloud-based UI and currently has 150+ connectors.\nMatillion Data Loader Official Documentation https://docs.matillion.com/\nMatillion Data Loader Advantages Can create custom connectors Matillion Data Loader Disadvantages #placeholder\nMatillion Data Loader Learning Resources https://academy.matillion.com/certifications\nMatillion Data Loader Recent Posts "},{"id":40,"href":"/en/hub/Data-Ingestion/Airbyte/","title":"Airbyte","parent":"Data Ingestion","content":" Airbyte is an open-core data ingestion platform that boasts 300+ pre-built connectors. One of it\u0026rsquo;s focuses is on the \u0026ldquo;long tail\u0026rdquo; data sources and empowering data engineers to customize existing connectors. Many of the open-source connectors it uses are built on top of the singer protocol.\nAirbyte Official Documentation https://docs.airbyte.com/\nAirbyte Advantages Flexibility of self-hosting or using their cloud platform Easy to understand UI even for non-technical folks You can create a custom connector with their no-code connector builder or low-code CDK Large community and learning resources Airbyte Disadvantages Some users report Airbyte being slow for large amounts of data Most connectors are maintained by the community which can lead to instability Airbyte Learning Resources https://airbyte.com/tutorials\nAirbyte Recent Posts ","description":"Airbyte is an open-core data ingestion platform that boasts 300+ pre-built connectors. One of it\u0026rsquo;s focuses is on the \u0026ldquo;long tail\u0026rdquo; data sources and empowering data engineers to customize existing connectors. Many of the open-source connectors it uses are built on top of the singer protocol.\nAirbyte Official Documentation https://docs.airbyte.com/\nAirbyte Advantages Flexibility of self-hosting or using their cloud platform Easy to understand UI even for non-technical folks You can create a custom connector with their no-code connector builder or low-code CDK Large community and learning resources Airbyte Disadvantages Some users report Airbyte being slow for large amounts of data Most connectors are maintained by the community which can lead to instability Airbyte Learning Resources https://airbyte."},{"id":41,"href":"/en/hub/Data-Ingestion/Debezium/","title":"Debezium","parent":"Data Ingestion","content":"Debezium is an open-source log-based [[Change Data Capture|change data capture]] tool used for streaming changes from your database. It works by reading the [[Transaction Log|transaction log]] of your database to capture INSERT/UPDATE/DELETE events and propagates those events to a consumer (most commonly [[Apache Kafka]]).\nOfficial Documentation Debezium Advantages Captures changes in a way that has minimal impact on the source Changes can be captured with very low latency Debezium Disadvantages Debezium Learning Resources Recent Posts ","description":"Debezium is an open-source log-based [[Change Data Capture|change data capture]] tool used for streaming changes from your database. It works by reading the [[Transaction Log|transaction log]] of your database to capture INSERT/UPDATE/DELETE events and propagates those events to a consumer (most commonly [[Apache Kafka]]).\nOfficial Documentation Debezium Advantages Captures changes in a way that has minimal impact on the source Changes can be captured with very low latency Debezium Disadvantages Debezium Learning Resources Recent Posts "},{"id":42,"href":"/en/hub/Data-Ingestion/Fivetran/","title":"Fivetran","parent":"Data Ingestion","content":"Fivetran is a SaaS platform for ingesting and transforming data which they describe as a \u0026ldquo;data movement platform.\u0026rdquo; It is primarily used for ingesting data from a wide variety of data sources and you only pay for what you use.\nFivetran Official Documentation https://fivetran.com/docs/getting-started\nFivetran Advantages Generous free tier Only pay for data that has changed (they call this Monthly Active Rows or MAR) Simple to set up and use Dedicated support for connectors Integration with dbt Fivetran Disadvantages At large data volumes the usage-based pricing can be expensive For some data sources you must ingest all objects Fivetran Recent Posts ","description":"Fivetran is a SaaS platform for ingesting and transforming data which they describe as a \u0026ldquo;data movement platform.\u0026rdquo; It is primarily used for ingesting data from a wide variety of data sources and you only pay for what you use.\nFivetran Official Documentation https://fivetran.com/docs/getting-started\nFivetran Advantages Generous free tier Only pay for data that has changed (they call this Monthly Active Rows or MAR) Simple to set up and use Dedicated support for connectors Integration with dbt Fivetran Disadvantages At large data volumes the usage-based pricing can be expensive For some data sources you must ingest all objects Fivetran Recent Posts "},{"id":43,"href":"/en/hub/Data-Ingestion/Meltano/","title":"Meltano","parent":"Data Ingestion","content":" Meltano is an open-core data ingestion tool that uses a declarative code-first data integration engine. It allows you to declaratively define your data pipelines inside your codebase and use common DevOps workflows. Meltano\u0026rsquo;s open-source connectors are built on top of the singer protocol.\nMeltano Official Documentation Meltano Advantages Allows you to version control your data ingestion Can be fully configured from the CLI Can write your own connectors with their SDK Meltano Disadvantages Most connectors are maintained by the community which can lead to instability Meltano Learning Resources Meltano Recent Posts ","description":" Meltano is an open-core data ingestion tool that uses a declarative code-first data integration engine. It allows you to declaratively define your data pipelines inside your codebase and use common DevOps workflows. Meltano\u0026rsquo;s open-source connectors are built on top of the singer protocol.\nMeltano Official Documentation Meltano Advantages Allows you to version control your data ingestion Can be fully configured from the CLI Can write your own connectors with their SDK Meltano Disadvantages Most connectors are maintained by the community which can lead to instability Meltano Learning Resources Meltano Recent Posts "},{"id":44,"href":"/en/hub/Data-Analytics/Propel/","title":"Propel","parent":"Data Analytics","content":"Propel is an Analytics Backend as a Service platform. Essentially a serverless alternative to Clickhouse or Elasticsearch.\nPropel Official Documentation https://www.propeldata.com/docs\nPropel Advantages A managed serverless service. Ingest data and speed up queries from any data source. Low latency, typically in milliseconds. Build at any layer of the stack: SQL, APIs, or React UI Components. Semantic layer for data modeling with GraphQL APIs. Use cases Embedded analytics: build analytics products. Data APIs: Instant low-latency APIs over any data source. Data Sharing: Give your customers their data. Self-service custom reporting: Empower your customers to build their own reports. ","description":"Propel is an Analytics Backend as a Service platform. Essentially a serverless alternative to Clickhouse or Elasticsearch.\nPropel Official Documentation https://www.propeldata.com/docs\nPropel Advantages A managed serverless service. Ingest data and speed up queries from any data source. Low latency, typically in milliseconds. Build at any layer of the stack: SQL, APIs, or React UI Components. Semantic layer for data modeling with GraphQL APIs. Use cases Embedded analytics: build analytics products. Data APIs: Instant low-latency APIs over any data source."},{"id":45,"href":"/en/hub/Data-Analytics/Metricflow/","title":"Metricflow","parent":"Data Analytics","content":" Metricflow translates a simple metric definition into reusable SQL, and executes it against the SQL engine of your choice. This makes it easy to get consistent metric output broken down by attributes (dimensions) of interest. Metricflow was acquired by dbt labs in Feburary 2023 and now requires a dbt project in order to use it.\nMetricflow Official Documentation https://docs.getdbt.com/docs/build/build-metrics-intro\nMetricflow Advantages #placeholder/description\nMetricflow Disadvantages #placeholder/description\n","description":"Metricflow translates a simple metric definition into reusable SQL, and executes it against the SQL engine of your choice. This makes it easy to get consistent metric output broken down by attributes (dimensions) of interest. Metricflow was acquired by dbt labs in Feburary 2023 and now requires a dbt project in order to use it.\nMetricflow Official Documentation https://docs.getdbt.com/docs/build/build-metrics-intro\nMetricflow Advantages #placeholder/description\nMetricflow Disadvantages #placeholder/description"},{"id":46,"href":"/en/hub/Data-Analytics/Lightdash/","title":"Lightdash","parent":"Data Analytics","content":"Lightdash is an open-source BI tool that integrates with [[dbt]] metrics.\nLightdash Official Documentation https://docs.lightdash.com/\nLightdash Advantages #placeholder/description\nLightdash Disadvantages #placeholder/description\n","description":"Lightdash is an open-source BI tool that integrates with [[dbt]] metrics.\nLightdash Official Documentation https://docs.lightdash.com/\nLightdash Advantages #placeholder/description\nLightdash Disadvantages #placeholder/description"},{"id":47,"href":"/en/hub/Data-Analytics/Power-BI/","title":"Power BI","parent":"Data Analytics","content":"Microsoft\u0026rsquo;s Power BI is a unified, scalable platform for self-service and enterprise business intelligence (BI). Connect to and visualize any data, and seamlessly infuse the visuals into the apps you use every day.\nPower BI Official Documentation https://learn.microsoft.com/en-us/power-bi/\nPower BI Advantages #placeholder/description\nPower BI Disadvantages #placeholder/description\n","description":"Microsoft\u0026rsquo;s Power BI is a unified, scalable platform for self-service and enterprise business intelligence (BI). Connect to and visualize any data, and seamlessly infuse the visuals into the apps you use every day.\nPower BI Official Documentation https://learn.microsoft.com/en-us/power-bi/\nPower BI Advantages #placeholder/description\nPower BI Disadvantages #placeholder/description"},{"id":48,"href":"/en/hub/Data-Analytics/Metriql/","title":"Metriql","parent":"Data Analytics","content":" Metriql is an open-source [[Metrics Layer|metrics store]] which allows companies to define their metrics as code and share them across their BI and data tools easily. It uses [[data build tool|dbt]] for the transformation layer and integrates with dbt via its manifest.json artifact.\nMetriql Official Documentation https://metriql.com/introduction/intro\nMetriql Advantages #placeholder/description\nMetriql Disadvantages #placeholder/description\n","description":"Metriql is an open-source [[Metrics Layer|metrics store]] which allows companies to define their metrics as code and share them across their BI and data tools easily. It uses [[data build tool|dbt]] for the transformation layer and integrates with dbt via its manifest.json artifact.\nMetriql Official Documentation https://metriql.com/introduction/intro\nMetriql Advantages #placeholder/description\nMetriql Disadvantages #placeholder/description"},{"id":49,"href":"/en/hub/Data-Analytics/Apache-Superset/","title":"Apache Superset","parent":"Data Analytics","content":"Apache Superset is an open-source software application for data exploration and data visualization able to handle data at petabyte scale.\nApache Superset Official Documentation https://superset.apache.org/docs/intro\nApache Superset Advantages #placeholder/description\nApache Superset Disadvantages #placeholder/description\n","description":"Apache Superset is an open-source software application for data exploration and data visualization able to handle data at petabyte scale.\nApache Superset Official Documentation https://superset.apache.org/docs/intro\nApache Superset Advantages #placeholder/description\nApache Superset Disadvantages #placeholder/description"},{"id":50,"href":"/en/hub/Data-Process/AWS-Batch/","title":"AWS Batch","parent":"Data Process","content":"AWS Batch enables running hundreds of thousands of batch jobs that scale efficiently.\nAWS Batch Official Documentation https://aws.amazon.com/batch/\n","description":"AWS Batch enables running hundreds of thousands of batch jobs that scale efficiently.\nAWS Batch Official Documentation https://aws.amazon.com/batch/"},{"id":51,"href":"/en/hub/Data-Process/AWS-Fargate/","title":"AWS Fargate","parent":"Data Process","content":"Fargate enables serverless compute for container based applications.\nAWS Fargate Official Documentation https://aws.amazon.com/fargate/\n","description":"Fargate enables serverless compute for container based applications.\nAWS Fargate Official Documentation https://aws.amazon.com/fargate/"},{"id":52,"href":"/en/hub/Data-Process/Amazon-ECS/","title":"Amazon ECS","parent":"Data Process","content":"Amazon’s Elastic Container Service offers integrated docker deployments, allowing for seamless scalability and access to its powerful EKS engine as an option.\nAmazon ECS Official Documentation https://aws.amazon.com/ecs/\n","description":"Amazon’s Elastic Container Service offers integrated docker deployments, allowing for seamless scalability and access to its powerful EKS engine as an option.\nAmazon ECS Official Documentation https://aws.amazon.com/ecs/"},{"id":53,"href":"/en/hub/Data-Process/AWS-Lambda/","title":"AWS Lambda","parent":"Data Process","content":"AWS Lambda is a service that allows you to run code for various kinds of applications or backend services, without the need to provision or manage servers. It operates on a serverless, event-driven model.\nAWS Lambda Official Documentation https://aws.amazon.com/lambda\n","description":"AWS Lambda is a service that allows you to run code for various kinds of applications or backend services, without the need to provision or manage servers. It operates on a serverless, event-driven model.\nAWS Lambda Official Documentation https://aws.amazon.com/lambda"},{"id":54,"href":"/en/hub/Data-Process/Amazon-EC2/","title":"Amazon EC2","parent":"Data Process","content":"Amazon’s Elastic Compute Cloud offers secure and resizable compute instances in a variety of operating systems.\nAmazon EC2 Official Documentation https://aws.amazon.com/ec2/\n","description":"Amazon’s Elastic Compute Cloud offers secure and resizable compute instances in a variety of operating systems.\nAmazon EC2 Official Documentation https://aws.amazon.com/ec2/"},{"id":55,"href":"/en/hub/Data-Pipeline/Azure-Data-Factory/","title":"Azure Data Factory","parent":"Data Pipeline","content":"Azure Data Factory is Azure\u0026rsquo;s cloud ETL service for scale-out serverless data integration and data transformation. It offers a code-free UI for intuitive authoring and single-pane-of-glass monitoring and management. You can also lift and shift existing SSIS packages to Azure and run them with full compatibility in ADF. SSIS Integration Runtime offers a fully managed service, so you don\u0026rsquo;t have to worry about infrastructure management.\nAzure Data Factory Official Documentation https://learn.microsoft.com/en-us/azure/data-factory/\n","description":"Azure Data Factory is Azure\u0026rsquo;s cloud ETL service for scale-out serverless data integration and data transformation. It offers a code-free UI for intuitive authoring and single-pane-of-glass monitoring and management. You can also lift and shift existing SSIS packages to Azure and run them with full compatibility in ADF. SSIS Integration Runtime offers a fully managed service, so you don\u0026rsquo;t have to worry about infrastructure management.\nAzure Data Factory Official Documentation https://learn."},{"id":56,"href":"/en/hub/Data-Pipeline/Mage/","title":"Mage","parent":"Data Pipeline","content":"","description":""},{"id":57,"href":"/en/hub/Data-Pipeline/Dagster/","title":"Dagster","parent":"Data Pipeline","content":"Dagster is a next-generation open source orchestration platform for the development, production, and observation of data assets.\nDagster Official Documentation https://docs.dagster.io/getting-started\n","description":"Dagster is a next-generation open source orchestration platform for the development, production, and observation of data assets.\nDagster Official Documentation https://docs.dagster.io/getting-started"},{"id":58,"href":"/en/hub/Data-Pipeline/Prefect/","title":"Prefect","parent":"Data Pipeline","content":"Prefect is an open-source framework that allows you to design, test, run, and monitor your data pipelines with Python. Prefect also offers it\u0026rsquo;s framework as a service called Prefect Cloud.\nPrefect Official Documentation Prefect Advantages Easy to test and run workflows locally Minimal scheduling latency Clean intuitive UI Prefect Disadvantages Newer/smaller community \u0026amp; fewer learning resources Prefect Learning Resources https://docs.prefect.io/core/getting_started/quick-start.html\nPrefect Recent Posts ","description":"Prefect is an open-source framework that allows you to design, test, run, and monitor your data pipelines with Python. Prefect also offers it\u0026rsquo;s framework as a service called Prefect Cloud.\nPrefect Official Documentation Prefect Advantages Easy to test and run workflows locally Minimal scheduling latency Clean intuitive UI Prefect Disadvantages Newer/smaller community \u0026amp; fewer learning resources Prefect Learning Resources https://docs.prefect.io/core/getting_started/quick-start.html\nPrefect Recent Posts "},{"id":59,"href":"/en/hub/Data-Pipeline/AWS-Step-Functions/","title":"AWS Step Functions","parent":"Data Pipeline","content":"AWS Step Functions is a visual workflow tool to allow users to drag and drop components to express business logic.\nAWS Step Functions Official Documentation https://aws.amazon.com/step-functions/\n","description":"AWS Step Functions is a visual workflow tool to allow users to drag and drop components to express business logic.\nAWS Step Functions Official Documentation https://aws.amazon.com/step-functions/"},{"id":60,"href":"/en/hub/Data-Pipeline/Apache-Airflow/","title":"Apache Airflow","parent":"Data Pipeline","content":"![[Assets/apache-airflow-logo.png|100]]\nApache Airflow is an open-source platform used to programmatically develop, schedule, and orchestrate [[Batch Data Processing|batch workflows]]. It is most commonly used to schedule Python and SQL scripts but is flexible enough to schedule any other type of script you might use to build your data pipeline. There are also several companies that offer Airflow as a service if you don\u0026rsquo;t want to manage the infrastructure yourself.\nOfficial Documentation https://airflow.apache.org/docs/\nLearning Resources https://airflow.apache.org/ecosystem/#learning-resources\nRecent Posts ","description":"![[Assets/apache-airflow-logo.png|100]]\nApache Airflow is an open-source platform used to programmatically develop, schedule, and orchestrate [[Batch Data Processing|batch workflows]]. It is most commonly used to schedule Python and SQL scripts but is flexible enough to schedule any other type of script you might use to build your data pipeline. There are also several companies that offer Airflow as a service if you don\u0026rsquo;t want to manage the infrastructure yourself.\nOfficial Documentation https://airflow."},{"id":61,"href":"/en/hub/Data-Pipeline/Amazon-MWAA/","title":"Amazon MWAA","parent":"Data Pipeline","content":"Amazon Managed Workflows for Apache Airflow is a serverless managed workflow orchestration tool to allow the scaling of [[Airflow]]\nAmazon MWAA Official Documentation https://aws.amazon.com/managed-workflows-for-apache-airflow/\n","description":"Amazon Managed Workflows for Apache Airflow is a serverless managed workflow orchestration tool to allow the scaling of [[Airflow]]\nAmazon MWAA Official Documentation https://aws.amazon.com/managed-workflows-for-apache-airflow/"},{"id":62,"href":"/en/hub/Data-Storage/Amazon-S3-Glacier/","title":"Amazon S3 Glacier","parent":"Data Stroage","content":"Glacier is a related service to S3 where data is archived. With extremely cheap data storage, Glacier’s main downside is that its cost-optimized for infrequent data access.\nAmazon S3 Glacier Official Documentation https://aws.amazon.com/s3/storage-classes/glacier/\n","description":"Glacier is a related service to S3 where data is archived. With extremely cheap data storage, Glacier’s main downside is that its cost-optimized for infrequent data access.\nAmazon S3 Glacier Official Documentation https://aws.amazon.com/s3/storage-classes/glacier/"},{"id":63,"href":"/en/hub/Data-Storage/Amazon-S3/","title":"Amazon S3","parent":"Data Stroage","content":"S3 is a blob storage platform that is different in nature than standard flat file system storage. By treating individual files as objects, S3 offers a way to securely and reliably store large quantities of data.\nAmazon S3 Official Documentation https://aws.amazon.com/s3/\n","description":"S3 is a blob storage platform that is different in nature than standard flat file system storage. By treating individual files as objects, S3 offers a way to securely and reliably store large quantities of data.\nAmazon S3 Official Documentation https://aws.amazon.com/s3/"},{"id":64,"href":"/en/hub/Data-Process/Apache-Hadoop/","title":"Apache Hadoop","parent":"Data Process","content":"![[Assets/hadoop_logo.jpg|100]]\nApache Hadoop is a data processing framework designed to batch process big amounts of data.\nOfficial Documentation https://hadoop.apache.org/docs/current/\nApache Hadoop Advantages MapReduce function Divide and Conquer strategy to deal with data HDFS system to store data Apache Hadoop Disadvantages Hadoop works on disk, which makes it slower than memory Low efficiency on small files High latency HDFS HDFS or Hadoop Distributed File System is a native tool on Hadoop that let us store structured and non-structured data on a local cluster. Although HDFS is the main option and is the one Apache Spark uses as well, Hadoop offers other tools such as HFTP, HSFTP, WebHDFS, and Amazon S3.\nHow does Hadoop work? Apache Hadoop is settled upon a Leader-Follower system. A Leader node, also known as NameNode in HDFS, will be responsible for creating tasks and sending information to Follower nodes through the information nodes, known as DataNodes.\nOnce the name node and the data nodes are configured, the Leader will set a job-tracker. This job-tracker will have control over the tasks using task-trackers on the Follower nodes. This is done to prevent Followers from trying to complete every task, that is, job-trackers indicate exactly which tasks must be done on each follower node as well as which information every follower node needs to fetch from the data nodes.\nThis way of working, the Divide and Conquer system, is also what defines MapReduce on Hadoop. Tasks get divided to every follower node, which improves data processing speed on real large datasets. MapReduce consists of two functions:\nMap: Map will read important information in the HDFS environment, analyzing key-value pairs and sending them to the datanodes. Reduce: Reduce will retrieve all pairs and group them by its key to produce the final pairs. Once this finishes, the data will be stored in a datanode. When the MapReduce function is finished, the leader will be informed.\n![[Assets/mapreduce.png]]\nWhen to use Hadoop? Use it if\u0026hellip; Don\u0026rsquo;t use it if\u0026hellip; You are working with tasks that can be divided on side jobs You are working with serial tasks or low latency tasks ","description":"![[Assets/hadoop_logo.jpg|100]]\nApache Hadoop is a data processing framework designed to batch process big amounts of data.\nOfficial Documentation https://hadoop.apache.org/docs/current/\nApache Hadoop Advantages MapReduce function Divide and Conquer strategy to deal with data HDFS system to store data Apache Hadoop Disadvantages Hadoop works on disk, which makes it slower than memory Low efficiency on small files High latency HDFS HDFS or Hadoop Distributed File System is a native tool on Hadoop that let us store structured and non-structured data on a local cluster."},{"id":65,"href":"/en/hub/Data-Process/Apache-Spark/","title":"Apache Spark","parent":"Data Process","content":"![[Assets/apache_spark_logo.png|100]]\nApache Spark is a data processing engine used in Data Engineering primarily for large scale data processing.\nOfficial Documentation https://spark.apache.org/docs/latest/\nApache Spark Advantages High speed data querying, analysis, and transformation with large data sets Easy to use APIs Support for multiple programming languages Apache Spark Disadvantages Spark lacks a native storage option Wrong usage of RDD partitions on Spark Context can cause negative effects over HDFS and MemoryOverhead driver Apache Spark storage Apache Spark is compatible with Hadoop APIs, like HDFS. Spark also works with other storage services such as NoSQL databases, ElasticSearch and Amazon S3.\nApache Spark model Apache Spark\u0026rsquo;s programming model is based on parallel operators. This is Spark\u0026rsquo;s main advantage and feature, which let us use a Leader-Follower strategy to tackle data.\nSpark describes tasks based on a DAG (Directed Acyclic Graphs), it gives programmers the option of developing complex pipelines. To understand what DAG does, we must focus on its name:\nDirected: stands for a one way process Acyclic: means that there are no loops on the tasks Graph: is a reference on how they can be displayed as an actual graph A DAG system is also fault-tolerant.\nApache Spark RDDs RDDs (Resilient Distributed Datasets) are the main Spark abstraction. They consist of element sets that are fault tolerance and able to be parallel processed. RDDs also provide scalability due to being distributed processes as well as being immutable.\nRDDs emit two kind of operations: transformations (such as filter, map\u0026hellip;) and actions (such as reduce, collect\u0026hellip;). Spark RDDs tend to perform lazy evaluation, which improves efficiency by executing operations only when they are needed.\nApache Spark Learning Resources Apache Spark Recent Posts ","description":"![[Assets/apache_spark_logo.png|100]]\nApache Spark is a data processing engine used in Data Engineering primarily for large scale data processing.\nOfficial Documentation https://spark.apache.org/docs/latest/\nApache Spark Advantages High speed data querying, analysis, and transformation with large data sets Easy to use APIs Support for multiple programming languages Apache Spark Disadvantages Spark lacks a native storage option Wrong usage of RDD partitions on Spark Context can cause negative effects over HDFS and MemoryOverhead driver Apache Spark storage Apache Spark is compatible with Hadoop APIs, like HDFS."},{"id":66,"href":"/en/hub/Data-Process/AWS-Glue/","title":"AWS Glue","parent":"Data Process","content":"Glue and its relative Data Brew are services that revolve around the creation of data catalogues and crawlers to index databases and files stored on S3. Commonly used with Amazon Athena, Glue is Amazon’s most popular ETL tool.\nAWS Glue Official Documentation https://aws.amazon.com/glue/\n","description":"Glue and its relative Data Brew are services that revolve around the creation of data catalogues and crawlers to index databases and files stored on S3. Commonly used with Amazon Athena, Glue is Amazon’s most popular ETL tool.\nAWS Glue Official Documentation https://aws.amazon.com/glue/"},{"id":67,"href":"/en/hub/Data-Process/Amazon-EMR/","title":"Amazon EMR","parent":"Data Process","content":"EMR allows for running and scaling big data workloads using a variety of different tools like Spark, Hive, etc.\nAmazon EMR Official Documentation https://aws.amazon.com/emr/\n","description":"EMR allows for running and scaling big data workloads using a variety of different tools like Spark, Hive, etc.\nAmazon EMR Official Documentation https://aws.amazon.com/emr/"},{"id":68,"href":"/en/hub/Data-Process/Databricks/","title":"Databricks","parent":"Data Process","content":"Databricks is a Data Architecture Platform used for Data analytics, Data Engineering, and Data Science work.\nDatabricks Official Documentation https://docs.databricks.com/ https://learn.microsoft.com/en-us/azure/databricks/ https://docs.gcp.databricks.com/\nDatabricks Advantages Uses Jupyter Notebooks Can use Python, Scala, R or/and SQL Enhances collaboration through notebook comments, history, and realtime editing. Ability to stand up compute based on specific needs. Use of clustering through Apache Spark. Databricks Disadvantages Learning curve due to size of platform and available tools. ","description":"Databricks is a Data Architecture Platform used for Data analytics, Data Engineering, and Data Science work.\nDatabricks Official Documentation https://docs.databricks.com/ https://learn.microsoft.com/en-us/azure/databricks/ https://docs.gcp.databricks.com/\nDatabricks Advantages Uses Jupyter Notebooks Can use Python, Scala, R or/and SQL Enhances collaboration through notebook comments, history, and realtime editing. Ability to stand up compute based on specific needs. Use of clustering through Apache Spark. Databricks Disadvantages Learning curve due to size of platform and available tools. "},{"id":69,"href":"/en/hub/Databases/relational-database/Amazon-Aurora/","title":"Amazon Aurora","parent":"relational database","content":"Amazon Aurora is an AWS managed relational database engine compatible with MySQL and Postgres. Commonly used in conjunction with Glue, Aurora is part of AWS\u0026rsquo; \u0026ldquo;serverless\u0026rdquo; offerings.\nAurora is a part of [[Amazon RDS|AWS RDS]].\nAmazon Aurora Official Documentation https://aws.amazon.com/rds/aurora/features/\n","description":"Amazon Aurora is an AWS managed relational database engine compatible with MySQL and Postgres. Commonly used in conjunction with Glue, Aurora is part of AWS\u0026rsquo; \u0026ldquo;serverless\u0026rdquo; offerings.\nAurora is a part of [[Amazon RDS|AWS RDS]].\nAmazon Aurora Official Documentation https://aws.amazon.com/rds/aurora/features/"},{"id":70,"href":"/en/hub/Databases/OLAP-database/Azure-Synapse-Analytics/","title":"Azure Synapse Analytics","parent":"OLAP database","content":"Azure Synapse Analytics is Microsoft\u0026rsquo;s [[Data Warehouse|enterprise data warehouse]] offering. It offers serverless and dedicated options and is designed for big data analytics and machine learning.\nAzure Synapse Analytics Official Documentation https://learn.microsoft.com/en-us/azure/synapse-analytics/overview-what-is\nAzure Synapse Analytics Learning Resources https://azure.microsoft.com/en-us/resources/developers/synapse-analytics-for-data-engineers/#overview\n","description":"Azure Synapse Analytics is Microsoft\u0026rsquo;s [[Data Warehouse|enterprise data warehouse]] offering. It offers serverless and dedicated options and is designed for big data analytics and machine learning.\nAzure Synapse Analytics Official Documentation https://learn.microsoft.com/en-us/azure/synapse-analytics/overview-what-is\nAzure Synapse Analytics Learning Resources https://azure.microsoft.com/en-us/resources/developers/synapse-analytics-for-data-engineers/#overview"},{"id":71,"href":"/en/hub/Databases/column-database/Couchbase/","title":"Couchbase","parent":"column database","content":"Couchbase is the highest performing NoSQL distributed database.\n","description":"Couchbase is the highest performing NoSQL distributed database."},{"id":72,"href":"/en/hub/Databases/k-v-database/","title":"key-value database","parent":"Databases","content":"","description":""},{"id":73,"href":"/en/hub/Databases/OLAP-database/","title":"OLAP database","parent":"Databases","content":"","description":""},{"id":74,"href":"/en/hub/Databases/others/","title":"others","parent":"Databases","content":"","description":""},{"id":75,"href":"/en/hub/Databases/OLAP-database/Amazon-Redshift/","title":"Amazon Redshift","parent":"OLAP database","content":"Amazon Redshift is an [[Amazon Web Services|AWS]] cloud-based [[Data Warehouse|data warehouse]] service based on [[PostgreSQL]].\nDeveloper Documentation ","description":"Amazon Redshift is an [[Amazon Web Services|AWS]] cloud-based [[Data Warehouse|data warehouse]] service based on [[PostgreSQL]].\nDeveloper Documentation "},{"id":76,"href":"/en/hub/Databases/k-v-database/Redis/","title":"Redis","parent":"key-value database","content":"![[Assets/redis_logo.png|100]]\nPopular in-memory data platform used as a cache, message broker, and database that can be deployed on-premises, across clouds, and hybrid environments.\nRedis Official Documentation https://redis.io/docs/\nRedis Recent Posts ","description":"![[Assets/redis_logo.png|100]]\nPopular in-memory data platform used as a cache, message broker, and database that can be deployed on-premises, across clouds, and hybrid environments.\nRedis Official Documentation https://redis.io/docs/\nRedis Recent Posts "},{"id":77,"href":"/en/hub/Databases/document-database/Amazon-DocumentDB/","title":"Amazon DocumentDB","parent":"document database","content":"Amazon DocumentDB is a fully managed JSON document database built around developing applications.\nAmazon DocumentDB Official Documentation https://aws.amazon.com/documentdb/\n","description":"Amazon DocumentDB is a fully managed JSON document database built around developing applications.\nAmazon DocumentDB Official Documentation https://aws.amazon.com/documentdb/"},{"id":78,"href":"/en/hub/Databases/others/ClickHouse/","title":"ClickHouse","parent":"others","content":" ![[Assets/clickhouse_logo.png|100]]\nClickHouse is a column-oriented database management system (DBMS) for online analytical processing of queries ([[Online Analytical Processing|OLAP]]). It can process billions of rows and tens of gigabytes of data per server per second.\nClickHouse Official Documentation https://clickhouse.com/docs\nClickHouse Advantages #placeholder/description\nClickHouse Disadvantages #placeholder/description\n","description":"![[Assets/clickhouse_logo.png|100]]\nClickHouse is a column-oriented database management system (DBMS) for online analytical processing of queries ([[Online Analytical Processing|OLAP]]). It can process billions of rows and tens of gigabytes of data per server per second.\nClickHouse Official Documentation https://clickhouse.com/docs\nClickHouse Advantages #placeholder/description\nClickHouse Disadvantages #placeholder/description"},{"id":79,"href":"/en/hub/Databases/OLAP-database/Google-BigQuery/","title":"Google BigQuery","parent":"OLAP database","content":"![[Assets/google_bigquery_logo.png|100]]\nGoogle BigQuery is an analytics data warehouse that Google Cloud offers. It lets you run complex queries quickly over large data sets, and there\u0026rsquo;s no need to set up or manage any infrastructure.\nGoogle BigQuery Official Documentation https://cloud.google.com/bigquery/docs\nGoogle BigQuery Learning Resources https://cloud.google.com/bigquery/docs/quickstarts https://www.youtube.com/playlist?list=PLIivdWyY5sqIZLeLzyg1B-Pd1MIOo6d-g\n","description":"![[Assets/google_bigquery_logo.png|100]]\nGoogle BigQuery is an analytics data warehouse that Google Cloud offers. It lets you run complex queries quickly over large data sets, and there\u0026rsquo;s no need to set up or manage any infrastructure.\nGoogle BigQuery Official Documentation https://cloud.google.com/bigquery/docs\nGoogle BigQuery Learning Resources https://cloud.google.com/bigquery/docs/quickstarts https://www.youtube.com/playlist?list=PLIivdWyY5sqIZLeLzyg1B-Pd1MIOo6d-g"},{"id":80,"href":"/en/hub/Databases/relational-database/Amazon-RDS/","title":"Amazon RDS","parent":"relational database","content":"Amazon RDS (Relational Database Service) is a collection of AWS managed services. Featuring several different popular relational database engines, including Aurora, Postgres, and MySQL, RDS is a popular choice for spinning up databases.\nOne of the key features of RDS is that it is a cloud distributed product. This simplifies the setup of the databases and allows for easier scalability. There are a few features in particular that RDS is known for:\nRead Replicas Read replicas create multiple “replica” copies of data. This allows high-volume applications to have enhanced performance for read-heavy operations. Read replica can be turned into standalone database instances, and elastically scale.\nMulti-AZ AWS is well known for their availability zone architecture, which separates traffic from physical geographic regions. Multi-AZ settings allow for the creation of standby replicas in different regions to allow for enhanced performance.\nAmazon RDS Official Documentation https://aws.amazon.com/rds/\n","description":"Amazon RDS (Relational Database Service) is a collection of AWS managed services. Featuring several different popular relational database engines, including Aurora, Postgres, and MySQL, RDS is a popular choice for spinning up databases.\nOne of the key features of RDS is that it is a cloud distributed product. This simplifies the setup of the databases and allows for easier scalability. There are a few features in particular that RDS is known for:"},{"id":81,"href":"/en/hub/Databases/relational-database/MySQL/","title":"MySQL","parent":"relational database","content":"![[Assets/mysql_logo.png|100]]\nMySQL is an open-source [[Relational Database Management System]] who\u0026rsquo;s name comes from a combination of \u0026ldquo;My\u0026rdquo; the name of co-founder Michael Widenius\u0026rsquo;s daughter, and \u0026ldquo;[[SQL]]\u0026rdquo;. When Oracle acquired the company that owned MySQL, Widenius created a copy of it and built on it to create [[MariaDB]]. MySQL is a very popular database which today is commonly used in web applications.\n","description":"![[Assets/mysql_logo.png|100]]\nMySQL is an open-source [[Relational Database Management System]] who\u0026rsquo;s name comes from a combination of \u0026ldquo;My\u0026rdquo; the name of co-founder Michael Widenius\u0026rsquo;s daughter, and \u0026ldquo;[[SQL]]\u0026rdquo;. When Oracle acquired the company that owned MySQL, Widenius created a copy of it and built on it to create [[MariaDB]]. MySQL is a very popular database which today is commonly used in web applications."},{"id":82,"href":"/en/hub/Databases/document-database/MongoDB/","title":"MongoDB","parent":"document database","content":"MongoDB is an open-source, document database designed for ease of development and scaling.\n","description":"MongoDB is an open-source, document database designed for ease of development and scaling."},{"id":83,"href":"/en/hub/Databases/relational-database/PostgreSQL/","title":"PostgreSQL","parent":"relational database","content":"![[Assets/postgresql_logo.png|100]]\nPostgreSQL or Postgres as it is commonly referred to is an open-source [[Relational Database Management System]] with over 30 years of active development. It has a strong reputation for reliability, feature robustness, and performance.\nLearning Resources https://pgexercises.com/\n","description":"![[Assets/postgresql_logo.png|100]]\nPostgreSQL or Postgres as it is commonly referred to is an open-source [[Relational Database Management System]] with over 30 years of active development. It has a strong reputation for reliability, feature robustness, and performance.\nLearning Resources https://pgexercises.com/"},{"id":84,"href":"/en/hub/Databases/relational-database/Microsoft-SQL-Server/","title":"Microsoft SQL Server","parent":"relational database","content":"Microsoft SQL Server is a [[Relational Database Management System]] developed by Microsoft. Microsoft currently supports several versions of SQL Server that target different business applications.\n","description":"Microsoft SQL Server is a [[Relational Database Management System]] developed by Microsoft. Microsoft currently supports several versions of SQL Server that target different business applications."},{"id":85,"href":"/en/hub/Databases/document-database/Amazon-DynamoDB/","title":"Amazon DynamoDB","parent":"document database","content":"DynamoDB is a fast and flexible NoSQL database service for any scale\n","description":"DynamoDB is a fast and flexible NoSQL database service for any scale"},{"id":86,"href":"/en/hub/Data-Warehouse/Data-Mart/","title":"Data Mart","parent":"Data Warehouse","content":"A data mart is a subject-specific database which acts as a partitioned segment of an enterprise data warehouse. The domain of a data mart aligns with a particular business unit - for instance there would be separate data marts created for finance, marketing, or supply chain departments within a company.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A((Data Warehouse)) A --\u0026gt; B[Data Mart A] A --\u0026gt; C[Data Mart B] A --\u0026gt; D[Data Mart C] Data Mart Advantages In general, better performance due to querying only a subset of data rather than a whole data warehouse. Should also require less resources and maintenance than a monolithic data warehouse. The domain focus means data marts should be more flexible and grant bigger user empowerment and encouragement. Data Mart Disadvantages Data quality risk - the risk of arising discrepancies between a data mart and the original data warehouse. Implementation challenges - poor design may lead to extensive complexity and inconsistencies over time. Data Mart Learning Resources What is a Data Mart? - Data Mart Explained - AWS (amazon.com)\n","description":"A data mart is a subject-specific database which acts as a partitioned segment of an enterprise data warehouse. The domain of a data mart aligns with a particular business unit - for instance there would be separate data marts created for finance, marketing, or supply chain departments within a company.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A((Data Warehouse)) A --\u0026gt; B[Data Mart A] A --\u0026gt; C[Data Mart B] A --\u0026gt; D[Data Mart C] Data Mart Advantages In general, better performance due to querying only a subset of data rather than a whole data warehouse."},{"id":87,"href":"/en/hub/Architecture/Activity-Schema/","title":"Activity Schema","parent":"architecture","content":"Created by Ahmed Elsamadisi, the activity schema is a standard that is designed to make data modeling and analysis substantially simpler, faster, and more reliable by modeling all data as a single time series table, which makes it easier to answer any data question using a single query pattern.\nBusiness concepts are represented as entity doing an activity (\u0026lsquo;a customer completed an order\u0026rsquo;) instead of facts or nouns (orders, products). Activities are built directly from source tables, store only their own data, and are the single source of truth for each concept.\nAll queries run against an activity stream table to assemble data for analysis, BI, and reporting. Instead of traditional foreign key joins, queries combine activities using relationships in time (e.g. all customers who completed an order and submitted a support ticket before their next completed order).\n![[Assets/activity-schema.png]]\nActivity Schema Advantages Simple to build, query, and maintain (only one table). Can improve query performance because no joins are required. Activity Schema Disadvantages Requires data platform to support JSON columns and for data analysts to be comfortable with SQL/JSON. Keeping all data in one table (even PII or financial data) may not be desirable for security reasons. Some BI tools might not be able to load a large activity schema table. Sources:\nhttps://www.activityschema.com/ https://github.com/ActivitySchema/ActivitySchema ","description":"Created by Ahmed Elsamadisi, the activity schema is a standard that is designed to make data modeling and analysis substantially simpler, faster, and more reliable by modeling all data as a single time series table, which makes it easier to answer any data question using a single query pattern.\nBusiness concepts are represented as entity doing an activity (\u0026lsquo;a customer completed an order\u0026rsquo;) instead of facts or nouns (orders, products). Activities are built directly from source tables, store only their own data, and are the single source of truth for each concept."},{"id":88,"href":"/en/hub/Databases/relational-database/Relational-Database-Management-System/","title":"Relational Database Management System","parent":"relational database","content":"A relational database management system (RDBMS) is software that allows you to create and manage a relational database. Most relational database management systems use [[SQL]] to access and modify the database. They also usually provide a visual representation of the data.\n","description":"A relational database management system (RDBMS) is software that allows you to create and manage a relational database. Most relational database management systems use [[SQL]] to access and modify the database. They also usually provide a visual representation of the data."},{"id":89,"href":"/en/hub/Concepts/concepts/Data-Mesh/","title":"Data Mesh","parent":"concepts","content":"Data Mesh is an analytical data architecture and operating model where data is treated as a product, leveraging a domain-driven and self-serve design.\nThe four principles of Data Mesh Domain Ownership: Arranging Data in Domains and declaring full end-to-end ownership Data as a Product: Applying Product-thinking to Data Assets and bridging the gap between Producers and Consumers Self-serve Data Platform: Removing the intricacies of Infrastructure provisioning to enable domain autonomy and shorten lifecycles Federated Computational Governance: Seeking interoperability through global standardization Data Mesh Advantages Better Data Governance Improved Data Quality Data Products are built thinking on the consumer\u0026rsquo;s needs first Fine-grained Access by Domain Data Mesh Disadvantages The decentralization of data is challenging. It requires changes not only technically, but also at organizational and mindset levels. ","description":"Data Mesh is an analytical data architecture and operating model where data is treated as a product, leveraging a domain-driven and self-serve design.\nThe four principles of Data Mesh Domain Ownership: Arranging Data in Domains and declaring full end-to-end ownership Data as a Product: Applying Product-thinking to Data Assets and bridging the gap between Producers and Consumers Self-serve Data Platform: Removing the intricacies of Infrastructure provisioning to enable domain autonomy and shorten lifecycles Federated Computational Governance: Seeking interoperability through global standardization Data Mesh Advantages Better Data Governance Improved Data Quality Data Products are built thinking on the consumer\u0026rsquo;s needs first Fine-grained Access by Domain Data Mesh Disadvantages The decentralization of data is challenging."},{"id":90,"href":"/en/hub/Concepts/concepts/Data-Governance/","title":"Data Governance","parent":"concepts","content":"Data Governance is a term used to describe the set of policies and procedures that ensure the data used in an organization is of high quality throughout its lifecycle (input, storage, transformation, access, and deletion).\n","description":"Data Governance is a term used to describe the set of policies and procedures that ensure the data used in an organization is of high quality throughout its lifecycle (input, storage, transformation, access, and deletion)."},{"id":91,"href":"/en/hub/Concepts/concepts/Hybrid-Transactional-Analytical-Processing/","title":"Hybrid Transactional Analytical Processing","parent":"concepts","content":"Brief description of the concept.\nHybrid Transactional Analytical Processing Advantages Hybrid Transactional Analytical Processing Disadvantages ","description":"Brief description of the concept.\nHybrid Transactional Analytical Processing Advantages Hybrid Transactional Analytical Processing Disadvantages "},{"id":92,"href":"/en/hub/Concepts/concepts/Data-Catalog/","title":"Data Catalog","parent":"concepts","content":"A data catalog is a collection of all data assets of an organization which uses metadata along with data management to help business users find data for their use case. A data catalog covers the discoverability feature of a database. It can also help build the lineage of the data.\nSome examples of data catalog tools are:\nAlation Ataccama A data catalog can help solve the below problems:\nData security Data redundancy Data reproducibility Data steward Data staleness checks Frequency of refresh ","description":"A data catalog is a collection of all data assets of an organization which uses metadata along with data management to help business users find data for their use case. A data catalog covers the discoverability feature of a database. It can also help build the lineage of the data.\nSome examples of data catalog tools are:\nAlation Ataccama A data catalog can help solve the below problems:\nData security Data redundancy Data reproducibility Data steward Data staleness checks Frequency of refresh "},{"id":93,"href":"/en/hub/Concepts/concepts/Database-Sharding/","title":"Database Sharding","parent":"concepts","content":"Brief description of the concept.\nSharding Advantages Sharding Disadvantages ","description":"Brief description of the concept.\nSharding Advantages Sharding Disadvantages "},{"id":94,"href":"/en/hub/Concepts/concepts/One-Big-Table/","title":"One Big Table","parent":"concepts","content":"The main idea behind one big table (OBT) is to join all of the data necessary for analytics into wide [[Denormalization|denormalized]] tables. One big table is a popular approach to serving analytics at larger scales and takes advantage of the benefits of [[Column-oriented Database|columnar databases]]. It\u0026rsquo;s usually combined with/built on top of other techniques such as a [[Dimensional Modeling|Dimensional Model]] or [[Data Vault Modeling|Data Vault]].\n[!tip] You can use OBT when starting new data warehouse to provide value immediately while another longer-term approach is worked on like a [[Dimensional Modeling|Dimensional Model]] or a [[Data Vault Modeling|Data Vault]].\nOne Big Table Advantages Increases query performance by removing the need for joins. Simple to query. One Big Table Disadvantages Very wide tables can get messy to read and maintain. Does not adapt to changes well. Adding new data sources will require rebuilding the table. ","description":"The main idea behind one big table (OBT) is to join all of the data necessary for analytics into wide [[Denormalization|denormalized]] tables. One big table is a popular approach to serving analytics at larger scales and takes advantage of the benefits of [[Column-oriented Database|columnar databases]]. It\u0026rsquo;s usually combined with/built on top of other techniques such as a [[Dimensional Modeling|Dimensional Model]] or [[Data Vault Modeling|Data Vault]].\n[!tip] You can use OBT when starting new data warehouse to provide value immediately while another longer-term approach is worked on like a [[Dimensional Modeling|Dimensional Model]] or a [[Data Vault Modeling|Data Vault]]."},{"id":95,"href":"/en/hub/Concepts/concepts/Workflow-Orchestration/","title":"Workflow Orchestration","parent":"concepts","content":"In the context of Data Engineering, workflow orchestration refers to the process of scheduling and arranging tasks that form your [[Data Pipeline|data pipeline]]. A workflow orchestration tool allows you to schedule, run, and observe the entire process.\nPopular Workflow Orchestration Tools [[Apache Airflow]] [[Dagster]] [[Prefect]]\nWorkflow Orchestration Advantages Create complex custom workflows Makes it easier to create [[Idempotence|idempotent]] workflows Alert you if something fails Allows you to gracefully retry and recover from failures Workflow Orchestration Disadvantages Adds complexity in scheduling Requires additional infrastructure and maintenance costs ","description":"In the context of Data Engineering, workflow orchestration refers to the process of scheduling and arranging tasks that form your [[Data Pipeline|data pipeline]]. A workflow orchestration tool allows you to schedule, run, and observe the entire process.\nPopular Workflow Orchestration Tools [[Apache Airflow]] [[Dagster]] [[Prefect]]\nWorkflow Orchestration Advantages Create complex custom workflows Makes it easier to create [[Idempotence|idempotent]] workflows Alert you if something fails Allows you to gracefully retry and recover from failures Workflow Orchestration Disadvantages Adds complexity in scheduling Requires additional infrastructure and maintenance costs "},{"id":96,"href":"/en/hub/Concepts/concepts/Data-Vault-Modeling/","title":"Data Vault Modeling","parent":"concepts","content":"Developed by Dan Linstedt, data vault modeling aims to be the most flexible modeling technique, adapting to changes and new datasets easily while storing all historical data by default. There are 3 core types of tables in data vault: hubs, links, and satellites.\nHubs: tables that contain a list of unique business keys (natural keys), surrogate keys, and metadata describing the data source for each hub item. Links: tables that associate hubs and satellites via the business key. Satellites: tables that hold the descriptive data about the entities being modeled as well as start and end date columns to track historical changes. Data Vault Modeling Advantages Tracks historical changes by default (good for auditing/tracing) Extremely resilient to changing data Enables parallel loading Data Vault Modeling Disadvantages Considered an advanced technique that requires more experience to implement Querying data vault is more complex compared to other techniques ","description":"Developed by Dan Linstedt, data vault modeling aims to be the most flexible modeling technique, adapting to changes and new datasets easily while storing all historical data by default. There are 3 core types of tables in data vault: hubs, links, and satellites.\nHubs: tables that contain a list of unique business keys (natural keys), surrogate keys, and metadata describing the data source for each hub item. Links: tables that associate hubs and satellites via the business key."},{"id":97,"href":"/en/hub/Architecture/Fan-out/","title":"Fan-out","parent":"architecture","content":"Fan-out is a pattern where a message from a source is spread or copied to one or more destinations. In data engineering, fan-out is commonly used to send data from a microservice (publisher) to multiple subscribers. The fan-out service normally doesn\u0026rsquo;t save the message once it has been sent, so a message queue is also common to see between the fan-out service and the subscriber for catch-up/re-try scenarios.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A[Publisher] --\u0026gt;|Message 1| B(Fan-out service) B --\u0026gt;|Message 1| C[Subscriber 1] B --\u0026gt;|Message 1| D[Subscriber 2] B --\u0026gt;|Message 1| E[Subscriber 3] Fan-out Advantages Send data from one source to many destinations Fan-out Disadvantages Usually limited re-try capabilities if a subscriber is unavailable for an extended period ","description":"Fan-out is a pattern where a message from a source is spread or copied to one or more destinations. In data engineering, fan-out is commonly used to send data from a microservice (publisher) to multiple subscribers. The fan-out service normally doesn\u0026rsquo;t save the message once it has been sent, so a message queue is also common to see between the fan-out service and the subscriber for catch-up/re-try scenarios.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A[Publisher] --\u0026gt;|Message 1| B(Fan-out service) B --\u0026gt;|Message 1| C[Subscriber 1] B --\u0026gt;|Message 1| D[Subscriber 2] B --\u0026gt;|Message 1| E[Subscriber 3] Fan-out Advantages Send data from one source to many destinations Fan-out Disadvantages Usually limited re-try capabilities if a subscriber is unavailable for an extended period "},{"id":98,"href":"/en/hub/Databases/k-v-database/Key-Value-Database/","title":"Key-Value Database","parent":"key-value database","content":"A Key/Value database is a type of [[Non-relational Database|NoSQL]] database that stores data as a table where you have a unique key for each data value.\n![[key_value_database_example.png]] \u0026ldquo;Key/Value data store\u0026rdquo; by Microsoft.com\nPopular Key/Value Databases [[Redis]] [[Amazon DynamoDB|DynamoDB]] Riak IonDB\nKey-Value Database Advantages Optimized for simple lookups using the key or a range of keys on a single table. Can be very scalable because data can be distributed across multiple machines. Key-Value Database Disadvantages Not great if you need to query or filter by non-key values. Often more expensive than other kinds of databases because they tend to run in-memory. When to use a Key-Value Database A key-value database is mostly used when you need to cache data because it is very fast and doesn\u0026rsquo;t require complex querying.\nKey-Value Database Use Cases Saving user session attributes Shopping cart ","description":"A Key/Value database is a type of [[Non-relational Database|NoSQL]] database that stores data as a table where you have a unique key for each data value.\n![[key_value_database_example.png]] \u0026ldquo;Key/Value data store\u0026rdquo; by Microsoft.com\nPopular Key/Value Databases [[Redis]] [[Amazon DynamoDB|DynamoDB]] Riak IonDB\nKey-Value Database Advantages Optimized for simple lookups using the key or a range of keys on a single table. Can be very scalable because data can be distributed across multiple machines."},{"id":99,"href":"/en/hub/Concepts/concepts/Table-Locking/","title":"Table Locking","parent":"concepts","content":"Brief description of the concept.\nTable Locking Advantages Table Locking Disadvantages ","description":"Brief description of the concept.\nTable Locking Advantages Table Locking Disadvantages "},{"id":100,"href":"/en/hub/Databases/others/Timeseries-Database/","title":"Timeseries Database","parent":"others","content":"A timeseries database (TSDB) is optimized to store, aggregate and analyze large amounts of continuously generated time-stamped data from sources such as IoT devices or sensors. They are used in applications that require monitoring performance changes over time or tracking sequences of events.\nTimeseries Database Advantages Generally provide built-in functions for analyzing timeseries data and lifecycle management More efficiently compresses and stores timeseries data vs general purpose databases Timeseries Database Disadvantages Not optimal for analyzing relationships between datasets Requires more storage because all timeseries data is indexed Require a greater amount of code and complexity vs general purpose databases Timeseries Database Use Cases Stock market data Trading platform Stock exchange Real-time ad bidding Popular Timeseries Databases InfluxDB Kdb+ Prometheus Graphite TimescaleDB ","description":"A timeseries database (TSDB) is optimized to store, aggregate and analyze large amounts of continuously generated time-stamped data from sources such as IoT devices or sensors. They are used in applications that require monitoring performance changes over time or tracking sequences of events.\nTimeseries Database Advantages Generally provide built-in functions for analyzing timeseries data and lifecycle management More efficiently compresses and stores timeseries data vs general purpose databases Timeseries Database Disadvantages Not optimal for analyzing relationships between datasets Requires more storage because all timeseries data is indexed Require a greater amount of code and complexity vs general purpose databases Timeseries Database Use Cases Stock market data Trading platform Stock exchange Real-time ad bidding Popular Timeseries Databases InfluxDB Kdb+ Prometheus Graphite TimescaleDB "},{"id":101,"href":"/en/hub/Architecture/Claim-Check-Pattern/","title":"Claim Check Pattern","parent":"architecture","content":"The claim-check pattern is used to reduce the cost and size of large messages by first storing the data in an external storage location and then sending a reference to the data/event to the consumer.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A[[Message with data]]--\u0026gt;|1.| B((Producer)) B --\u0026gt;|2. Store data, save key| C[(Storage)] D[[Smaller message with key only]] B --\u0026gt;|3.| D --\u0026gt;|4.| E((Consumer)) C --\u0026gt;|5. Get data with key| E --\u0026gt;|6.| F[[Message with data]] Send message Store message in data store Enqueue the message\u0026rsquo;s reference (i.e. key) Read the message\u0026rsquo;s reference Retrieve the message Process the message Claim Check Pattern Advantages Reduces cost of data transfer via messaging/streams. This is because storage is usually cheaper than messaging/streaming resources (memory). Helps protect the message bus and client from being overwhelmed or slowed down by large messages. Allows you to asynchronously process data which can help with scalability/performance. Claim Check Pattern Disadvantages If the external service used to store the payload fails, then the message will not be delivered. Requires additional storage space and adds additional time to store/retrieve data. Claim Check Pattern Examples Kafka client writes payload to S3/Azure Blob Storage/GCS. Then it sends a notification message. The consumer receives the message and accesses the payload from S3/Azure Blob Storage/GCS. In Airflow, you sometimes need to pass data between tasks. You can do this using XComs but there is a limit to the size of the message you can send. For passing large messages via XComs you can use the claim check pattern. Sources:\nhttps://learn.microsoft.com/en-us/azure/architecture/patterns/claim-check https://www.enterpriseintegrationpatterns.com/patterns/messaging/StoreInLibrary.html https://serverlessland.com/event-driven-architecture/visuals/claim-check-pattern https://aws.plainenglish.io/an-introduction-to-claim-check-pattern-and-its-uses-b018649a380d ","description":"The claim-check pattern is used to reduce the cost and size of large messages by first storing the data in an external storage location and then sending a reference to the data/event to the consumer.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A[[Message with data]]--\u0026gt;|1.| B((Producer)) B --\u0026gt;|2. Store data, save key| C[(Storage)] D[[Smaller message with key only]] B --\u0026gt;|3.| D --\u0026gt;|4.| E((Consumer)) C --\u0026gt;|5. Get data with key| E --\u0026gt;|6."},{"id":102,"href":"/en/hub/Concepts/concepts/Vertical-Scaling/","title":"Vertical Scaling","parent":"concepts","content":"Vertical scaling is when you increase the capacity of a system by increasing the compute size.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% flowchart LR subgraph Before A[(Computer 1)] end subgraph After B[(Computer 1 \\n\\n\\n\\n\\n)] end Before --\u0026gt; After Vertical Scaling Advantages Very simple and straight forward No code or design changes needed Vertical Scaling Disadvantages Becomes more expensive (compared to [[Horizontal Scaling|horizontal scaling]]) as more specialized hardware is needed to achieve higher levels of performance Increased risk of a single point of failure ","description":"Vertical scaling is when you increase the capacity of a system by increasing the compute size.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% flowchart LR subgraph Before A[(Computer 1)] end subgraph After B[(Computer 1 \\n\\n\\n\\n\\n)] end Before --\u0026gt; After Vertical Scaling Advantages Very simple and straight forward No code or design changes needed Vertical Scaling Disadvantages Becomes more expensive (compared to [[Horizontal Scaling|horizontal scaling]]) as more specialized hardware is needed to achieve higher levels of performance Increased risk of a single point of failure "},{"id":103,"href":"/en/hub/Concepts/concepts/Change-Data-Capture/","title":"Change Data Capture","parent":"concepts","content":"Change data capture describes the process of recording the change of data in a database. Typically, this means tracking when records are inserted, updated, and deleted along with the data itself.\nChange Data Capture Advantages Better use of bandwidth Can keep historical data changes Change Data Capture Disadvantages More complex to set up than [[Full Load|full loads]] or [[Delta Load|delta loads]] Usually requires higher permissions to access the database transaction log. When to use change data capture Change data capture is typically used to replicate data that is overwritten to another database. For example, replicating data from an operational database to a data warehouse. While the operational database may not need to store historical changes, it might be useful for analysis.\nA few situations where you might use change data capture:\nReplicate changes into a [[Data Warehouse|data warehouse]] or [[Data Lake|data lake]] Replicate changes into [[Apache Kafka|Kafka]] (or other streaming tool) in a microservices architecture Upgrade a database to a higher version with minimal downtime Migrate data from database X to database Y with minimal downtime Popular Change Data Capture Tools [[Debezium]] Confluent [[Amazon DMS]] Qlik Striim Matillion Data Loader ","description":"Change data capture describes the process of recording the change of data in a database. Typically, this means tracking when records are inserted, updated, and deleted along with the data itself.\nChange Data Capture Advantages Better use of bandwidth Can keep historical data changes Change Data Capture Disadvantages More complex to set up than [[Full Load|full loads]] or [[Delta Load|delta loads]] Usually requires higher permissions to access the database transaction log."},{"id":104,"href":"/en/hub/Data-Process/Batch-Data-Processing/","title":"Batch Data Processing","parent":"Data Process","content":"Batch processing is a term used to describe collecting, modifying, or exporting multiple data records at a regular cadence with downtime in between batches. Because large amounts of data can be processed all at once in these batches it can be a very efficient approach and is best suited for handling frequent, repetitive tasks. It is the most common form of data processing that fits many businesses data needs.\nA key point to remember about batch processing is that it was originally designed to handle non-continuous data. [[Stream Data Processing|Stream processing]] is typically used for continuous data, though there are ways to configure the architecture behind batch processing to enable quasi-continuous workflows. (See Concurrent Batch Processing below)\nMany businesses face increasingly complicated and diverse data challenges due to the sheer magnitude of data available. Batch processing has increased in sophistication, and is also often used in conjunction with other processing techniques for modern analysis. While batch processing used to be by far the most common and widely used method of data processing, recently real-time or near real-time [[Stream Data Processing|stream processing]] has proven to be a worthy competitor. As traditional batch systems run overnight to process data accumulated during the day, there is naturally a delta between the real world versus what the data is actually describing. Advanced Batch Processing partially solves this issue, but even the most advanced systems cannot compete with [[Stream Data Processing|stream processing]] for real-time continuous data.\nBatch Data Processing Advantages Efficiency Batch processing allows a company to process data when computing or other resources are available. For example, a common schedule is to process data overnight when the database and servers aren\u0026rsquo;t being used by employees. If data isn\u0026rsquo;t frequently updated, one can simply change the batch processing schedule to make it less frequent as well.\nSimplicity Compared to [[Stream Data Processing|stream processing]], batch processing is usually less complex and doesn\u0026rsquo;t require special hardware or system support for incoming data. Batch processing systems typically require less maintenance than stream processing.\nProcessing Speed Because batch processing allows companies to process large amounts of data quickly, this speeds up processing time and delivers data that companies can use in a timely fashion.\nAdvanced Batch Processing Traditionally, batch processing was usually configured to run sequentially. Each job was processed one after another on a single machine. The need for more sophistication led to the rise of concurrent and parallel batch processing.\nConcurrent Batch Processing Concurrent batch processing typically refers to jobs that run batches partially overlapping in time. This overlap allows for a piece of the data to always be analyzed at a given time. Concurrent batch processing gives the illusion of parallelism without requiring more than a single CPU core. Due to this concurrent \u0026ldquo;multi-threading\u0026rdquo; behavior, the architecture for concurrent batch processing must have fault tolerance in mind. As batches are not run one after another, a single batch failure could cause a domino effect on other batches should the architecture be configured poorly.\nParallel Batch Processing Parallel batch processing takes a similar approach as concurrent batch processing, however instead of overlapping parts of batches over time, entire batches are scheduled in parallel. By taking advantage of the relative cheapness of multicore machines in the modern age, parallel batch processing can multitask effectively.\nModern Batch Processing Modern day batch processing methods often use a combination of both concurrent and parallel batch processing. Also called parallel concurrent batch processing, by finding the right balance of parameter tunings to optimize how each CPU core handles multiple tasks and how each worker system handles a single task, when properly configured, parallel concurrent batch processing is a state of the art solution. Institutions that require greater stability and security such as the financial sector most commonly use parallel concurrent batch processing. For the most important data, often multiple redundant batches are run so that even if one batch fails, other batches can cover for the mistakes of the failure.\nAs mentioned above, live data streaming is a challenge for batch processing traditionally. While attempts have been made to use concurrent and parallel batch processing methods to analyze \u0026ldquo;microbatches\u0026rdquo; stacked on top of each other on extremely powerful machines, the use case for complex architectures like this is niche. For the majority of live data cases, [[Stream Data Processing|stream processing]] is still preferred. The main business use case for batch processing for this application is when such large quantities of data needs to be analyzed that stream data processing is not a viable option.\n","description":"Batch processing is a term used to describe collecting, modifying, or exporting multiple data records at a regular cadence with downtime in between batches. Because large amounts of data can be processed all at once in these batches it can be a very efficient approach and is best suited for handling frequent, repetitive tasks. It is the most common form of data processing that fits many businesses data needs.\nA key point to remember about batch processing is that it was originally designed to handle non-continuous data."},{"id":105,"href":"/en/hub/Concepts/concepts/Normalization/","title":"Normalization","parent":"concepts","content":"Normalization is the process of organizing data in a database to reduce redundancy and improve data integrity. This makes the database design simpler and faster, as well as more accurate and efficient. Since inserts, updates, and deletes occur rapidly in [[Online Transaction Processing|OLTP]] systems, normalization is particularly important for those types of systems. There are several steps to normalization:\nDivide the data into tables, based on how the data is related. Make sure each table only contains related data. If data is not directly related, create a new table for that data. Ensure that each column in a table has a specific purpose (e.g., to store a particular type of information). Avoid repeating groups of information within columns (or tables). Normalization Advantages Minimized data redundancy (duplicate data) and null values A more compact database structure Simplified queries Faster searching, sorting, and indexing Normalization Disadvantages Complex queries can be slower due to the number of joins needed Normalization Examples ![[Assets/normalization_example.png|800]]\n","description":"Normalization is the process of organizing data in a database to reduce redundancy and improve data integrity. This makes the database design simpler and faster, as well as more accurate and efficient. Since inserts, updates, and deletes occur rapidly in [[Online Transaction Processing|OLTP]] systems, normalization is particularly important for those types of systems. There are several steps to normalization:\nDivide the data into tables, based on how the data is related."},{"id":106,"href":"/en/hub/Concepts/concepts/Data-Unit-Test/","title":"Data Unit Test","parent":"concepts","content":"A data unit test is an automated test you can create which ensures that the data coming through your data pipeline is what you expect it to be. Data unit tests are most useful for knowing when upstream data changes, when data is stale/cached, and preventing bad data from ruining machine learning models or public-facing reports and dashboards.\nCreating data unit tests are also a good way of documenting what the data set should look like which you can then show a teammate or stakeholder and get on the same page quickly.\nData Unit Testing Tools Some commonly used tools for data unit testing are [[Great Expectations|great expectations]] if you are using Python and dbt tests if you are using SQL and dbt. You can also write your own tests using Python pytest or SQL depending on your [[data pipeline]].\nData Unit Test Examples One of the most common tests to incorporate in your data pipeline is a check to see if there is recently created data by looking at a created date column. This is known as a stale data check or data freshness check. The reason why it\u0026rsquo;s common is that your data pipeline can run successfully, but the data inside it might have been old or malformed which is considered a \u0026ldquo;silent failure.\u0026rdquo;\nOther commonly used tests for data are checking to make sure all values in a column are unique, checking for null values, and checking to make sure all the values in a column are within a certain expected range.\n","description":"A data unit test is an automated test you can create which ensures that the data coming through your data pipeline is what you expect it to be. Data unit tests are most useful for knowing when upstream data changes, when data is stale/cached, and preventing bad data from ruining machine learning models or public-facing reports and dashboards.\nCreating data unit tests are also a good way of documenting what the data set should look like which you can then show a teammate or stakeholder and get on the same page quickly."},{"id":107,"href":"/en/hub/Data-Process/Stream-Data-Processing/","title":"Stream Data Processing","parent":"Data Process","content":"Stream processing refers to the computer programming architecture for continuously collecting, modifying, or exporting data as it is produced or received. This method is typically used instead of [[Batch Data Processing|batch processing]] when the timeliness of the data is critical to the business needs and the data is needed in real-time.\n","description":"Stream processing refers to the computer programming architecture for continuously collecting, modifying, or exporting data as it is produced or received. This method is typically used instead of [[Batch Data Processing|batch processing]] when the timeliness of the data is critical to the business needs and the data is needed in real-time."},{"id":108,"href":"/en/hub/Architecture/Lambda-Architecture/","title":"Lambda Architecture","parent":"architecture","content":"Lambda architecture is a data processing pattern designed to strike a balance between low latency, high throughput, and fault tolerance. This architecture type uses a combination of batch processing to create accurate views of large data sets and real-time stream processing to provide views of live data. The results from both sets can then be merged and presented together.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A((Data Source)) subgraph Batch Layer B(\u0026#34;Batch view(s)\u0026#34;) end subgraph Speed Layer C(\u0026#34;Real-time view(s)\u0026#34;) end A --\u0026gt; B A --\u0026gt; C subgraph Serving Layer D(\u0026#34;Combined view(s)\u0026#34;) end B --\u0026gt; D C --\u0026gt; D Lambda Architecture Advantages Efficiently serves batch and real-time workloads Lambda Architecture Disadvantages Duplicated code/logic for both batch and real-time views Lambda Architecture Learning Resources http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html ","description":"Lambda architecture is a data processing pattern designed to strike a balance between low latency, high throughput, and fault tolerance. This architecture type uses a combination of batch processing to create accurate views of large data sets and real-time stream processing to provide views of live data. The results from both sets can then be merged and presented together.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A((Data Source)) subgraph Batch Layer B(\u0026#34;Batch view(s)\u0026#34;) end subgraph Speed Layer C(\u0026#34;Real-time view(s)\u0026#34;) end A --\u0026gt; B A --\u0026gt; C subgraph Serving Layer D(\u0026#34;Combined view(s)\u0026#34;) end B --\u0026gt; D C --\u0026gt; D Lambda Architecture Advantages Efficiently serves batch and real-time workloads Lambda Architecture Disadvantages Duplicated code/logic for both batch and real-time views Lambda Architecture Learning Resources http://nathanmarz."},{"id":109,"href":"/en/hub/Data-Warehouse/Relational-Modeling/","title":"Relational Modeling","parent":"Data Warehouse","content":"Relational modeling revolves around using tables, columns, and rows to represent data. Each table denotes entities or subjects, while every row signifies individual records or instances belonging to that entity. These tables are connected via unique identifiers called foreign keys. Essentially, a foreign key is a column in a table that refers to the primary key of another table. A key component for relational modeling is [[Normalization|normalization]] (reduces data redundancy).\nRelational Modeling Advantages Great for transactional/operational workloads where data is constantly inserted, updated, or deleted. Supports complex queries and joins. Useful when accuracy and reliability are the most important requirements. Relational Modeling Disadvantages Analytical queries become slow at larger data scales. ","description":"Relational modeling revolves around using tables, columns, and rows to represent data. Each table denotes entities or subjects, while every row signifies individual records or instances belonging to that entity. These tables are connected via unique identifiers called foreign keys. Essentially, a foreign key is a column in a table that refers to the primary key of another table. A key component for relational modeling is [[Normalization|normalization]] (reduces data redundancy)."},{"id":110,"href":"/en/hub/Concepts/concepts/Indexing/","title":"Indexing","parent":"concepts","content":"An index is a data structure which allows you to quickly retrieve records from a database object by creating pointers that point to where data is stored. It\u0026rsquo;s typically stored in a key-value format where the key is the field or column (sorted) and the value is the pointer(s).\n![[Assets/index_example.png|800]] Image courtesy of dataschool.com\n","description":"An index is a data structure which allows you to quickly retrieve records from a database object by creating pointers that point to where data is stored. It\u0026rsquo;s typically stored in a key-value format where the key is the field or column (sorted) and the value is the pointer(s).\n![[Assets/index_example.png|800]] Image courtesy of dataschool.com"},{"id":111,"href":"/en/hub/Databases/graph-database/Graph-Database/","title":"Graph Database","parent":"graph database","content":"A graph database is a type of [[Non-relational Database|NoSQL]] database that uses nodes, edges, and properties to store data about entities and the relationships between them. The main purpose of a graph database is to allow for efficiently traversing the network of nodes and edges, and for analyzing the relationships between entities.\n![[graph_database_example.png|500]]\nPopular Graph Databases Neo4j ArangoDB OrientDB Azure Cosmos DB Graph API Graph Database Use Cases Social networks Fraud detection Anti-money laundering Machine Learning ","description":"A graph database is a type of [[Non-relational Database|NoSQL]] database that uses nodes, edges, and properties to store data about entities and the relationships between them. The main purpose of a graph database is to allow for efficiently traversing the network of nodes and edges, and for analyzing the relationships between entities.\n![[graph_database_example.png|500]]\nPopular Graph Databases Neo4j ArangoDB OrientDB Azure Cosmos DB Graph API Graph Database Use Cases Social networks Fraud detection Anti-money laundering Machine Learning "},{"id":112,"href":"/en/hub/Data-Warehouse/Dimensional-Modeling/","title":"Dimensional Modeling","parent":"Data Warehouse","content":"Developed by Ralph Kimball, dimensional modeling is a popular technique used to model data for analytics. At it\u0026rsquo;s core, dimensional modeling revolves around organizing data into two types of datasets: fact tables and dimension tables. Facts are usually comprised of numerical values that can be aggregated while dimensions hold descriptive attributes of entities/objects. A key tradeoff the dimensional model makes is it [[Denormalization|denormalizes]] data (increases data redundancy) in order to speed up queries.\nWithin dimensional modeling there are a few different schema design patterns: star schema (recommended in most cases), snowflake schema, and galaxy schema.\nDimensional Modeling Advantages Intuitive to understand. Good query performance for analytics. Keeps track of historical changes easily. Dimensional Modeling Disadvantages Can be complicated to query sometimes. ","description":"Developed by Ralph Kimball, dimensional modeling is a popular technique used to model data for analytics. At it\u0026rsquo;s core, dimensional modeling revolves around organizing data into two types of datasets: fact tables and dimension tables. Facts are usually comprised of numerical values that can be aggregated while dimensions hold descriptive attributes of entities/objects. A key tradeoff the dimensional model makes is it [[Denormalization|denormalizes]] data (increases data redundancy) in order to speed up queries."},{"id":113,"href":"/en/hub/Data-Warehouse/Data-Modeling/","title":"Data Modeling","parent":"Data Warehouse","content":"Data Modeling is the process of mapping out an information system and how multiple parts are connected. Data models are typically illustrated in an entity-relationship diagram for relational databases like the picture below.\n![[Assets/data_model_example.png|500]]\nBenefits of data modeling Data modeling makes it easier for developers and other stakeholders to view and understand the relationships between data in a database or data warehouse. A good data model can also have the following benefits:\nReduce errors in software and database development. Increase consistency in documentation and system design across the enterprise. Improve application and database performance. Ease data mapping throughout the organization. Improve communication between developers and business intelligence teams. Ease and speed the process of database design at the conceptual, logical and physical levels. Types of Data Models Conceptual Offers a big-picture view of what the system will contain, how it will be organized, and which business rules are involved. Conceptual models are usually created as part of the process of gathering initial project requirements. Typically, they include entity classes (defining the types of things that are important for the business to represent in the data model), their characteristics and constraints, the relationships between them and relevant security and data integrity requirements\nLogical They are less abstract and provide greater detail about the concepts and relationships in the domain under consideration. One of several formal data modeling notation systems is followed. These indicate data attributes, such as data types and their corresponding lengths, and show the relationships among entities. Logical data models don’t specify any technical system requirements. This stage is frequently omitted in agile or DevOps practices. Logical data models can be useful in highly procedural implementation environments, or for projects that are data-oriented by nature, such as data warehouse design or reporting system development.\nPhysical They provide a schema for how the data will be physically stored within a database. As such, they’re the least abstract of all. They offer a finalized design that can be implemented as a relational database, including associative tables that illustrate the relationships among entities as well as the primary keys and foreign keys that will be used to maintain those relationships. Physical data models can include database management system (DBMS)-specific properties, including performance tuning.\nData Modeling Techniques [[Relational Modeling]] [[Dimensional Modeling]] [[Data Vault Modeling]] [[One Big Table]] [[Activity Schema]] [[Unified Star Schema]]\n![[Learning Resources#Data Modeling Learning Resources]]\nData Modeling Tools Oracle Data Modeler dbdiagram.io This is not a full fledged modeling tool like Oracle Data modeler but supports the modeling using DBML which you can view and then convert DBML to DDL using the dbml cli ","description":"Data Modeling is the process of mapping out an information system and how multiple parts are connected. Data models are typically illustrated in an entity-relationship diagram for relational databases like the picture below.\n![[Assets/data_model_example.png|500]]\nBenefits of data modeling Data modeling makes it easier for developers and other stakeholders to view and understand the relationships between data in a database or data warehouse. A good data model can also have the following benefits:"},{"id":114,"href":"/en/hub/Databases/column-database/Column-oriented-Database/","title":"Column-oriented Database","parent":"column database","content":"![[Assets/row_oriented_vs_column_oriented_database.jpeg|500]]\nIn a column-oriented or columnar database, the data for each column in a datatable is stored together. Because of their characteristics, they are a popular option for building a [[Data Warehouse|data warehouse]].\nColumn-oriented Database Example In a datatable like this:\nEmpId Lastname FirstnameSalary 10 Smith Joe 12 Jones Mary 11 Johnson Cathy 22 Jones Bob The data would be stored like this (simplified example):\n10,12,11,22; Smith,Jones,Johnson,Jones; Joe,Mary,Cathy,Bob; 60000,80000,94000,55000; Column-oriented Database Advantages More efficient querying when querying a subset of columns because the database doesn\u0026rsquo;t need to read columns that aren\u0026rsquo;t relevant. Data can be compressed further which translates into storage and query improvements. Column-oriented Database Disadvantages Typically less efficient when inserting data. When to use a column-oriented database When you tend to only query a subset of columns in your data. When you often run analytical queries or [[Online Analytical Processing|OLAP]] workloads such as metrics and aggregations. Column-oriented Database Use Cases Reporting Big Data Analytics Business Intelligence ","description":"![[Assets/row_oriented_vs_column_oriented_database.jpeg|500]]\nIn a column-oriented or columnar database, the data for each column in a datatable is stored together. Because of their characteristics, they are a popular option for building a [[Data Warehouse|data warehouse]].\nColumn-oriented Database Example In a datatable like this:\nEmpId Lastname FirstnameSalary 10 Smith Joe 12 Jones Mary 11 Johnson Cathy 22 Jones Bob The data would be stored like this (simplified example):\n10,12,11,22; Smith,Jones,Johnson,Jones; Joe,Mary,Cathy,Bob; 60000,80000,94000,55000; Column-oriented Database Advantages More efficient querying when querying a subset of columns because the database doesn\u0026rsquo;t need to read columns that aren\u0026rsquo;t relevant."},{"id":115,"href":"/en/hub/Databases/document-database/Document-Database/","title":"Document Database","parent":"document database","content":"A document database is a type of [[Non-relational Database|NoSQL]] database that is designed to store and query data as JSON-like documents. Document databases make it easier to store an query data in a way that can evolve with an application\u0026rsquo;s needs. The document model works well with use cases such as catalogs, user profiles, and content management systems where each document is unique and evolves over time.\n![[document_database_example.png]] \u0026ldquo;Document data stores\u0026rdquo; by Microsoft.com\nPopular Document Databases [[MongoDB]] [[Couchbase]] [[Amazon DynamoDB]] [[RavenDB]] Azure Cosmos DB\nDocument Database Advantages Create documents without needing to define their structure upfront. Add new fields to the database without changing the fields of existing documents. Can scale horizontally very easily. Document Database Disadvantages Query performance can be less efficient compared to a [[Relational Database]] because the data isn\u0026rsquo;t necessarily structured or organized for queries. Generally requires more technical knowledge to query which means usage is typically limited to technical staff vs other non-technical business people. Updating data can be a slow process because the data can be distributed between machines and can be duplicated. Atomic transactions are not inherently supported. When to use a Document Database Storing article content, social media posts, sensor data, and other unstructured data. You need to develop and iterate rapidly when building a product. Document Database Use Cases Content management Catalogs. For example, in an e-commerce application, storing different products with different attributes. ","description":"A document database is a type of [[Non-relational Database|NoSQL]] database that is designed to store and query data as JSON-like documents. Document databases make it easier to store an query data in a way that can evolve with an application\u0026rsquo;s needs. The document model works well with use cases such as catalogs, user profiles, and content management systems where each document is unique and evolves over time.\n![[document_database_example.png]] \u0026ldquo;Document data stores\u0026rdquo; by Microsoft."},{"id":116,"href":"/en/hub/Concepts/concepts/Window-Function/","title":"Window Function","parent":"concepts","content":"A [[SQL]] window function is a special syntax that allows aggregate functions to be performed over a set of rows that are related to the current row in the query. It\u0026rsquo;s similar to using an aggregate function but, unlike regular aggregations, it doesn\u0026rsquo;t collapse the result into one value; instead, each row retains its identity and the calculated result is returned for every row. Window functions are commonly used when creating running totals or ranking values within datasets.\n![[Assets/sql-window-function.png|500]]\nWindow Function Examples SELECT Name, SUM(Sales) OVER(PARTITION BY Name) AS TotalSales FROM my_table; This statement will calculate a running total for each name listed in my_table.\nWindow Function Advantages Perform complex calculations with less code than would otherwise be required Window Function Disadvantages Can be more complicated to understand and debug ","description":"A [[SQL]] window function is a special syntax that allows aggregate functions to be performed over a set of rows that are related to the current row in the query. It\u0026rsquo;s similar to using an aggregate function but, unlike regular aggregations, it doesn\u0026rsquo;t collapse the result into one value; instead, each row retains its identity and the calculated result is returned for every row. Window functions are commonly used when creating running totals or ranking values within datasets."},{"id":117,"href":"/en/hub/Data-Warehouse/Data-Warehouse/","title":"Data Warehouse","parent":"Data Warehouse","content":"A data warehouse is a central repository for data which will be used for reporting and analytics. Data comes into the data warehouse from [[Online Transaction Processing|transactional systems]], relational databases, or [[Data Lake|other sources]] usually on a regular cadence. Business analysts, data engineers, data scientists, and decision makers then access the data through [[Business Intelligence|business intelligence]] tools, [[SQL]] clients, and other analytics applications. Because the primary use cases for a data warehouse revolve around analytics, they typically use an [[Online Analytical Processing|OLAP]] technology for performance.\nData Warehouse Advantages Consolidate data from multiple data sources into one \u0026ldquo;source of truth\u0026rdquo; Optimized for read access which makes generating reports faster than using a source transaction system for reporting Store and analyze large amounts of historical data Data Warehouse Disadvantages A significant investment of time and resources to properly build Not designed for ingesting data in real-time (although they can typically handle near real-time) When to use a Data Warehouse Data warehouses are made for complex queries on large datasets. You should consider a data warehouse if you\u0026rsquo;re looking to keep your historical data separate from current transactions for performance reasons.\nPopular Data Warehouses [[Tools/Databases/Amazon Redshift|Amazon Redshift]] [[Azure Synapse Analytics]] [[Google BigQuery]] [[Microsoft SQL Server]] [[Snowflake]] Data Warehouse Benchmarks 1 TB: 2020 - Redshift, Snowflake, Presto and BigQuery 30 TB: 2019 - Redshift, Azure SQL Data Warehouse, BigQuery, Snowflake ![[Learning Resources#Data Warehousing Learning Resources]]\n","description":"A data warehouse is a central repository for data which will be used for reporting and analytics. Data comes into the data warehouse from [[Online Transaction Processing|transactional systems]], relational databases, or [[Data Lake|other sources]] usually on a regular cadence. Business analysts, data engineers, data scientists, and decision makers then access the data through [[Business Intelligence|business intelligence]] tools, [[SQL]] clients, and other analytics applications. Because the primary use cases for a data warehouse revolve around analytics, they typically use an [[Online Analytical Processing|OLAP]] technology for performance."},{"id":118,"href":"/en/hub/Concepts/concepts/CAP-Theorem/","title":"CAP Theorem","parent":"concepts","content":"The CAP theorem in computer science states that it\u0026rsquo;s impossible for a distributed data store to provide more than two out of the three following guarantees:\nConsistency: Every read receives the most recent write or an error Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes ","description":"The CAP theorem in computer science states that it\u0026rsquo;s impossible for a distributed data store to provide more than two out of the three following guarantees:\nConsistency: Every read receives the most recent write or an error Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes "},{"id":119,"href":"/en/hub/Architecture/Metrics-Layer/","title":"Metrics Layer","parent":"architecture","content":"A single source of truth for how metrics are defined and their business logic in an organization.\nMetrics Layer Advantages You can define a metric once and use it everywhere Metrics Layer Disadvantages #placeholder/description\nPopular Metrics Layer Tools [[Cube.js]] [[Metriql]] [[data build tool|dbt]] (with dbt metrics + dbt server coming soon) [[Metricflow]]\n","description":"A single source of truth for how metrics are defined and their business logic in an organization.\nMetrics Layer Advantages You can define a metric once and use it everywhere Metrics Layer Disadvantages #placeholder/description\nPopular Metrics Layer Tools [[Cube.js]] [[Metriql]] [[data build tool|dbt]] (with dbt metrics + dbt server coming soon) [[Metricflow]]"},{"id":120,"href":"/en/hub/Concepts/concepts/Data-Pipeline/","title":"Data Pipeline","parent":"concepts","content":"A Data Pipeline is a term used to describe a workflow consisting of one or more tasks that ingest, move, and transform raw data from one or more sources to a destination. Usually, the data at the destination is then used for analysis, machine learning, or other business functions. You can generally separate data pipelines into 2 categories: [[Batch Data Processing|batch processing]] (most common) and [[Stream Data Processing|real-time processing]] pipelines.\n![[data_pipeline_patterns.png|800]] \u0026ldquo;Data Pipeline Patterns\u0026rdquo; by Informatica.com\nData Pipeline Architecture A data pipeline\u0026rsquo;s architecture is made up of 4 main parts: a data source, business logic, a destination, and a scheduler (for batch). It can be as simple as a script running on your computer and automated with Cron or Windows task scheduler. A more complex example might be streaming event data, processing it, then powering dashboards or using it to train machine learning models.\nThe architecture you choose can vary wildly and there is no one size fits all. However, once you learn the basics of building data pipelines, you\u0026rsquo;ll be able to better understand the tradeoffs between architecture decisions.\nData Sources Common data sources are application databases, APIs, or files from an SFTP server. If you are looking for data sources to practice with or analyze, many governments publish their datasets publicly and you can find them by searching \u0026ldquo;open data portal.\u0026rdquo;\nBusiness Logic Business logic is a general term that encompasses the type of transformations that need to be applied to the data inside the data pipeline. It usually involves cleaning, filtering, and applying logic to the data that is specific to the business.\nData Destination or Data Target Typically, the target where you send your data is another database. Common data targets are databases or data storage areas that are made for analytics. For example, a [[Data Warehouse|data warehouse]] or [[Data Lake|data lake]].\nScheduler or Orchestration Tool For simple pipelines, Cron is one of the most commonly used tools to schedule a pipeline to run. You typically install it onto a server where you want your script to run and then use cron syntax to tell it when to run.\nFor more advanced data pipelines where there are multiple steps that depend on each other, an advanced scheduler called a workflow orchestrator is more appropriate. Popular workflow orchestrations tools are: [[Apache Airflow|Airflow]], Prefect, and Dagster.\nData Pipeline Examples If a business sold software as a service, a Data Engineer might create a data pipeline that runs on a daily basis which takes some of the data generated by the software application, combine it with some data the marketing department has and send the data to a dashboard. The dashboard could then be used to better understand how customers are using the software.\nChange Data Capture (CDC) Pipeline Example This is a very commonly created type of data pipeline. You can use a [[Change Data Capture|CDC]] pipeline to replicate changes (inserted, updated, and deleted data) from a source database (typically a db backing an application) and into a destination for analytics such as a data warehouse or data lake.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR subgraph Source direction TB A[(Application Database)] A---\u0026gt;|Save incoming changes to log first|AB AB[Database Transaction Log] end subgraph Server B[CDC Process/Software] end Source ---\u0026gt;|Read transaction log|B subgraph Target B ---\u0026gt;|Replicate changes|C C[(Data Warehouse)] end class C internal-link; CDC Pipeline example by Confluent CDC Pipeline example using Debezium and Kafka CDC Pipeline example using AWS DMS ETL Pipeline Example ETL is an acronym for extract, transform, and load. Data engineers often use the terms ETL pipeline and data pipeline interchangeably. It is also synonymous with [[Batch Data Processing|batch processing]]. An ETL pipeline generally works by extracting data from a source using custom code or a replication tool, transforming the data using custom code or a transformation tool, and then loading the transformed data to your destination. This pattern was necessary when storage costs were higher but storage has become very cheap over the past decade and is no longer a limitation in most cases. One of the downsides of transforming raw data before loading it to a destination is that you lose the flexibility to re-compute something because the raw data at that point in time is not saved.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR subgraph Source A[(Application Database)] end subgraph Server A ---\u0026gt;|Extract raw data|B B[Transform Logic/process data] end subgraph Target B ---\u0026gt;|Load transformed data|C C[(Data Warehouse)] end class C internal-link; ETL Pipeline example using Python, Docker, and Airflow ETL Pipeline example using Airflow, Python/Pandas, and S3 ELT Pipeline Example A similar concept to the previous ETL pipeline example except you load the raw data straight from the source into the destination and then transform the raw data in the destination itself. There are a few reasons why this pattern has become more mainstream. Storage cost has become cheap, enabling the ability to store raw data economically. By always having a copy of the raw data in your destination, you can change the way you transform it and perform business logic. In ETL, since the data is transformed before it\u0026rsquo;s loaded to the destination, if you need to change the way you calculate something you can\u0026rsquo;t because the raw data may no longer be available.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR subgraph Source A[(Application Database)] end subgraph Target direction TB A ---\u0026gt;|Extract \u0026amp; Load raw data|B B[(Data Warehouse)] B --\u0026gt; C C[Transform Logic/process data] C --\u0026gt; B end class B internal-link; ELT Pipeline example with Spark, S3, and Airflow ELT Pipeline example using dbt, Terraform, and Metabase ","description":"A Data Pipeline is a term used to describe a workflow consisting of one or more tasks that ingest, move, and transform raw data from one or more sources to a destination. Usually, the data at the destination is then used for analysis, machine learning, or other business functions. You can generally separate data pipelines into 2 categories: [[Batch Data Processing|batch processing]] (most common) and [[Stream Data Processing|real-time processing]] pipelines."},{"id":121,"href":"/en/hub/Architecture/Semantic-Layer/","title":"Semantic Layer","parent":"architecture","content":"The semantic layer is a term used to describe the [[Data Modeling|data model]] that takes multiple enterprise data source models and combines them into one unified model for the business. It traditionally is built in the [[Data Warehouse|data warehouse]] and used by reporting tools. Companies have been using semantic layers to manage data since the early 1990s.\nSemantic Layer Advantages Reduces complexity Makes it easier to find information for business users Facilitates self-serve reporting Improve aggregated query performance Semantic Layer Disadvantages Requires regular maintenance ","description":"The semantic layer is a term used to describe the [[Data Modeling|data model]] that takes multiple enterprise data source models and combines them into one unified model for the business. It traditionally is built in the [[Data Warehouse|data warehouse]] and used by reporting tools. Companies have been using semantic layers to manage data since the early 1990s.\nSemantic Layer Advantages Reduces complexity Makes it easier to find information for business users Facilitates self-serve reporting Improve aggregated query performance Semantic Layer Disadvantages Requires regular maintenance "},{"id":122,"href":"/en/hub/Concepts/concepts/Data-Lake/","title":"Data Lake","parent":"concepts","content":" A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics - from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\nAWS, What is a data lake? ","description":" A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics - from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\nAWS, What is a data lake? "},{"id":123,"href":"/en/hub/Databases/relational-database/Relational-Database/","title":"Relational Database","parent":"relational database","content":"A relational database organizes data into tables which can be linked together based on data that is common in each table. Each table can have one or more columns with unique identifiers (primary key) that point to an id column in another table (foreign key) which forms the relationship between the two tables.\nRelational Database Advantages Easier to do complex queries Referential integrity enforced by the system Updates are fast because all data is on one machine Supports atomic transactions Relational Database Disadvantages Harder to scale ([[Vertical Scaling|vertical scaling]]) More effort required to design data structures Relational Database Use Cases Payment/Booking System Enterprise resource planning (ERP) system Customer Relationship Management (CRM) system SaaS application Ecommerce and Web Web frameworks Traditional applications ","description":"A relational database organizes data into tables which can be linked together based on data that is common in each table. Each table can have one or more columns with unique identifiers (primary key) that point to an id column in another table (foreign key) which forms the relationship between the two tables.\nRelational Database Advantages Easier to do complex queries Referential integrity enforced by the system Updates are fast because all data is on one machine Supports atomic transactions Relational Database Disadvantages Harder to scale ([[Vertical Scaling|vertical scaling]]) More effort required to design data structures Relational Database Use Cases Payment/Booking System Enterprise resource planning (ERP) system Customer Relationship Management (CRM) system SaaS application Ecommerce and Web Web frameworks Traditional applications "},{"id":124,"href":"/en/hub/Data-Warehouse/Online-Analytical-Processing/","title":"Online Analytical Processing","parent":"Data Warehouse","content":"Online analytical processing is the term used for a [[Data Modeling|data model]] that aggregates data in a way that makes it easier and faster to query across multiple dimensions. For this reason, OLAP systems are mostly optimized for reading the data and are used primarily for reporting and analysis.\n","description":"Online analytical processing is the term used for a [[Data Modeling|data model]] that aggregates data in a way that makes it easier and faster to query across multiple dimensions. For this reason, OLAP systems are mostly optimized for reading the data and are used primarily for reporting and analysis."},{"id":125,"href":"/en/hub/Databases/k-v-database/In-Memory-Database/","title":"In-Memory Database","parent":"key-value database","content":"An in-memory database is a type of database that stores data in the computer\u0026rsquo;s main memory or RAM, giving it faster access speed compared to other types of databases. This makes it particularly useful for applications that require very high or real-time read/write speeds such as gaming, web applications and financial transactions. In contrast to traditional databases which store their data on disk, in-memory databases are generally limited in the amount of data they can store due to their reliance on the computer\u0026rsquo;s available RAM.\nIn-Memory Database Advantages Data is accessible almost instantly In-Memory Database Disadvantages Storing data in-memory is more expensive than on disk In the event of a database failure, data loss will occur In-Memory Database Use Cases Cache Ecommerce Gaming Leaderboard Session management Social chat or news feed Personalization Adtech Popular In-Memory Databases Redis Memcached ","description":"An in-memory database is a type of database that stores data in the computer\u0026rsquo;s main memory or RAM, giving it faster access speed compared to other types of databases. This makes it particularly useful for applications that require very high or real-time read/write speeds such as gaming, web applications and financial transactions. In contrast to traditional databases which store their data on disk, in-memory databases are generally limited in the amount of data they can store due to their reliance on the computer\u0026rsquo;s available RAM."},{"id":126,"href":"/en/hub/Architecture/Kappa-Architecture/","title":"Kappa Architecture","parent":"architecture","content":"Kappa architecture is a big data processing pattern that has historically diverged from [[Lambda Architecture|Lambda]]. Its foundation is to treat all arriving data as a stream, therefore it contains no batch layer by design, relying solely on a [[Stream Data Processing|stream processing]] layer (\u0026ldquo;speed layer\u0026rdquo;).\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A((Message Broker)) subgraph Speed Layer B(\u0026#34;Stream processing job (n)\u0026#34;) C(\u0026#34;Stream processing job (n\u0026#43;1)\u0026#34;) end A --\u0026gt; B A --\u0026gt; C subgraph Serving Layer D(\u0026#34;Output table (n)\u0026#34;) E(\u0026#34;Output table (n\u0026#43;1)\u0026#34;) end B --\u0026gt; D C --\u0026gt; E Kappa Architecture Advantages Only need to maintain, develop and debug a much smaller codebase compared to Lambda architecture. Advantageous for use cases that require high data velocity. Kappa Architecture Disadvantages General challenges related to implementing stream processing at scale. Higher data loss risks by design - requires specific data storage and recovery strategies. Kappa Architecture Learning Resources Questioning the Lambda Architecture – O’Reilly (oreilly.com)\n","description":"Kappa architecture is a big data processing pattern that has historically diverged from [[Lambda Architecture|Lambda]]. Its foundation is to treat all arriving data as a stream, therefore it contains no batch layer by design, relying solely on a [[Stream Data Processing|stream processing]] layer (\u0026ldquo;speed layer\u0026rdquo;).\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A((Message Broker)) subgraph Speed Layer B(\u0026#34;Stream processing job (n)\u0026#34;) C(\u0026#34;Stream processing job (n\u0026#43;1)\u0026#34;) end A --\u0026gt; B A --\u0026gt; C subgraph Serving Layer D(\u0026#34;Output table (n)\u0026#34;) E(\u0026#34;Output table (n\u0026#43;1)\u0026#34;) end B --\u0026gt; D C --\u0026gt; E Kappa Architecture Advantages Only need to maintain, develop and debug a much smaller codebase compared to Lambda architecture."},{"id":127,"href":"/en/hub/Concepts/concepts/Idempotence/","title":"Idempotence","parent":"concepts","content":"Idempotence in the context of data engineering means that if you execute a data pipeline multiple times with the same input, the output will stay the same.\nIdempotence Advantages Keeps data duplicate-free Can remove stale data Saves on storage and cost ","description":"Idempotence in the context of data engineering means that if you execute a data pipeline multiple times with the same input, the output will stay the same.\nIdempotence Advantages Keeps data duplicate-free Can remove stale data Saves on storage and cost "},{"id":128,"href":"/en/hub/Architecture/Medallion-Architecture/","title":"Medallion Architecture","parent":"architecture","content":"A medallion architecture is a data design pattern, coined by Databricks, used to logically organize data in a lakehouse, with the goal of incrementally improving the quality of data as it flows through various layers.\nThis architecture consists of three distinct layers – bronze (raw), silver (validated) and gold (enriched) – each representing progressively higher levels of quality. Medallion architectures are sometimes referred to as \u0026ldquo;multi-hop\u0026rdquo; architectures.\n![[Assets/lakehouse-medallion-architecture.jpeg|800]] \u0026ldquo;Delta Lake Medallion Architecture\u0026rdquo; by Databricks\u0026quot;\nMedallion Stages 🥉 Bronze The bronze stage serves as the initial point for data ingestion and storage. Data is saved without processing or transformation. This might be saving logs from an application to a distributed file system or streaming events from Kafka.\n🥈 Silver The silver layer is where tables are cleaned, filtered, or transformed into a more usable format. Note that the transformations here should be light modifications, not aggregations or enrichments. From our first example, those logs might be parsed slightly to extract useful information— like unnesting structs or eliminating abbreviations. Our events might be standardized to coalesce naming conventions or split a single stream into multiple tables.\n🥇 Gold Finally, in the gold stage, data is refined to meet specific business and analytics requirements. This might mean aggregating data to a certain grain, like daily/hourly, or enriching data with information from external sources. After the gold stage, data should be ready for consumption by downstream teams, like analytics, data science, or ML ops.\nMedallion Architecture Advantages Simple data model: Medallion architectures are familiar to many who have used dbt or warehouse data staging techniques like source ➡️ stage ➡️ curated. Logical progression of data cleanliness: Each stage in a medallion architecture follows a logical pattern that makes it easy for new members to understand. The final stage (gold) used for analytics is entirely separate than the raw stage (bronze) used for ingestion. Allows you to recreate any downstream tables from raw sources.: Since all captured data exists in raw tables, it\u0026rsquo;s possible to recreate downstream tables to add additional columns, rebuild incremental models, or recover data from a disaster. Medallion Architecture Disadvantages Does not replace dimensional modeling techniques: schemas and tables within each layer must still be modeled. Medallion architecture provides a framework for data cleaning, not data architecture. Uses large amounts of storage: though, as many have proclaimed, \u0026ldquo;storage is cheap,\u0026rdquo; a medallion architecture effectively triples the amount of storage used in a data lake. For that reason, it might not be practical for data teams with intensive storage demands. Often requires additional downstream processing: If data engineers are maintaining the medallion architecture, there needs to be a place for analysts/analytics engineers to build business-focused transformations that power BI. Some teams might prefer those processes remain separate, rather than having analysts develop in the gold layer. As such, a medallion architecture is not a drop-in replacement for existing data transformation solutions. Implies a data lakehouse architecture: The medallion architecture is built on the premise of a data lakehouse. If a lakehouse is impractical for your team, this architecture might not make sense. Nonetheless, medallion architectures can be used effectively in hybrid data lake/warehouse implementations. ","description":"A medallion architecture is a data design pattern, coined by Databricks, used to logically organize data in a lakehouse, with the goal of incrementally improving the quality of data as it flows through various layers.\nThis architecture consists of three distinct layers – bronze (raw), silver (validated) and gold (enriched) – each representing progressively higher levels of quality. Medallion architectures are sometimes referred to as \u0026ldquo;multi-hop\u0026rdquo; architectures.\n![[Assets/lakehouse-medallion-architecture.jpeg|800]] \u0026ldquo;Delta Lake Medallion Architecture\u0026rdquo; by Databricks\u0026quot;"},{"id":129,"href":"/en/hub/Concepts/concepts/Denormalization/","title":"Denormalization","parent":"concepts","content":"Denormalization is the process of combining data into a \u0026ldquo;wide\u0026rdquo; tables that are optimized for read workloads. Denormalized tables are best suited for [[Online Analytical Processing|OLAP]] systems where you need to analyze historical data, as updates are not required and data redundancy is not an issue.\nDenormalization Advantages Faster reads of historical/analytical data because fewer joins needed Denormalization Disadvantages Duplicate data ","description":"Denormalization is the process of combining data into a \u0026ldquo;wide\u0026rdquo; tables that are optimized for read workloads. Denormalized tables are best suited for [[Online Analytical Processing|OLAP]] systems where you need to analyze historical data, as updates are not required and data redundancy is not an issue.\nDenormalization Advantages Faster reads of historical/analytical data because fewer joins needed Denormalization Disadvantages Duplicate data "},{"id":130,"href":"/en/hub/Architecture/Data-Architecture/","title":"Data Architecture","parent":"architecture","content":"Data Architecture describes how data is processed, stored, and utilized in an information system.\nPopular Data Architecture Patterns [[Data Lake]] [[Data Mart]] [[Data Mesh]] [[Data Warehouse]] [[Lambda Architecture]] [[Kappa Architecture]] Data Architecture Examples AWS Reference Architecture Examples Azure Architecture Examples GCP Architecture Center ","description":"Data Architecture describes how data is processed, stored, and utilized in an information system.\nPopular Data Architecture Patterns [[Data Lake]] [[Data Mart]] [[Data Mesh]] [[Data Warehouse]] [[Lambda Architecture]] [[Kappa Architecture]] Data Architecture Examples AWS Reference Architecture Examples Azure Architecture Examples GCP Architecture Center "},{"id":131,"href":"/en/hub/Concepts/concepts/Non-relational-Database/","title":"Non-relational Database","parent":"concepts","content":"A non-relational database is a database that does not use a tabular schema of rows and columns. Instead, it uses a storage model that is optimized for the specific type of data being stored. For example, data may be stored as a key/value pair, as JSON, or as a graph consisting of nodes and edges.\nNon-relational Database Advantages Easier to scale ([[Horizontal Scaling|horizontal scaling]]) Better at simpler queries Flexible schema makes development faster Non-relational Database Disadvantages More difficult to query complex data with relationships Flexible schema can become a mess Types of Non-relational Databases [[Key-Value Database]] [[Document Database]] [[Graph Database]] [[In-Memory Database]] [[Search-Engine Database]] [[Timeseries Database]]\n","description":"A non-relational database is a database that does not use a tabular schema of rows and columns. Instead, it uses a storage model that is optimized for the specific type of data being stored. For example, data may be stored as a key/value pair, as JSON, or as a graph consisting of nodes and edges.\nNon-relational Database Advantages Easier to scale ([[Horizontal Scaling|horizontal scaling]]) Better at simpler queries Flexible schema makes development faster Non-relational Database Disadvantages More difficult to query complex data with relationships Flexible schema can become a mess Types of Non-relational Databases [[Key-Value Database]] [[Document Database]] [[Graph Database]] [[In-Memory Database]] [[Search-Engine Database]] [[Timeseries Database]]"},{"id":132,"href":"/en/hub/Concepts/concepts/Horizontal-Scaling/","title":"Horizontal Scaling","parent":"concepts","content":"A horizontally scalable system is one that can increase capacity by adding more computers to the system.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% flowchart LR subgraph Before A[(Computer 1)] end subgraph After B[(Computer 1)] C[(Computer 2)] D[(Computer 3)] end Before --\u0026gt; After Horizontal Scaling Advantages Allows for parallel execution of workloads Increased fault tolerance Cheaper compared to [[Vertical Scaling|vertical scaling]] Horizontal Scaling Disadvantages Decreased consistency Joining data between nodes is more time consuming ","description":"A horizontally scalable system is one that can increase capacity by adding more computers to the system.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% flowchart LR subgraph Before A[(Computer 1)] end subgraph After B[(Computer 1)] C[(Computer 2)] D[(Computer 3)] end Before --\u0026gt; After Horizontal Scaling Advantages Allows for parallel execution of workloads Increased fault tolerance Cheaper compared to [[Vertical Scaling|vertical scaling]] Horizontal Scaling Disadvantages Decreased consistency Joining data between nodes is more time consuming "},{"id":133,"href":"/en/hub/Data-Warehouse/Delta-Load/","title":"Delta Load","parent":"Data Warehouse","content":"A delta load refers to extracting only the data that has changed since the last time the extract process has run. The most commonly used steps to perform a delta load are:\nEnsure there is a modified_at timestamp or incremental id column such as a primary key on the data source. On the initial run of the pipeline, do a full load of the dataset. On following runs of the pipeline, query the target dataset using MAX(column_name). Query the source dataset and filter records where values are greater than the value from step 3. Delta Load Advantages More resource efficient Easy to implement and maintain Only requires read permissions to perform Delta Load Disadvantages Does not capture deleted records Requires extra metadata on the source (commonly a unique id or updated timestamp) Does not capture multiple changes between the polling interval. If a row changes multiple times, you may only capture the latest state. Querying the database for changes may impact the database performance. ","description":"A delta load refers to extracting only the data that has changed since the last time the extract process has run. The most commonly used steps to perform a delta load are:\nEnsure there is a modified_at timestamp or incremental id column such as a primary key on the data source. On the initial run of the pipeline, do a full load of the dataset. On following runs of the pipeline, query the target dataset using MAX(column_name)."},{"id":134,"href":"/en/hub/Concepts/concepts/Full-Load/","title":"Full Load","parent":"concepts","content":"With a full load, the entire dataset is dumped, or loaded, and is then completely replaced (i.e., deleted and replaced) with the new, updated dataset. No additional information, such as timestamps, is required.\nFull Load Advantages Easy to build and maintain A full load won\u0026rsquo;t require you to manage the keys and even if some data is up to date or not, it won\u0026rsquo;t matter since all the data will be updated. Simple design No need to worry about database design. For example, new records that would otherwise invalidate existing data (giving an integer to a column that expects text) don\u0026rsquo;t need to be considered. Full Load Disadvantages Resource and time inefficient Deciding to do a full load on larger datasets will take a great amount of time and other server resources. Ideally, all the data loads are performed overnight with the exception of completing them before users can see the data the next day. There might not be enough time during the overnight window for the full load to complete. Preserving history When dealing with a OLTP source that is not designed to keep history, a full load will remove history from the destination as well, since a full load removes all the records first. Not scalable It can be an inconvenience to load data when only a handful of records need to be updated but millions of records have to be loaded due to the nature of a full load having to update all the records each time. ","description":"With a full load, the entire dataset is dumped, or loaded, and is then completely replaced (i.e., deleted and replaced) with the new, updated dataset. No additional information, such as timestamps, is required.\nFull Load Advantages Easy to build and maintain A full load won\u0026rsquo;t require you to manage the keys and even if some data is up to date or not, it won\u0026rsquo;t matter since all the data will be updated."},{"id":135,"href":"/en/hub/Data-Warehouse/Online-Transaction-Processing/","title":"Online Transaction Processing","parent":"Data Warehouse","content":"The management of transactional data which occurs in the day-to-day operation of an organization. OLTP systems record the transactional data and support the querying of the data.\nTransactional Data Transactional data is data that tracks the activities of an organization and are typically business transactions such as payments received from customers, payments made to vendors, orders made, etc. These transactional events typically contain a timestamp of when it occurred and some numeric values or references to other data.\nOnline Transaction Processing Examples ATM machines and online banking applications Credit card payment processing (both online and in-store) Online bookings (ticketing, reservation systems, etc.) Record keeping (including health records, inventory control, production scheduling, claims processing, customer service ticketing, and many other applications) ","description":"The management of transactional data which occurs in the day-to-day operation of an organization. OLTP systems record the transactional data and support the querying of the data.\nTransactional Data Transactional data is data that tracks the activities of an organization and are typically business transactions such as payments received from customers, payments made to vendors, orders made, etc. These transactional events typically contain a timestamp of when it occurred and some numeric values or references to other data."},{"id":136,"href":"/en/geekdoc/posts/initial-release/","title":"Initial release","parent":"News","content":"This is the first release of the Geekdoc theme.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\n","description":"This is the first release of the Geekdoc theme.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo."},{"id":137,"href":"/en/tags/Documentation/","title":"Documentation","parent":"Tags","content":"","description":""},{"id":138,"href":"/en/geekdoc/posts/hello_geekdoc/","title":"Hello Geekdoc","parent":"News","content":"This is the first release of the Geekdoc theme.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nAmalia id per in minimum facility, quid facet modifier ea ma. Ill um select ma ad, en ferric patine sentient vim. Per expendable foreordained interpretations cu, maxim sole pertinacity in ram. Que no rota alters, ad sea sues exercise main rum, cu diam mas facility sea.\n","description":"This is the first release of the Geekdoc theme.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nAmalia id per in minimum facility, quid facet modifier ea ma. Ill um select ma ad, en ferric patine sentient vim."},{"id":139,"href":"/en/tags/","title":"Tags","parent":"Data Engineering Hub","content":"","description":""},{"id":140,"href":"/en/tags/Updates/","title":"Updates","parent":"Tags","content":"","description":""},{"id":141,"href":"/en/geekdoc/_includes/","title":"Includes","parent":"geekdoc","content":"","description":""},{"id":142,"href":"/en/geekdoc/_includes/include-page/","title":"Include Page","parent":"Includes","content":"Example page include\nExample Shortcode\nShortcode used in an include page. Head 1 Head 2 Head 3 1 2 3 ","description":"Example page include\nExample Shortcode\nShortcode used in an include page. Head 1 Head 2 Head 3 1 2 3 "},{"id":143,"href":"/en/geekdoc/collapse/level-1/","title":"Level 1","parent":"Collapse","content":"Level 1\nLevel 1.1 Level 1.2 ","description":"Level 1\nLevel 1.1 Level 1.2 "},{"id":144,"href":"/en/geekdoc/collapse/level-2/","title":"Level 2","parent":"Collapse","content":"Level-2\n","description":"Level-2"},{"id":145,"href":"/en/geekdoc/toc-tree/level-1/","title":"Level 1","parent":"ToC-Tree","content":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 ","description":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 "},{"id":146,"href":"/en/geekdoc/toc-tree/level-2/","title":"Level 2","parent":"ToC-Tree","content":"Level-2\n","description":"Level-2"},{"id":147,"href":"/en/geekdoc/asciidoc/admonition-icons/","title":"Admonition Icons","parent":"geekdoc","content":" By default, the admonition is rendered with a plain text label. To enable font icons the document attribute :icons: font need to be set.\nExample Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. ","description":"By default, the admonition is rendered with a plain text label. To enable font icons the document attribute :icons: font need to be set.\nExample Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re."},{"id":148,"href":"/en/geekdoc/asciidoc/admonitions/","title":"Admonitions","parent":"geekdoc","content":" Admonition types Example Admonition icons Example Admonition types There are certain statements you may want to draw attention to by taking them out of the content’s flow and labeling them with a priority. These are called admonitions.\n[NOTE|TIP|IMPORTANT|CAUTION|WARNING] Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Example Note Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Tip Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Important Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Caution Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Warning Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re.\nRomanesque acclimates investiture.\nAdmonition icons Icons can be added by setting a unicode glyph or a character reference to the tip-caption attribute:\n:tip-caption: 💡 [TIP] It\u0026#39;s possible to use Unicode glyphs as admonition icons. :tip-caption: pass:[\u0026amp;#128293;] [TIP] It\u0026#39;s possible to use Unicode glyphs as admonition icons. Example 💡 It’s possible to use Unicode glyphs as admonition icons. 🔥 It’s possible to use Unicode glyphs as admonition icons. ","description":"Admonition types Example Admonition icons Example Admonition types There are certain statements you may want to draw attention to by taking them out of the content’s flow and labeling them with a priority. These are called admonitions.\n[NOTE|TIP|IMPORTANT|CAUTION|WARNING] Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Example Note Dolor sit, sumo unique argument um no."},{"id":149,"href":"/en/hub/Data-on-Cloud/Aliyun/","title":"Aliyun","parent":"Data on Cloud","content":"","description":""},{"id":150,"href":"/en/hub/Architecture/","title":"architecture","parent":"Data Engineering Hub","content":" Activity Schema Fan-out Claim Check Pattern Lambda Architecture Metrics Layer Semantic Layer Kappa Architecture Medallion Architecture Data Architecture ","description":" Activity Schema Fan-out Claim Check Pattern Lambda Architecture Metrics Layer Semantic Layer Kappa Architecture Medallion Architecture Data Architecture "},{"id":151,"href":"/en/hub/Data-on-Cloud/AWS/","title":"AWS","parent":"Data on Cloud","content":"","description":""},{"id":152,"href":"/en/hub/Data-on-Cloud/Azure/","title":"Azure","parent":"Data on Cloud","content":"","description":""},{"id":153,"href":"/en/geekdoc/shortcodes/buttons/","title":"Buttons","parent":"Shortcodes","content":"Buttons are styled links that can lead to local page or external link.\nUsage {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;, size=\u0026#34;large|regular\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/thegeeklab/hugo-geekdoc\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} Attributes class optional list List of space-separated CSS class names to apply. Default: none href optional string The URL to use as target of the button. Default: none relref optional string Executes the relref Hugo function to resolve the relative permalink of the specified page. The result is set as the target of the button. Default: none size optional string Preset of different button sizes. Supported values are regular|large. Default: none Example Get Home Contribute ","description":"Buttons are styled links that can lead to local page or external link.\nUsage {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;, size=\u0026#34;large|regular\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/thegeeklab/hugo-geekdoc\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} Attributes class optional list List of space-separated CSS class names to apply. Default: none href optional string The URL to use as target of the button. Default: none relref optional string Executes the relref Hugo function to resolve the relative permalink of the specified page."},{"id":154,"href":"/en/geekdoc/features/code-blocks/","title":"Code Blocks","parent":"Features","content":"There are several ways to add code blocks. Most of them work out of the box, only the Hugo shortcode \u0026lt;highlight\u0026gt; needs to be configured to work properly. The theme also provides some additional features like a copy button and an option to set the maximum length of code blocks. Both of these functions and the dependent formatting rely on the .highlight CSS class. You must ensure that you always assign a language to your code blocks if you want to use these functions. If you do not want to apply syntax highlighting, you can also specify plain or text as the language.\nInline code Code blocks Highlight shortcode Gist Shortcode Inline code To display an inline shortcode use single quotes:\n`some code` Example: some code with a link\nCode blocks Code blocks can be uses without language specification:\n```plain some code ``` Example:\nsome code \u0026hellip; or if you need language specific syntax highlighting:\n```shell # some code echo \u0026#34;Hello world\u0026#34; ``` Example:\n# some code echo \u0026#34;Hello World\u0026#34; Highlight shortcode Hugo has a build-in shortcode for syntax highlighting. To work properly with this theme, you have to set following options in your site configuration:\nTOML pygmentsUseClasses=true pygmentsCodeFences=true YAML pygmentsUseClasses: true pygmentsCodeFences: true You can use it like every other shortcode:\n{{\u0026lt; highlight Shell \u0026#34;linenos=table\u0026#34; \u0026gt;}} # some code echo \u0026#34;Hello World\u0026#34; {{\u0026lt; /highlight \u0026gt;}} Example:\n1 2 # some code echo \u0026#34;Hello World\u0026#34; Gist Shortcode The Gist shortcode is a built-in Hugo shortcode to load GitHub gists. For details usage information please check the Hugo documentation.\n{{\u0026lt; gist spf13 7896402 \u0026gt;}} Example:\n","description":"There are several ways to add code blocks. Most of them work out of the box, only the Hugo shortcode \u0026lt;highlight\u0026gt; needs to be configured to work properly. The theme also provides some additional features like a copy button and an option to set the maximum length of code blocks. Both of these functions and the dependent formatting rely on the .highlight CSS class. You must ensure that you always assign a language to your code blocks if you want to use these functions."},{"id":155,"href":"/en/geekdoc/collapse/","title":"Collapse","parent":"geekdoc","content":"Demo collapsible menu entries.\n","description":"Demo collapsible menu entries."},{"id":156,"href":"/en/hub/Databases/column-database/","title":"column database","parent":"Databases","content":"","description":""},{"id":157,"href":"/en/geekdoc/shortcodes/columns/","title":"Columns","parent":"Shortcodes","content":"The Columns shortcode can be used to organize content side-by-side (horizontally) for better readability.\nUsage {{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; ## Left Content Dolor sit, sumo unique argument um no ... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; ## Mid Content Dolor sit, sumo unique argument um no ... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; ## Right Content Dolor sit, sumo unique argument um no ... {{\u0026lt; /columns \u0026gt;}} Attributes size optional string Preset of different sizes for the first column. Supported values are small|regular|large. Default: regular Example Left Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nMid Content Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re.\nRight Content Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\n","description":"The Columns shortcode can be used to organize content side-by-side (horizontally) for better readability.\nUsage {{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; ## Left Content Dolor sit, sumo unique argument um no ... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; ## Mid Content Dolor sit, sumo unique argument um no ... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; ## Right Content Dolor sit, sumo unique argument um no ... {{\u0026lt; /columns \u0026gt;}} Attributes size optional string Preset of different sizes for the first column."},{"id":158,"href":"/en/hub/Concepts/concepts/","title":"concepts","parent":"Concepts","content":" Data Mesh Data Governance Hybrid Transactional Analytical Processing Data Catalog Database Sharding One Big Table Workflow Orchestration Data Vault Modeling Table Locking Vertical Scaling Change Data Capture Normalization Data Unit Test Indexing Window Function CAP Theorem Data Pipeline Data Lake Idempotence Denormalization Non-relational Database Horizontal Scaling Full Load ","description":" Data Mesh Data Governance Hybrid Transactional Analytical Processing Data Catalog Database Sharding One Big Table Workflow Orchestration Data Vault Modeling Table Locking Vertical Scaling Change Data Capture Normalization Data Unit Test Indexing Window Function CAP Theorem Data Pipeline Data Lake Idempotence Denormalization Non-relational Database Horizontal Scaling Full Load "},{"id":159,"href":"/en/hub/Concepts/","title":"Concepts","parent":"Data Engineering Hub","content":"","description":""},{"id":160,"href":"/en/geekdoc/features/dark-mode/","title":"Dark Mode","parent":"Features","content":"Say hello to the dark mode of the Geekdoc theme!\nThe dark mode can be used in two different ways. If you have JavaScript disabled in your browser, the dark mode automatically detects the preferred system settings via the prefers-color-scheme parameter. Depending on the value, the theme will automatically switch between dark and light mode if this feature is supported by your operating system and browser.\nThe second mode requires JavaScript and is controlled by a dark mode switch in the upper right corner. You can switch between three modes: Auto, Dark and Light. Auto mode works the same as the first method mentioned above and automatically detects the system setting. Dark and Light modes allow you to force one of them for your Geekdoc page only, regardless of the system setting. This works even if your browser or operating system does not support the system setting. The current selection is stored locally via the Web Storage API.\nTo avoid very bright spots often caused by images while using the dark mode we have added an optional auto-dim feature that can be enabled with the site parameter geekdocDarkModeDim (see Configuration). As this may have an impact on the quality of the images it is disabled by default.\n","description":"Say hello to the dark mode of the Geekdoc theme!\nThe dark mode can be used in two different ways. If you have JavaScript disabled in your browser, the dark mode automatically detects the preferred system settings via the prefers-color-scheme parameter. Depending on the value, the theme will automatically switch between dark and light mode if this feature is supported by your operating system and browser.\nThe second mode requires JavaScript and is controlled by a dark mode switch in the upper right corner."},{"id":161,"href":"/en/hub/Data-Analytics/","title":"Data Analytics","parent":"Data Engineering Hub","content":" Propel Metricflow Lightdash Power BI Metriql Apache Superset ","description":" Propel Metricflow Lightdash Power BI Metriql Apache Superset "},{"id":162,"href":"/en/","title":"Data Engineering Hub","parent":"","content":"Welcome! It\u0026rsquo;s a great place for data engineers who want to learn and share their knowledge and ideas through GitHub ⭐ Our GitHub repository and 🗣️ shared wiki Star\nLet\u0026rsquo;s get started~ Database There are various types of databases, such as relational databases, key-value pair databases, document databases, wide column databases, graph databases, analytical databases, etc.\nData Ingestion Data ingestion, or data integration, is the first step of a data platform, and both open source and cloud service providers provide data integration services for different situations.\nData Storage The look and feel can be easily customized by CSS custom properties (variables), features can be adjusted by Hugo parameters.\nData Compute Data computing corresponds to various ETL operations in data warehouse construction, and the focus here is on open source Spark, Flink, and common cloud data computing services.\nData Analytics Data analysis is closely related to the business value of the data platform, and the focus here is on the relationship between data analysis and business value, and how to use business value to drive the construction of the entire data platform.\nData Visualization The data ultimately needs to be presented to the user in a certain format, such as line charts, pie charts, funnel charts, etc., here are several common open-source data visualization tools and business suites.\nData Government Data governance is a comprehensive field that involves many fragmented aspects, such as data quality management, metadata management, data compliance management, data permission control, data lineage, etc., which are introduced in this chapter.\nData Science Data science is a more ambitious topic, and here is just a discussion of common machine learning algorithms and their application scenarios, natural language processing technology, computer vision technology, etc.\nData on Cloud This chapter introduces how to build data platforms in the cloud from cloud vendors such as AWS and Alibaba Cloud, as well as the related services involved.\n","description":"Welcome! It\u0026rsquo;s a great place for data engineers who want to learn and share their knowledge and ideas through GitHub ⭐ Our GitHub repository and 🗣️ shared wiki Star\nLet\u0026rsquo;s get started~ Database There are various types of databases, such as relational databases, key-value pair databases, document databases, wide column databases, graph databases, analytical databases, etc.\nData Ingestion Data ingestion, or data integration, is the first step of a data platform, and both open source and cloud service providers provide data integration services for different situations."},{"id":163,"href":"/en/hub/_Index/","title":"Data Engineering Hub","parent":"Data Engineering Hub","content":" architecture Activity Schema Fan-out Claim Check Pattern Lambda Architecture Metrics Layer Semantic Layer Kappa Architecture Medallion Architecture Data Architecture Concepts concepts Data Mesh Data Governance Hybrid Transactional Analytical Processing Data Catalog Database Sharding One Big Table Workflow Orchestration Data Vault Modeling Table Locking Vertical Scaling Change Data Capture Normalization Data Unit Test Indexing Window Function CAP Theorem Data Pipeline Data Lake Idempotence Denormalization Non-relational Database Horizontal Scaling Full Load formats JSON CSV Apache Parquet Delta Lake Protocol Buffers languages SQL Python Java Scala Data Analytics Propel Metricflow Lightdash Power BI Metriql Apache Superset Data Governance Data Quality Soda Great Expectations Monte Carlo Deequ Choosing your optimal messaging service Cost Optimization in the Cloud SQL Guide Testing Your Data Pipeline Guides Data Governance Guide Getting Started With Data Engineering Data Pipeline Best Practices Data Ingestion dlt Stitch Data Amazon DMS Matillion Data Loader Airbyte Debezium Fivetran Meltano Data on Cloud Aliyun AWS AWS DynamoDB Amazon MSK Benthos Amazon Web Services Azure Microsoft Azure Google Cloud Platform Google Cloud Platform Data Pipeline Azure Data Factory Mage Dagster Prefect AWS Step Functions Apache Airflow Amazon MWAA Data Process Apache Spark RDD example dbt CLI autocomplete in Docker AWS Batch AWS Fargate Amazon ECS AWS Lambda Amazon EC2 Apache Hadoop Apache Spark AWS Glue Amazon EMR Databricks Batch Data Processing Stream Data Processing Data Stroage Amazon S3 Glacier Amazon S3 Data Warehouse Data Mart Relational Modeling Dimensional Modeling Data Modeling Data Warehouse Online Analytical Processing Delta Load Online Transaction Processing Databases relational database Amazon Aurora Amazon RDS MySQL PostgreSQL Microsoft SQL Server Relational Database Management System Relational Database column database Couchbase Column-oriented Database document database Amazon DocumentDB MongoDB Amazon DynamoDB Document Database graph database Graph Database key-value database Redis Key-Value Database In-Memory Database OLAP database Azure Synapse Analytics Amazon Redshift Google BigQuery others ClickHouse Timeseries Database ","description":" architecture Activity Schema Fan-out Claim Check Pattern Lambda Architecture Metrics Layer Semantic Layer Kappa Architecture Medallion Architecture Data Architecture Concepts concepts Data Mesh Data Governance Hybrid Transactional Analytical Processing Data Catalog Database Sharding One Big Table Workflow Orchestration Data Vault Modeling Table Locking Vertical Scaling Change Data Capture Normalization Data Unit Test Indexing Window Function CAP Theorem Data Pipeline Data Lake Idempotence Denormalization Non-relational Database Horizontal Scaling Full Load formats JSON CSV Apache Parquet Delta Lake Protocol Buffers languages SQL Python Java Scala Data Analytics Propel Metricflow Lightdash Power BI Metriql Apache Superset Data Governance Data Quality Soda Great Expectations Monte Carlo Deequ Choosing your optimal messaging service Cost Optimization in the Cloud SQL Guide Testing Your Data Pipeline Guides Data Governance Guide Getting Started With Data Engineering Data Pipeline Best Practices Data Ingestion dlt Stitch Data Amazon DMS Matillion Data Loader Airbyte Debezium Fivetran Meltano Data on Cloud Aliyun AWS AWS DynamoDB Amazon MSK Benthos Amazon Web Services Azure Microsoft Azure Google Cloud Platform Google Cloud Platform Data Pipeline Azure Data Factory Mage Dagster Prefect AWS Step Functions Apache Airflow Amazon MWAA Data Process Apache Spark RDD example dbt CLI autocomplete in Docker AWS Batch AWS Fargate Amazon ECS AWS Lambda Amazon EC2 Apache Hadoop Apache Spark AWS Glue Amazon EMR Databricks Batch Data Processing Stream Data Processing Data Stroage Amazon S3 Glacier Amazon S3 Data Warehouse Data Mart Relational Modeling Dimensional Modeling Data Modeling Data Warehouse Online Analytical Processing Delta Load Online Transaction Processing Databases relational database Amazon Aurora Amazon RDS MySQL PostgreSQL Microsoft SQL Server Relational Database Management System Relational Database column database Couchbase Column-oriented Database document database Amazon DocumentDB MongoDB Amazon DynamoDB Document Database graph database Graph Database key-value database Redis Key-Value Database In-Memory Database OLAP database Azure Synapse Analytics Amazon Redshift Google BigQuery others ClickHouse Timeseries Database "},{"id":164,"href":"/en/hub/Data-Governance/","title":"Data Governance","parent":"Data Engineering Hub","content":" Data Quality Soda Great Expectations Monte Carlo Deequ Choosing your optimal messaging service Cost Optimization in the Cloud SQL Guide Testing Your Data Pipeline Guides Data Governance Guide Getting Started With Data Engineering Data Pipeline Best Practices ","description":" Data Quality Soda Great Expectations Monte Carlo Deequ Choosing your optimal messaging service Cost Optimization in the Cloud SQL Guide Testing Your Data Pipeline Guides Data Governance Guide Getting Started With Data Engineering Data Pipeline Best Practices "},{"id":165,"href":"/en/hub/Data-Ingestion/","title":"Data Ingestion","parent":"Data Engineering Hub","content":" dlt Stitch Data Amazon DMS Matillion Data Loader Airbyte Debezium Fivetran Meltano ","description":" dlt Stitch Data Amazon DMS Matillion Data Loader Airbyte Debezium Fivetran Meltano "},{"id":166,"href":"/en/hub/Data-on-Cloud/","title":"Data on Cloud","parent":"Data Engineering Hub","content":" Aliyun AWS AWS DynamoDB Amazon MSK Benthos Amazon Web Services Azure Microsoft Azure Google Cloud Platform Google Cloud Platform ","description":" Aliyun AWS AWS DynamoDB Amazon MSK Benthos Amazon Web Services Azure Microsoft Azure Google Cloud Platform Google Cloud Platform "},{"id":167,"href":"/en/hub/Data-Pipeline/","title":"Data Pipeline","parent":"Data Engineering Hub","content":"","description":""},{"id":168,"href":"/en/hub/Data-Process/","title":"Data Process","parent":"Data Engineering Hub","content":" Apache Spark RDD example dbt CLI autocomplete in Docker AWS Batch AWS Fargate Amazon ECS AWS Lambda Amazon EC2 Apache Hadoop Apache Spark AWS Glue Amazon EMR Databricks Batch Data Processing Stream Data Processing ","description":" Apache Spark RDD example dbt CLI autocomplete in Docker AWS Batch AWS Fargate Amazon ECS AWS Lambda Amazon EC2 Apache Hadoop Apache Spark AWS Glue Amazon EMR Databricks Batch Data Processing Stream Data Processing "},{"id":169,"href":"/en/hub/Data-Governance/data-quality/","title":"Data Quality","parent":"Data Governance","content":"","description":""},{"id":170,"href":"/en/hub/Data-Storage/","title":"Data Stroage","parent":"Data Engineering Hub","content":" Amazon S3 Glacier Amazon S3 ","description":" Amazon S3 Glacier Amazon S3 "},{"id":171,"href":"/en/hub/Data-Warehouse/","title":"Data Warehouse","parent":"Data Engineering Hub","content":" Data Mart Relational Modeling Dimensional Modeling Data Modeling Data Warehouse Online Analytical Processing Delta Load Online Transaction Processing ","description":" Data Mart Relational Modeling Dimensional Modeling Data Modeling Data Warehouse Online Analytical Processing Delta Load Online Transaction Processing "},{"id":172,"href":"/en/hub/Databases/","title":"Databases","parent":"Data Engineering Hub","content":" relational database Amazon Aurora Amazon RDS MySQL PostgreSQL Microsoft SQL Server Relational Database Management System Relational Database column database Couchbase Column-oriented Database document database Amazon DocumentDB MongoDB Amazon DynamoDB Document Database graph database Graph Database key-value database Redis Key-Value Database In-Memory Database OLAP database Azure Synapse Analytics Amazon Redshift Google BigQuery others ClickHouse Timeseries Database ","description":" relational database Amazon Aurora Amazon RDS MySQL PostgreSQL Microsoft SQL Server Relational Database Management System Relational Database column database Couchbase Column-oriented Database document database Amazon DocumentDB MongoDB Amazon DynamoDB Document Database graph database Graph Database key-value database Redis Key-Value Database In-Memory Database OLAP database Azure Synapse Analytics Amazon Redshift Google BigQuery others ClickHouse Timeseries Database "},{"id":173,"href":"/en/hub/Databases/document-database/","title":"document database","parent":"Databases","content":"","description":""},{"id":174,"href":"/en/geekdoc/shortcodes/expand/","title":"Expand","parent":"Shortcodes","content":"Expand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.\nUsage {{\u0026lt; expand \u0026gt;}} ### Markdown content Dolor sit, sumo unique ... {{\u0026lt; /expand \u0026gt;}} It is also possible to use a custom label and symbol.\n{{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}} ### More markdown Dolor sit, sumo unique ... {{\u0026lt; /expand \u0026gt;}} Example Expand ↕ Markdown content Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re.\nCustom Label ... More markdown Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\n","description":"Expand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.\nUsage {{\u0026lt; expand \u0026gt;}} ### Markdown content Dolor sit, sumo unique ... {{\u0026lt; /expand \u0026gt;}} It is also possible to use a custom label and symbol.\n{{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}} ### More markdown Dolor sit, sumo unique ... {{\u0026lt; /expand \u0026gt;}} Example Expand ↕ Markdown content Dolor sit, sumo unique argument um no."},{"id":175,"href":"/en/hub/Concepts/formats/","title":"formats","parent":"Concepts","content":" JSON CSV Apache Parquet Delta Lake Protocol Buffers ","description":" JSON CSV Apache Parquet Delta Lake Protocol Buffers "},{"id":176,"href":"/en/hub/Data-on-Cloud/GCP/","title":"Google Cloud Platform","parent":"Data on Cloud","content":"","description":""},{"id":177,"href":"/en/hub/Databases/graph-database/","title":"graph database","parent":"Databases","content":"","description":""},{"id":178,"href":"/en/geekdoc/shortcodes/hints/","title":"Hints","parent":"Shortcodes","content":"Hint shortcode can be used as hint/alerts/notification block.\nUsage {{\u0026lt; hint type=[note|tip|important|caution|warning] (icon=gdoc_github) (title=GitHub) \u0026gt;}} **Markdown content**\\ Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. {{\u0026lt; /hint \u0026gt;}} Attributes icon optional string Icon to use. The value need to be an icon from an SVG sprite. Default: none title optional string Title text of the hint. Default: none type optional string Type of the hint. Supported values are note|tip|important|caution|warning. Default: note Example Markdown content\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Markdown content\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Markdown content\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Markdown content\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Markdown content\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re.\nRomanesque acclimates investiture.\nExample with a custom icon and title:\nGitHub Markdown content\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. ","description":"Hint shortcode can be used as hint/alerts/notification block.\nUsage {{\u0026lt; hint type=[note|tip|important|caution|warning] (icon=gdoc_github) (title=GitHub) \u0026gt;}} **Markdown content**\\ Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. {{\u0026lt; /hint \u0026gt;}} Attributes icon optional string Icon to use. The value need to be an icon from an SVG sprite. Default: none title optional string Title text of the hint."},{"id":179,"href":"/en/geekdoc/features/icon-sets/","title":"Icon Sets","parent":"Features","content":" Custom icon sets Build-in icons Custom icon sets The only supported source for custom icons are SVG sprites. Some icon frameworks provides ready to use sprites e.g. FontAwesome. If the framework don\u0026rsquo;t provide sprites, you can create your own from raw SVG icons. There are a lot of tools available to create sprites, please choose one that fits your need. One solution could be svgsprit.es.\nRegardless of which tool (or existing sprite) you choose, there are a few requirements that must be met:\nThe sprite must be a valid SVG file. You have to ensure to hide the sprite. Apply the predefined class svg-sprite or hidden to the root element of your sprite or add a small piece of inline CSS e.g. style=\u0026quot;display: none;\u0026quot;. Save the sprite to the folder assets/sprites right beside your content folder. The result of a valid minimal SVG sprite file could look like this:\n\u0026lt;svg class=\u0026#34;svg-sprite\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; xmlns:xlink=\u0026#34;http://www.w3.org/1999/xlink\u0026#34;\u0026gt; \u0026lt;symbol viewBox=\u0026#34;-2.29 -2.29 28.57 28.57\u0026#34; id=\u0026#34;arrow_back\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34;\u0026gt; \u0026lt;path d=\u0026#34;M24 10.526v2.947H5.755l8.351 8.421-2.105 2.105-12-12 12-12 2.105 2.105-8.351 8.421H24z\u0026#34;/\u0026gt; \u0026lt;/symbol\u0026gt; \u0026lt;/svg\u0026gt; Example:\nFontAwesome provides three pre-build sprites included in the regular Web download pack, sprites/brands.svg, sprites/regular.svg and sprites/solid.svg. Choose your sprite to use and copy it to your projects root directory into assets/sprites, right beside your content folder:\nmy_projcet/ ├── assets │ └── sprites │ └── regular.svg ├── config.yaml ├── content │ ├── _index.md │ ├── ... That\u0026rsquo;s it! The theme will auto-load all available SVG sprites provided in the assets folder. To use the icons e.g. in the bundle menu, you need to lookup the id of the icon. An example would be thumbs-up . There is also a shortcode available.\nBuild-in icons The theme bundles just a small set of hand crafted icons.\n","description":"Custom icon sets Build-in icons Custom icon sets The only supported source for custom icons are SVG sprites. Some icon frameworks provides ready to use sprites e.g. FontAwesome. If the framework don\u0026rsquo;t provide sprites, you can create your own from raw SVG icons. There are a lot of tools available to create sprites, please choose one that fits your need. One solution could be svgsprit.es.\nRegardless of which tool (or existing sprite) you choose, there are a few requirements that must be met:"},{"id":180,"href":"/en/geekdoc/shortcodes/icons/","title":"Icons","parent":"Shortcodes","content":"Simple shortcode to include icons from SVG sprites outside of menus.\nUsage {{\u0026lt; icon \u0026#34;thumbs-up\u0026#34; \u0026gt;}} Example Output Code {{\u0026lt; icon \u0026quot;thumbs-up\u0026quot; \u0026gt;}} {{\u0026lt; icon \u0026quot;thumbs-down\u0026quot; \u0026gt;}} {{\u0026lt; icon \u0026quot;laugh\u0026quot; \u0026gt;}} {{\u0026lt; icon \u0026quot;lemon\u0026quot; \u0026gt;}} {{\u0026lt; icon \u0026quot;moon\u0026quot; \u0026gt;}} ","description":"Simple shortcode to include icons from SVG sprites outside of menus.\nUsage {{\u0026lt; icon \u0026#34;thumbs-up\u0026#34; \u0026gt;}} Example Output Code {{\u0026lt; icon \u0026quot;thumbs-up\u0026quot; \u0026gt;}} {{\u0026lt; icon \u0026quot;thumbs-down\u0026quot; \u0026gt;}} {{\u0026lt; icon \u0026quot;laugh\u0026quot; \u0026gt;}} {{\u0026lt; icon \u0026quot;lemon\u0026quot; \u0026gt;}} {{\u0026lt; icon \u0026quot;moon\u0026quot; \u0026gt;}} "},{"id":181,"href":"/en/geekdoc/shortcodes/images/","title":"Images","parent":"Shortcodes","content":"If you need more flexibility for your embedded images, you could use the img shortcode. It is using Hugo\u0026rsquo;s page resources and supports lazy loading of your images.\nUsage Define your resources in the page front matter, custom parameter params.credits is optional.\n--- resources: - name: forest-1 src: \u0026#34;forest-1.jpg\u0026#34; title: Forest (1) params: credits: \u0026#34;[Jay Mantri](https://unsplash.com/@jaymantri) on [Unsplash](https://unsplash.com/s/photos/forest)\u0026#34; --- {{\u0026lt; img name=\u0026#34;forest-1\u0026#34; size=\u0026#34;large\u0026#34; lazy=false \u0026gt;}} Attributes alt optional string Description text for the image. Default: none lazy optional bool Enable/disable lazy loading for the image. Default: true name required string Mame of the image resource defined in page front matter. Default: none size optional string Thumbnail size. Supported values are origin|profile|tiny|small|medium|large. Default: none Example Forest (1) (Jay Mantri on Unsplash) Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nForest (2) (Jay Mantri on Unsplash) Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nForest (3) (Jay Mantri on Unsplash) Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nForest (4) (Jay Mantri on Unsplash) Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nForest (5) (Jay Mantri on Unsplash) Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nForest (6) (Asher Ward on Unsplash) Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nForest (7) (Asher Ward on Unsplash) Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nForest (8) (SVG Repo on SVGRepo) ","description":"If you need more flexibility for your embedded images, you could use the img shortcode. It is using Hugo\u0026rsquo;s page resources and supports lazy loading of your images.\nUsage Define your resources in the page front matter, custom parameter params.credits is optional.\n--- resources: - name: forest-1 src: \u0026#34;forest-1.jpg\u0026#34; title: Forest (1) params: credits: \u0026#34;[Jay Mantri](https://unsplash.com/@jaymantri) on [Unsplash](https://unsplash.com/s/photos/forest)\u0026#34; --- {{\u0026lt; img name=\u0026#34;forest-1\u0026#34; size=\u0026#34;large\u0026#34; lazy=false \u0026gt;}} Attributes alt optional string Description text for the image."},{"id":182,"href":"/en/geekdoc/shortcodes/includes/","title":"Includes","parent":"Shortcodes","content":"Include shortcode can include files of different types. By specifying a language, the included file will have syntax highlighting.\nUsage {{\u0026lt; include file=\u0026#34;relative/path/from/hugo/root\u0026#34; language=\u0026#34;go\u0026#34; \u0026gt;}} Attributes file required string Path of the file (relative to the Hugo root) to include. Default: none language optional string Language for syntax highlighting. Default: none options optional bool highlighting options. Default: linenos=table type optional string Special include type. Supported values are html|page. If not set the included file is rendered as markdown. Default: none Example Example 1: Markdown file (default) If no other options are specified, files will be rendered as Markdown using the RenderString function.\nLocation of markdown files\nIf you include markdown files that should not get a menu entry, place them outside the content folder or exclude them otherwise. {{\u0026lt; include file=\u0026#34;/static/_includes/example.md.part\u0026#34; \u0026gt;}} Example Mardown include\nFile including a simple Markdown table.\nHead 1 Head 2 Head 3 1 2 3 Example 2: Language files This method can be used to include source code files and keep them automatically up to date.\n{{\u0026lt; include file=\u0026#34;config/_default/config.yaml\u0026#34; language=\u0026#34;yaml\u0026#34; options=\u0026#34;linenos=table,hl_lines=5-6,linenostart=100\u0026#34; \u0026gt;}} Result:\n100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 pygmentsUseClasses: true pygmentsCodeFences: true timeout: 180000 pluralizeListTitles: false defaultContentLanguage: cn disablePathToLower: true enableGitInfo: true enableRobotsTXT: true markup: goldmark: renderer: unsafe: true tableOfContents: startLevel: 1 endLevel: 9 taxonomies: tag: tags outputs: home: - HTML page: - HTML section: - HTML taxonomy: - HTML term: - HTML security: exec: allow: - \u0026#34;^asciidoctor$\u0026#34; Example 3: HTML HTML content will be filtered by the safeHTML filter and added to the rendered page output.\n{{\u0026lt; include file=\u0026#34;/static/_includes/example.html.part\u0026#34; type=\u0026#34;html\u0026#34; \u0026gt;}} Example HTML include This is heading 4 This is heading 5 This is heading 6 Example 4: Hugo Pages In some situations, it can be helpful to include Markdown files that also contain shortcodes. While the default method works fine to render plain Markdown, shortcodes are not parsed. The only way to get this to work is to use Hugo pages. There are several ways to structure these include pages, so whatever you do, keep in mind that Hugo needs to be able to render and serve these files as regular pages! How it works:\nFirst you need to create a directory within your content directory. For this example site _includes is used. To prevent the theme from embedding the page in the navigation, create a file _includes/_index.md and add geekdocHidden: true to the front matter. Place your Markdown files within the _includes folder e.g. /_includes/include-page.md. Make sure to name it *.md. Include the page using {{\u0026lt; include file=\u0026quot;/_includes/include-page.md\u0026quot; type=\u0026quot;page\u0026quot; \u0026gt;}}. Resulting structure should look like this:\n_includes/ ├── include-page.md └── _index.md ","description":"Include shortcode can include files of different types. By specifying a language, the included file will have syntax highlighting.\nUsage {{\u0026lt; include file=\u0026#34;relative/path/from/hugo/root\u0026#34; language=\u0026#34;go\u0026#34; \u0026gt;}} Attributes file required string Path of the file (relative to the Hugo root) to include. Default: none language optional string Language for syntax highlighting. Default: none options optional bool highlighting options. Default: linenos=table type optional string Special include type. Supported values are html|page. If not set the included file is rendered as markdown."},{"id":183,"href":"/en/geekdoc/shortcodes/katex/","title":"KaTeX","parent":"Shortcodes","content":"KaTeX shortcode let you render math typesetting in markdown document.\nUsage {{\u0026lt; katex [display] [class=\u0026#34;text-center\u0026#34;] \u0026gt;}} f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi {{\u0026lt; /katex \u0026gt;}} Attributes class optional list List of space-separated CSS class names to apply. Default: none Example \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] KaTeX can be used inline, for example \\(\\pi(x)\\) or used with the display parameter as above.\n","description":"KaTeX shortcode let you render math typesetting in markdown document.\nUsage {{\u0026lt; katex [display] [class=\u0026#34;text-center\u0026#34;] \u0026gt;}} f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi {{\u0026lt; /katex \u0026gt;}} Attributes class optional list List of space-separated CSS class names to apply. Default: none Example \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] KaTeX can be used inline, for example \\(\\pi(x)\\) or used with the display parameter as above."},{"id":184,"href":"/en/hub/Concepts/languages/","title":"languages","parent":"Concepts","content":" SQL Python Java Scala ","description":" SQL Python Java Scala "},{"id":185,"href":"/en/geekdoc/collapse/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1"},{"id":186,"href":"/en/geekdoc/toc-tree/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1"},{"id":187,"href":"/en/geekdoc/collapse/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2"},{"id":188,"href":"/en/geekdoc/toc-tree/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2"},{"id":189,"href":"/en/geekdoc/toc-tree/level-1/level-1-3/","title":"Level 1.3","parent":"Level 1","content":"Level 1.3\nLevel 1.3.1 ","description":"Level 1.3\nLevel 1.3.1 "},{"id":190,"href":"/en/geekdoc/toc-tree/level-1/level-1-3/level-1-3-1/","title":"Level 1.3.1","parent":"Level 1.3","content":"Level 1.3.1\n","description":"Level 1.3.1"},{"id":191,"href":"/en/geekdoc/collapse/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1"},{"id":192,"href":"/en/geekdoc/toc-tree/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1"},{"id":193,"href":"/en/geekdoc/collapse/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2"},{"id":194,"href":"/en/geekdoc/toc-tree/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2"},{"id":195,"href":"/en/geekdoc/usage/menus/","title":"Menus","parent":"Usage","content":"The theme supports two different kinds of menus. File-tree menu is the default one and does not require further configuration to work. If you want full control about your menu the bundle menu is a powerful option to accomplish it.\nFile-tree menu Bundle menu More menu Extra Header Menu File-tree menu As the name already suggests, the file tree menu builds a menu from the file system structure of the content folder. By default, areas and subareas are sorted alphabetically by the title of the pages. To manipulate the order the weight parameter in a page front matter can be used. To structure your content folder you have to use page bundles, single files are not supported. Hugo will render build single files in the content folder just fine but it will not be added to the menu.\nExample:\nFile system structure:\ncontent/ ├── level-1 │ ├── _index.md │ ├── level-1-1.md │ ├── level-1-2.md │ └── level-1-3 │ ├── _index.md │ └── level-1-3-1.md └── level-2 ├── _index.md ├── level-2-1.md └── level-2-2.md Bundle menu This type of navigation needs to be enabled first by setting geekdocMenuBundle to true in your site configuration. After you have activated the bundle menu, you start with an empty navigation. This is intentional because bundle menus have to be defined manually in a data file. While this increases the effort it also offers maximum flexibility in the design. The data file needs to be written in YAML and placed at data/menu/main.yaml.\nExample:\n--- main: - name: Level 1 ref: \u0026#34;/level-1\u0026#34; icon: \u0026#34;gdoc_notification\u0026#34; sub: - name: Level 1.1 ref: \u0026#34;/level-1/level-1-1\u0026#34; - name: Level 1.2 ref: \u0026#34;/level-1/level-1-2\u0026#34; - name: Level 1.3 ref: \u0026#34;/level-1/level-1-3\u0026#34; sub: - name: Level 1.3.1 ref: \u0026#34;/level-1/level-1-3/level-1-3-1\u0026#34; - name: Level 2 ref: \u0026#34;/level-2\u0026#34; sub: - name: Level 2.1 ref: \u0026#34;/level-2/level-2-1\u0026#34; - name: Level 2.2 ref: \u0026#34;/level-2/level-2-2\u0026#34; - name: Level 2.2 Anchor ref: /level-2/level-2-2 # Anchor to add to the entry. This example will result in `/level-2/level-2-2/#anchor` anchor: anchor As an advantage you can add icons to your menu entries e.g. icon: \u0026quot;gdoc_notification\u0026quot;.\nMore menu Tip\nThe more menu is special type of the bundle menu and can be combined with the default file-tree menu. As this is a special type of the bundle menu it is basically working in the same way. To enable it just add a data file to data/menu/more.yaml. The more menu will also work with the file-tree menu and therefor don\u0026rsquo;t need to be enabled by the geekdocMenuBundle parameter.\nExample:\n--- more: - name: News ref: \u0026#34;/#\u0026#34; icon: \u0026#34;gdoc_notification\u0026#34; - name: Releases ref: \u0026#34;https://github.com/thegeeklab/hugo-geekdoc/releases\u0026#34; external: true icon: \u0026#34;gdoc_download\u0026#34; - name: \u0026#34;View Source\u0026#34; ref: \u0026#34;https://github.com/thegeeklab/hugo-geekdoc\u0026#34; external: true icon: \u0026#34;gdoc_github\u0026#34; Extra Header Menu If you want to customize the header menu, this can be achieved by using a data file written in YAML and placed at data/menu/extra.yaml.\nExample:\n--- header: - name: GitHub ref: https://github.com/thegeeklab/hugo-geekdoc icon: gdoc_github external: true ","description":"The theme supports two different kinds of menus. File-tree menu is the default one and does not require further configuration to work. If you want full control about your menu the bundle menu is a powerful option to accomplish it.\nFile-tree menu Bundle menu More menu Extra Header Menu File-tree menu As the name already suggests, the file tree menu builds a menu from the file system structure of the content folder."},{"id":196,"href":"/en/geekdoc/shortcodes/mermaid/","title":"Mermaid","parent":"Shortcodes","content":"Mermaid is library for generating SVG charts and diagrams from text.\nUsage {{\u0026lt; mermaid class=\u0026#34;text-center\u0026#34; \u0026gt;}} sequenceDiagram Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you? alt is sick Bob-\u0026gt;\u0026gt;Alice: Not so good :( else is well Bob-\u0026gt;\u0026gt;Alice: Feeling fresh like a daisy end opt Extra response Bob-\u0026gt;\u0026gt;Alice: Thanks for asking end {{\u0026lt; /mermaid \u0026gt;}} Attributes class optional list List of space-separated CSS class names to apply. Default: none Example sequenceDiagram Alice-\u003e\u003eBob: Hello Bob, how are you? alt is sick Bob-\u003e\u003eAlice: Not so good :( else is well Bob-\u003e\u003eAlice: Feeling fresh like a daisy end opt Extra response Bob-\u003e\u003eAlice: Thanks for asking end As an alternative to shortcodes, code blocks can be used for markdown as well.\n```mermaid flowchart LR A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` flowchart LR A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ","description":"Mermaid is library for generating SVG charts and diagrams from text. Usage {{\u0026lt; mermaid class=\u0026#34;text-center\u0026#34; \u0026gt;}} sequenceDiagram Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you? alt is sick Bob-\u0026gt;\u0026gt;Alice: Not so good :( else is well Bob-\u0026gt;\u0026gt;Alice: Feeling fresh like a daisy end opt Extra response Bob-\u0026gt;\u0026gt;Alice: Thanks for asking end {{\u0026lt; /mermaid \u0026gt;}} Attributes class optional list List of space-separated CSS class names to apply. Default: none Example sequenceDiagram Alice-\u003e\u003eBob: Hello Bob, how are you?"},{"id":197,"href":"/en/geekdoc/features/multilingual/","title":"Multilingual","parent":"Features","content":" Configuration Languages Translation Strings Menus Add Content Switch Content Hugo supports the creation of websites with multiple languages. In this post we will explain how to get configure Multilingual Mode with this theme.\nConfiguration Languages You need to set a default language and configure at least two different languages used by your site to your configuration file at config.toml:\ndefaultContentLanguage = \u0026#34;en\u0026#34; [languages.en] languageName = \u0026#34;English\u0026#34; contentDir = \u0026#34;content/en\u0026#34; weight = 10 [languages.de] languageName = \u0026#34;German\u0026#34; contentDir = \u0026#34;content/de\u0026#34; weight = 20 Translation Strings To customize translation strings used by the theme you can create a file i18n/\u0026lt;languagecode\u0026gt;.toml for every language you want to use e.g. i18n/en.toml. You can lookup all used strings in the default translation file.\nMenus For the Bundle Menu as well as for the Extra Header Menu you can translate the name within the data file of the menu:\n--- more: # If `name` is a text, this text will be used as name for each language. - name: News ref: \u0026#34;/#\u0026#34; icon: \u0026#34;gdoc_notification\u0026#34; # To translate the name you can add a sub-item per language. Important: If you miss a language key # that is configured in the languages list of your `config.toml` the name will be empty for this language! - name: en: Releases de: Veröffentlichung ref: \u0026#34;https://github.com/thegeeklab/hugo-geekdoc/releases\u0026#34; external: true icon: \u0026#34;gdoc_download\u0026#34; Add Content To translate your content you need to create a directory content/\u0026lt;languagecode\u0026gt;/ for each language you want to use e.g. content/en/. This language directories will hold the translated pages for the particular language.\nSwitch Content If you have configured at least two different languages, the language switcher will be enabled in the UI automatically. The switcher is as part of the header menu and displayed on all pages.\nOn pages for which a translation is available it will be displayed in the selection list and links to the translated page.\nPages without a translation will be displayed in the selection list as well but are marked with an asterisk and link to the start page of the respective language.\n","description":"Configuration Languages Translation Strings Menus Add Content Switch Content Hugo supports the creation of websites with multiple languages. In this post we will explain how to get configure Multilingual Mode with this theme.\nConfiguration Languages You need to set a default language and configure at least two different languages used by your site to your configuration file at config.toml:\ndefaultContentLanguage = \u0026#34;en\u0026#34; [languages.en] languageName = \u0026#34;English\u0026#34; contentDir = \u0026#34;content/en\u0026#34; weight = 10 [languages."},{"id":198,"href":"/en/geekdoc/shortcodes/progress/","title":"Progress","parent":"Shortcodes","content":"A progress bar shows how far a process has progressed.\nUsage {{\u0026lt; progress title=Eating value=65 icon=gdoc_heart \u0026gt;}} Attributes icon optional string Icon to use. The value need to be an icon from an SVG sprite. Default: none title optional string Title text of the progress bar. Default: none value optional integer Progress value. Default: 0 Example Eating 65% ","description":"A progress bar shows how far a process has progressed.\nUsage {{\u0026lt; progress title=Eating value=65 icon=gdoc_heart \u0026gt;}} Attributes icon optional string Icon to use. The value need to be an icon from an SVG sprite. Default: none title optional string Title text of the progress bar. Default: none value optional integer Progress value. Default: 0 Example Eating 65% "},{"id":199,"href":"/en/geekdoc/shortcodes/propertylist/","title":"Properties","parent":"Shortcodes","content":"The property list shortcode creates a custom HTML description list that can be used to display properties or variables and general dependent information. The shortcode requires a data file in data/properties/, e.g. data/properties/demo.yaml.\nUsage {{\u0026lt; propertylist name=demo (sort=name) (order=[asc|desc]) \u0026gt;}} The supported attributes can be taken from the following example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 --- properties: - name: prop1 type: string description: Dummy description of the prop1 string property. required: true - name: prop2 type: int defaultValue: 10 description: en: Another description for the integer property called prop2. required: false tags: en: - tag1 - tag2 - name: prop3 type: bool defaultValue: false description: | A `bool` property with a complex multiline description and embedded Markdown: - List item 1 - List item 2 More description how to use this property. required: false - name: a-prop type: string description: Property to demonstrate sorting. required: true Attributes class optional list List of space-separated CSS class names to apply. Default: none href optional string The URL to use as target of the button. Default: none relref optional string Executes the relref Hugo function to resolve the relative permalink of the specified page. The result is set as the target of the button. Default: none size optional string Preset of different button sizes. Supported values are regular|large. Default: none Example a-prop required string Property to demonstrate sorting. Default: none prop1 required string Dummy description of the prop1 string property. Default: none prop2 optional int tag1 tag2 Another description for the integer property called prop2. Default: 10 prop3 optional bool A bool property with a complex multiline description and embedded Markdown:\nList item 1 List item 2 More description how to use this property.\nDefault: false ","description":"The property list shortcode creates a custom HTML description list that can be used to display properties or variables and general dependent information. The shortcode requires a data file in data/properties/, e.g. data/properties/demo.yaml.\nUsage {{\u0026lt; propertylist name=demo (sort=name) (order=[asc|desc]) \u0026gt;}} The supported attributes can be taken from the following example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 --- properties: - name: prop1 type: string description: Dummy description of the prop1 string property."},{"id":200,"href":"/en/geekdoc/shortcodes/tabs/","title":"Tabs","parent":"Shortcodes","content":"Tabs let you organize content by context, for example installation instructions for each supported platform.\nUsage {{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}} {{\u0026lt; tab \u0026#34;macOS\u0026#34; \u0026gt;}} # macOS Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}} # Linux Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}} # Windows Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; /tabs \u0026gt;}} Example macOS macOS This is tab macOS content.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nLinux Linux This is tab Linux content.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nWindows Windows This is tab Windows content.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent.\n","description":"Tabs let you organize content by context, for example installation instructions for each supported platform.\nUsage {{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}} {{\u0026lt; tab \u0026#34;macOS\u0026#34; \u0026gt;}} # macOS Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}} # Linux Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}} # Windows Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; /tabs \u0026gt;}} Example macOS macOS This is tab macOS content.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv."},{"id":201,"href":"/en/geekdoc/features/theming/","title":"Theming","parent":"Features","content":" Color Scheme Favicons Simple replacement Full replacement Color Scheme If you want to customize the theme\u0026rsquo;s color scheme to give it your individual touch, you are only a few lines of CSS away. Generally, you need to override the default settings. The easiest way to do this is to create a file named static/custom.css right at the root of your site.\nAll the necessary CSS customization properties are listed below. If you want to customize elements that don\u0026rsquo;t use these properties, you can always look up the class name and override it directly. For inspiration, you can also take a look at https://www.color-hex.com. In this simple example, we\u0026rsquo;ll use the Beach color palette.\nCustom CSS:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 /* Global customization */ :root { --code-max-height: 60rem; } /* Light mode theming */ :root, :root[color-theme=\u0026#34;light\u0026#34;] { --header-background: #4ec58a; --header-font-color: #ffffff; --body-background: #ffffff; --body-font-color: #343a40; --mark-color: #ffab00; --button-background: #62cb97; --button-border-color: #4ec58a; --link-color: #518169; --link-color-visited: #c54e8a; --code-background: #f5f6f8; --code-accent-color: #e3e7eb; --code-accent-color-lite: #eff1f3; --code-font-color: #5f5f5f; --code-copy-background: #f5f6f8; --code-copy-font-color: #6b7784; --code-copy-border-color: #adb4bc; --code-copy-success-color: #00c853; --accent-color: #e9ecef; --accent-color-lite: #f8f9fa; --control-icons: #b2bac1; --footer-background: #112b3c; --footer-font-color: #ffffff; --footer-link-color: #ffcc5c; --footer-link-color-visited: #ffcc5c; } @media (prefers-color-scheme: light) { :root { --header-background: #4ec58a; --header-font-color: #ffffff; --body-background: #ffffff; --body-font-color: #343a40; --mark-color: #ffab00; --button-background: #62cb97; --button-border-color: #4ec58a; --link-color: #518169; --link-color-visited: #c54e8a; --code-background: #f5f6f8; --code-accent-color: #e3e7eb; --code-accent-color-lite: #eff1f3; --code-font-color: #5f5f5f; --code-copy-background: #f5f6f8; --code-copy-font-color: #6b7784; --code-copy-border-color: #adb4bc; --code-copy-success-color: #00c853; --accent-color: #e9ecef; --accent-color-lite: #f8f9fa; --control-icons: #b2bac1; --footer-background: #112b3c; --footer-font-color: #ffffff; --footer-link-color: #ffcc5c; --footer-link-color-visited: #ffcc5c; } } /* Dark mode theming */ :root[color-theme=\u0026#34;dark\u0026#34;] { --header-background: #4ec58a; --header-font-color: #ffffff; --body-background: #343a40; --body-font-color: #ced3d8; --mark-color: #ffab00; --button-background: #62cb97; --button-border-color: #4ec58a; --link-color: #7ac29e; --link-color-visited: #c27a9e; --code-background: #2f353a; --code-accent-color: #262b2f; --code-accent-color-lite: #2b3035; --code-font-color: #b9b9b9; --code-copy-background: #343a40; --code-copy-font-color: #6b7784; --code-copy-border-color: #6b7784; --code-copy-success-color: #37905c; --accent-color: #2b3035; --accent-color-lite: #2f353a; --control-icons: #b2bac1; --footer-background: #112b3c; --footer-font-color: #ffffff; --footer-link-color: #ffcc5c; --footer-link-color-visited: #ffcc5c; } @media (prefers-color-scheme: dark) { :root { --header-background: #4ec58a; --header-font-color: #ffffff; --body-background: #343a40; --body-font-color: #ced3d8; --mark-color: #ffab00; --button-background: #62cb97; --button-border-color: #4ec58a; --link-color: #7ac29e; --link-color-visited: #c27a9e; --code-background: #2f353a; --code-accent-color: #262b2f; --code-accent-color-lite: #2b3035; --code-font-color: #b9b9b9; --code-copy-background: #343a40; --code-copy-font-color: #6b7784; --code-copy-border-color: #6b7784; --code-copy-success-color: #37905c; --accent-color: #2b3035; --accent-color-lite: #2f353a; --control-icons: #b2bac1; --footer-background: #112b3c; --footer-font-color: #ffffff; --footer-link-color: #ffcc5c; --footer-link-color-visited: #ffcc5c; } } Favicons The Theme is shipped with a set of default Favicons in various formats generated by the Favicon Generator. All files can be found in the static/favicon folder of the release tarball. To make the replacement of the default Favicons as simple as possible, the theme loads only a very small subset of the Favicon formats.\n\u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/svg+xml\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/favicon.svg\u0026#34; | relURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;32x32\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/favicon-32x32.png\u0026#34; | relURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;16x16\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/favicon-16x16.png\u0026#34; | relURL }}\u0026#34;\u0026gt; Simple replacement The minimal steps to load a custom Favicon is to overwrite the three default Favicon files. Therefor place these files into your projects root folder:\nstatic/favicon/favicon.svg static/favicon/favicon-32x32.png static/favicon/favicon-16x16.png Full replacement If you want to add more Favicon formats you have to overwrite the default partial that is used to load the files. In the next step you have to place the required files in the static folder of your project as well.\nExample:\n\u0026lt;!-- layouts/partials/head/favicons.html --\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/svg+xml\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/favicon.svg\u0026#34; | relURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;apple-touch-icon\u0026#34; sizes=\u0026#34;180x180\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/apple-touch-icon.png\u0026#34; | relURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;32x32\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/favicon-32x32.png\u0026#34; | relURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;16x16\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/favicon-16x16.png\u0026#34; | relURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;manifest\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/site.webmanifest\u0026#34; | relURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;mask-icon\u0026#34; href=\u0026#34;{{ \u0026#34;favicon/safari-pinned-tab.svg\u0026#34; | relURL }}\u0026#34; color=\u0026#34;#efefef\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;msapplication-TileColor\u0026#34; content=\u0026#34;#efefef\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;theme-color\u0026#34; content=\u0026#34;#efefef\u0026#34;\u0026gt; Happy customizing!\n","description":"Color Scheme Favicons Simple replacement Full replacement Color Scheme If you want to customize the theme\u0026rsquo;s color scheme to give it your individual touch, you are only a few lines of CSS away. Generally, you need to override the default settings. The easiest way to do this is to create a file named static/custom.css right at the root of your site.\nAll the necessary CSS customization properties are listed below."},{"id":202,"href":"/en/geekdoc/shortcodes/toc/","title":"ToC","parent":"Shortcodes","content":"Simple wrapper to generate a page Table of Content from a shortcode.\nUsage {{\u0026lt; toc (format=[html|raw]) \u0026gt;}} Attributes format optional string Format of the returned ToC. The html format creates an HTML wrapper to enable the geekdocToC parameter that limits the maximum ToC level to be displayed. This variant also automatically inserts a horizontal line after the ToC. The raw format returns the unformatted ToC, the parameter geekdocToC does not work in this mode. Supported values are html|raw. Default: html Example Usage Attributes Example Level 1 Level 2 Level 2.1 Level 2.1.1 Level 2.1.1.1 Level 2.1.1.1.1 Level 2.2 Level 1 Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nLevel 2 Amalia id per in minimum facility, quid facet modifier ea ma. Ill um select ma ad, en ferric patine sentient vim. Per expendable foreordained interpretations cu, maxim sole pertinacity in ram.\nLevel 2.1 Amalia id per in minimum facility, quid facet modifier ea ma. Ill um select ma ad, en ferric patine sentient vim. Per expendable foreordained interpretations cu, maxim sole pertinacity in ram.\nLevel 2.1.1 Amalia id per in minimum facility, quid facet modifier ea ma. Ill um select ma ad, en ferric patine sentient vim.\nLevel 2.1.1.1 In pro quints consequent, denim fastidious copious quo ad.\nLevel 2.1.1.1.1 In pro quints consequent, denim fastidious copious quo ad.\nLevel 2.2 Dolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e.\nAmalia id per in minimum facility, quid facet modifier ea ma. Ill um select ma ad, en ferric patine sentient vim. Per expendable foreordained interpretations cu, maxim sole pertinacity in ram.\n","description":"Simple wrapper to generate a page Table of Content from a shortcode.\nUsage {{\u0026lt; toc (format=[html|raw]) \u0026gt;}} Attributes format optional string Format of the returned ToC. The html format creates an HTML wrapper to enable the geekdocToC parameter that limits the maximum ToC level to be displayed. This variant also automatically inserts a horizontal line after the ToC. The raw format returns the unformatted ToC, the parameter geekdocToC does not work in this mode."},{"id":203,"href":"/en/geekdoc/shortcodes/toc-tree/","title":"ToC-Tree","parent":"Shortcodes","content":"The toc-tree shortcode will generate a Table of Content from a section file tree of your content directory. The root of the resulting ToC will be the page on which you define the shortcode.\nUsage {{\u0026lt; toc-tree [sortBy=\u0026#34;title\u0026#34;] \u0026gt;}} Attributes sortBy optional string Override the default sort parameter set by geekdocFileTreeSortBy. Default: .Site.Params.geekdocFileTreeSortBy Example As said, the root will be the site on which the shortcode was used, you can see a demo including nesting in the ToC Tree section.\n","description":"The toc-tree shortcode will generate a Table of Content from a section file tree of your content directory. The root of the resulting ToC will be the page on which you define the shortcode.\nUsage {{\u0026lt; toc-tree [sortBy=\u0026#34;title\u0026#34;] \u0026gt;}} Attributes sortBy optional string Override the default sort parameter set by geekdocFileTreeSortBy. Default: .Site.Params.geekdocFileTreeSortBy Example As said, the root will be the site on which the shortcode was used, you can see a demo including nesting in the ToC Tree section."},{"id":204,"href":"/en/geekdoc/toc-tree/","title":"ToC-Tree","parent":"geekdoc","content":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 ","description":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 "}]