[{"id":0,"href":"/hub/Data-Ingestion/Apache-Kafka%E8%AF%A6%E8%A7%A3/","title":"Apache Kafka详解","parent":"数据摄取","content":"Apache Kafka is an Event Streaming platform\nKafka结合了三个关键功能，让您可以使用经过验证的解决方案端对端实现事件流的用例：\n发布（写入）和订阅（读取）事件流，包括不间断地将数据从其他系统导入/导出。 可靠地持久存储事件流，时间长短由您决定。 实时或回顾性地处理事件流。 Kafka的重要概念:\nProducer Consumer Topic Partition Consumer Group Offset Kafka Server Kafka Client Kafka admin Kafka Connect Kafka Stream Kafka为什么设置消费者组 Kafka 中的消费者组具有重要意义，让我们深入了解一下：\n并行处理：消费者组允许多个消费者协同工作，同时从一个或多个主题（Topic）中消费消息。每个消费者都可以处理不同的分区（Partitions），从而实现并行处理。这对于大规模数据流的实时处理非常有用。\n负载均衡：当一个主题被多个消费者订阅时，Kafka 会自动将分区分配给消费者组中的不同消费者。这确保了每个消费者都能够处理一部分分区，从而实现负载均衡。如果有新的消费者加入或离开消费者组，分区的重新分配也会自动发生。\n容错性：如果某个消费者崩溃或离线，其他消费者仍然可以继续处理消息。Kafka 会将该消费者负责的分区重新分配给其他活动的消费者，从而确保消息不会丢失。\n消费者状态管理：Kafka 维护每个消费者的消费进度。这意味着即使消费者组中的某个消费者离线，它重新加入时也能从上次消费的位置继续处理消息。\n总之，消费者组是 Kafka 中实现高吞吐量、容错性和并行处理的关键机制之一。\nKakfa 如何保证消息不丢失（message durability） Kafka 通过多种机制来保证消息不丢失：\n消息复制：Kafka 使用副本来确保消息的持久性。当消息被发布到**主分区（leader partition）时，它可以配置为在跟随分区（follower partitions）**之间同步复制。这意味着如果持有主分区的代理（broker）崩溃，副本可以接替主分区的角色，而不会丢失数据²³.\n消息确认机制：生产者可以配置**消息确认（acks）级别。默认情况下，设置为acks=all（或acks=-1），这意味着发送请求在数据被写入主副本和ISR（in-sync replica）**列表中的所有跟随副本之前不会被确认。这确保了消息在多个副本之间的持久性和一致性⁴.\nISR 列表：Kafka 维护一个ISR 列表，其中包含与主分区同步的副本。只有在ISR列表中的副本才能成为主分区的候选副本。这确保了消息在可用的副本之间进行复制，从而提高了消息的可靠性和持久性⁵.\n总之，Kafka 的设计和配置使其成为一个可靠的消息传递系统，确保消息不会丢失。\nkafka 可以保证消息是有序消费的吗？ Kafka 可以通过一些机制来保证消息的有序消费，但需要注意以下几点：\n消息有序性：我们需要从两个方面看待消息的有序性：\n首先，发送端能否保证发送到服务器的消息是有序的； 其次，接收端能否有序地消费服务器中的数据。 发送端消息有序性：\nKafka 分布式的单位是 partition（分区）。同一个 partition 使用一个 write ahead log 组织，因此可以保证 FIFO 的顺序。 不同 partition 之间不能保证顺序。但是绝大多数用户可以通过 message key 来定义，因为同一个 key 的消息可以保证只发送到同一个 partition。例如，使用 user ID 或 table row ID 作为 key，这样同一个用户或记录的消息永远只会发送到同一个 partition 上，从而保证了顺序。 客户端消息发送原理：\n在 Kafka 中，消息发送是异步的。在新版本的 Kafka 中，只有一种异步方式，即批量发送。 在 producer 端，存在两个线程： 主线程：用户端调用 send 方法时，数据被缓存到 RecordAccumulator 中，send 方法立即返回，但此时并不能确定消息是否真正发送到 broker。 sender IO 线程：不断轮询 RecordAccumulator，满足一定条件后，进行真正的网络 IO 发送，使用异步非阻塞的 NIO。 ProducerRecord 对象包含以下字段： 如果在发送时指定了 partition，消息将保存到指定的 partition 队列。 如果没有指定分区，将对 key 散列后计算分区，相同 key 的消息将被写到同一个分区队列中。 总结：\n通过队列，保证 partition 上的数据元素是有序的。 通过设置相同的路由，让多个数据被路由到同一个 partition 即可。 Kafka的读写速度为什么惊人的快？ Kafka 之所以具有出色的读写速度，是因为它在设计和实现上采用了一些关键策略和技术。以下是一些原因：\n分布式架构：Kafka 是一个分布式系统，它将数据分散到多个 broker（服务器）上。这样可以充分利用集群中的计算和存储资源，从而提高整体的读写速度。\n分区和副本：Kafka 中的主题被分成多个 partition（分区）。每个 partition 可以在不同的 broker 上有多个副本。这种设计允许多个消费者并行地读取数据，同时保证数据的冗余和可靠性。\n顺序写入：Kafka 的写入操作是顺序的。消息首先被追加到 partition 的日志文件中，然后再由 broker 处理。这种顺序写入的方式对于磁盘性能非常友好，因为它最大程度地减少了磁盘寻道时间。\n零拷贝技术：Kafka 使用零拷贝技术来最小化数据在内存和磁盘之间的复制。这样可以减少 CPU 和内存的开销，提高性能。\n批量处理：Kafka 支持批量处理，即一次性处理多条消息。这样可以减少网络传输和磁盘写入的次数，提高效率。\n异步处理：Kafka 的生产者和消费者是异步的。生产者可以缓冲多条消息，然后一次性发送到 broker。消费者也可以异步地拉取数据。这种异步处理方式可以提高吞吐量。\n总之，Kafka 的高性能和读写速度得益于其分布式架构、顺序写入、零拷贝技术以及其他优化策略。\n","description":"Apache Kafka is an Event Streaming platform\nKafka结合了三个关键功能，让您可以使用经过验证的解决方案端对端实现事件流的用例：\n发布（写入）和订阅（读取）事件流，包括不间断地将数据从其他系统导入/导出。 可靠地持久存储事件流，时间长短由您决定。 实时或回顾性地处理事件流。 Kafka的重要概念:\nProducer Consumer Topic Partition Consumer Group Offset Kafka Server Kafka Client Kafka admin Kafka Connect Kafka Stream Kafka为什么设置消费者组 Kafka 中的消费者组具有重要意义，让我们深入了解一下：\n并行处理：消费者组允许多个消费者协同工作，同时从一个或多个主题（Topic）中消费消息。每个消费者都可以处理不同的分区（Partitions），从而实现并行处理。这对于大规模数据流的实时处理非常有用。\n负载均衡：当一个主题被多个消费者订阅时，Kafka 会自动将分区分配给消费者组中的不同消费者。这确保了每个消费者都能够处理一部分分区，从而实现负载均衡。如果有新的消费者加入或离开消费者组，分区的重新分配也会自动发生。\n容错性：如果某个消费者崩溃或离线，其他消费者仍然可以继续处理消息。Kafka 会将该消费者负责的分区重新分配给其他活动的消费者，从而确保消息不会丢失。\n消费者状态管理：Kafka 维护每个消费者的消费进度。这意味着即使消费者组中的某个消费者离线，它重新加入时也能从上次消费的位置继续处理消息。\n总之，消费者组是 Kafka 中实现高吞吐量、容错性和并行处理的关键机制之一。\nKakfa 如何保证消息不丢失（message durability） Kafka 通过多种机制来保证消息不丢失：\n消息复制：Kafka 使用副本来确保消息的持久性。当消息被发布到**主分区（leader partition）时，它可以配置为在跟随分区（follower partitions）**之间同步复制。这意味着如果持有主分区的代理（broker）崩溃，副本可以接替主分区的角色，而不会丢失数据²³.\n消息确认机制：生产者可以配置**消息确认（acks）级别。默认情况下，设置为acks=all（或acks=-1），这意味着发送请求在数据被写入主副本和ISR（in-sync replica）**列表中的所有跟随副本之前不会被确认。这确保了消息在多个副本之间的持久性和一致性⁴.\nISR 列表：Kafka 维护一个ISR 列表，其中包含与主分区同步的副本。只有在ISR列表中的副本才能成为主分区的候选副本。这确保了消息在可用的副本之间进行复制，从而提高了消息的可靠性和持久性⁵.\n总之，Kafka 的设计和配置使其成为一个可靠的消息传递系统，确保消息不会丢失。\nkafka 可以保证消息是有序消费的吗？ Kafka 可以通过一些机制来保证消息的有序消费，但需要注意以下几点：\n消息有序性：我们需要从两个方面看待消息的有序性：\n首先，发送端能否保证发送到服务器的消息是有序的； 其次，接收端能否有序地消费服务器中的数据。 发送端消息有序性：\nKafka 分布式的单位是 partition（分区）。同一个 partition 使用一个 write ahead log 组织，因此可以保证 FIFO 的顺序。 不同 partition 之间不能保证顺序。但是绝大多数用户可以通过 message key 来定义，因为同一个 key 的消息可以保证只发送到同一个 partition。例如，使用 user ID 或 table row ID 作为 key，这样同一个用户或记录的消息永远只会发送到同一个 partition 上，从而保证了顺序。 客户端消息发送原理："},{"id":1,"href":"/hub/Data-Storage/format/Avro/","title":"Avro","parent":"文件格式","content":"Avro文件格式的优势和劣势如下：\n优势：\n紧凑的数据存储：Avro使用二进制格式存储数据，相比于文本格式，可以更加紧凑地存储数据，节省存储空间。 快速的数据序列化和反序列化：Avro采用了基于模式的数据序列化和反序列化机制，能够快速地将数据转换为二进制格式或从二进制格式还原为数据对象。 动态数据模式：Avro文件格式支持动态数据模式，可以在数据写入和读取时使用不同的模式，而无需事先定义固定的数据结构。 跨语言支持：Avro提供了多种编程语言的实现，支持跨语言的数据交换和处理。 劣势：\n不适合人类阅读：由于Avro使用二进制格式存储数据，对人类来说不太友好，无法像文本格式那样直接查看内容。 不支持追加操作：Avro文件格式一般用于存储静态数据，不支持在文件末尾追加数据，需要重新写入整个文件才能更新数据。 需要额外的模式管理：由于Avro支持动态数据模式，需要额外的模式管理机制来确保数据的正确序列化和反序列化，增加了复杂性。 可能存在兼容性问题：由于Avro文件格式依赖于数据模式，如果数据模式发生变化，可能会导致兼容性问题，需要额外的处理来处理不同版本的数据。 示例 以下是一个使用Python实现写入和读取Avro文件的示例：\nfrom avro import schema, datafile, io # 定义Avro数据模式 schema_str = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;} ] } \u0026#34;\u0026#34;\u0026#34; avro_schema = schema.Parse(schema_str) # 写入Avro文件 with open(\u0026#39;users.avro\u0026#39;, \u0026#39;wb\u0026#39;) as f: writer = datafile.DataFileWriter(f, io.DatumWriter(), avro_schema) writer.append({\u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: 30}) writer.append({\u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;age\u0026#34;: 25}) writer.close() # 读取Avro文件 with open(\u0026#39;users.avro\u0026#39;, \u0026#39;rb\u0026#39;) as f: reader = datafile.DataFileReader(f, io.DatumReader()) for user in reader: print(user) reader.close() 在这个示例中，首先定义了一个简单的Avro数据模式，然后使用DataFileWriter将数据写入到Avro文件users.avro中，接着使用DataFileReader读取并打印文件中的数据。你可以根据实际需求修改数据模式和数据内容。\n","description":"Avro文件格式的优势和劣势如下：\n优势：\n紧凑的数据存储：Avro使用二进制格式存储数据，相比于文本格式，可以更加紧凑地存储数据，节省存储空间。 快速的数据序列化和反序列化：Avro采用了基于模式的数据序列化和反序列化机制，能够快速地将数据转换为二进制格式或从二进制格式还原为数据对象。 动态数据模式：Avro文件格式支持动态数据模式，可以在数据写入和读取时使用不同的模式，而无需事先定义固定的数据结构。 跨语言支持：Avro提供了多种编程语言的实现，支持跨语言的数据交换和处理。 劣势：\n不适合人类阅读：由于Avro使用二进制格式存储数据，对人类来说不太友好，无法像文本格式那样直接查看内容。 不支持追加操作：Avro文件格式一般用于存储静态数据，不支持在文件末尾追加数据，需要重新写入整个文件才能更新数据。 需要额外的模式管理：由于Avro支持动态数据模式，需要额外的模式管理机制来确保数据的正确序列化和反序列化，增加了复杂性。 可能存在兼容性问题：由于Avro文件格式依赖于数据模式，如果数据模式发生变化，可能会导致兼容性问题，需要额外的处理来处理不同版本的数据。 示例 以下是一个使用Python实现写入和读取Avro文件的示例：\nfrom avro import schema, datafile, io # 定义Avro数据模式 schema_str = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;} ] } \u0026#34;\u0026#34;\u0026#34; avro_schema = schema.Parse(schema_str) # 写入Avro文件 with open(\u0026#39;users.avro\u0026#39;, \u0026#39;wb\u0026#39;) as f: writer = datafile.DataFileWriter(f, io.DatumWriter(), avro_schema) writer.append({\u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: 30}) writer.append({\u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;age\u0026#34;: 25}) writer.close() # 读取Avro文件 with open(\u0026#39;users.avro\u0026#39;, \u0026#39;rb\u0026#39;) as f: reader = datafile."},{"id":2,"href":"/hub/Data-Storage/NOSQL-DB/column-database/hbase/","title":"hbase","parent":"列存储数据库","content":" Hbase优势和劣势 HBase是一个分布式、面向列的NoSQL数据库，它有一些优势和劣势。\n优势：\n高可靠性：HBase具有自动数据复制和故障转移功能，可以确保数据的高可靠性。 高性能：HBase能够处理大规模数据，并且支持快速的随机读/写操作。 水平扩展：HBase可以方便地通过添加更多的节点来进行水平扩展，以满足不断增长的数据需求。 灵活的数据模型：HBase支持灵活的数据模型，可以存储半结构化和非结构化数据。 紧密集成Hadoop：HBase与Hadoop生态系统紧密集成，可以方便地与Hadoop的其他组件进行交互。 劣势：\n不适合低延迟读取：相对于传统的关系型数据库，HBase在低延迟读取方面可能表现不佳。 学习成本高：HBase需要用户具备一定的分布式系统和大数据处理的知识，学习成本相对较高。 维护复杂：由于HBase是一个分布式系统，因此对其进行维护和管理可能相对复杂。 总的来说，HBase适合存储大规模的半结构化或非结构化数据，并且在需要高可靠性和高扩展性的场景下表现优异，但在一些特定的使用场景下可能存在一些限制。\n应用场景 HBase适合用于以下应用场景：\n大数据存储与分析：HBase适合存储大规模的半结构化或非结构化数据，特别是在需要进行实时分析和查询大数据集时。\n时序数据存储：对于需要按时间顺序存储和查询数据的场景，比如物联网（IoT）数据、日志数据等，HBase能够提供高效的支持。\n在线系统的实时访问：HBase可以用于支持在线系统的实时访问，特别是需要快速随机读/写操作的场景。\n大规模的用户数据存储：对于需要存储和管理大规模用户数据的应用，比如社交网络、在线游戏等，HBase能够提供高可靠性和高性能的支持。\n日志分析和监控：HBase可以用于存储和分析大量的日志数据，以及用于监控系统状态和性能。\n总的来说，HBase适合于需要存储大规模数据，并且需要高可靠性、高性能和水平扩展能力的应用场景。\nhabse的行键（RowKey） 在HBase中，RowKey是表中每行数据的唯一标识符，它在表中具有重要的作用。RowKey的设计需要根据实际业务需求和数据访问模式进行合理规划。\n设计RowKey时需要考虑以下几点：\n唯一性：RowKey应该能够唯一标识一行数据，确保不会出现重复的RowKey。\n访问模式：根据数据的访问模式，设计RowKey可以帮助提高查询效率。如果经常需要按照某个顺序进行范围查询或者特定的过滤，可以将这些需求考虑在内。\n数据分布：设计RowKey时需要考虑数据的均匀分布，避免热点数据集中在某个RegionServer上，导致负载不均衡。\n长度和编码：RowKey的长度应该尽量控制在合理范围内，避免过长的RowKey导致存储和查询效率下降。另外，RowKey的编码方式也需要根据实际情况选择，比如可以使用字典序、时间戳等方式进行编码。\n举例来说，如果我们需要存储用户的订单信息，并且经常需要按照订单号进行查询，可以将订单号作为RowKey。另外，如果需要按照时间范围查询订单，可以将时间戳作为RowKey的一部分，以便支持按时间范围进行快速查询。\n总的来说，设计RowKey需要综合考虑唯一性、访问模式、数据分布和长度编码等因素，以便在保证唯一性的前提下，提高数据的访问效率和系统的性能。\nhbase的底层数据结构 HBase的底层数据结构主要由HFile和MemStore组成。\nHFile：HFile是HBase中的一种存储文件格式，用于持久化存储数据。HFile采用了块索引（Block Index）和块缓存（Block Cache）的设计，能够提供高效的随机读取和范围扫描能力。HFile中的数据按行键（RowKey）排序存储，每个列族（Column Family）对应一个HFile文件。\nMemStore：MemStore是HBase中的内存数据存储组件，用于暂时存储写入的数据。当数据写入HBase时，首先会被写入MemStore中，然后根据一定的条件（如大小、时间等）将数据刷写到HFile中。这种设计能够提高写入性能，并且减少了频繁的磁盘写入操作。\n除了HFile和MemStore之外，HBase还依赖于Apache Hadoop中的HDFS（Hadoop Distributed File System）来存储数据文件，并且利用ZooKeeper来进行分布式协调和管理。\n总的来说，HBase的底层数据结构通过HFile和MemStore实现了高效的数据存储和管理，能够支持大规模数据的存储和高性能的读写操作。\nhbase的使用 使用HBase命令行进行数据的存储和查询操作需要通过HBase Shell来实现。以下是一个较为复杂的例子，包括数据的存储和查询操作：\n创建表：首先，我们需要创建一个名为\u0026quot;employee\u0026quot;的表，包括\u0026quot;info\u0026quot;列族和\u0026quot;salary\u0026quot;列族。 create \u0026#39;employee\u0026#39;, \u0026#39;info\u0026#39;, \u0026#39;salary\u0026#39; 插入数据：接下来，我们向表中插入一条员工记录，包括姓名、部门、工资等信息。 put \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, \u0026#39;info:name\u0026#39;, \u0026#39;John Doe\u0026#39; put \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, \u0026#39;info:department\u0026#39;, \u0026#39;IT\u0026#39; put \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, \u0026#39;salary:base\u0026#39;, \u0026#39;60000\u0026#39; put \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, \u0026#39;salary:bonus\u0026#39;, \u0026#39;10000\u0026#39; 查询数据：然后，我们可以查询刚刚插入的员工记录，以及特定列族或列的数值。 get \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39; get \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, {COLUMN =\u0026gt; \u0026#39;info\u0026#39;} get \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, {COLUMN =\u0026gt; \u0026#39;salary\u0026#39;, VERSIONS =\u0026gt; 1} 扫描表：我们还可以对整个表进行扫描，获取所有的员工记录。 scan \u0026#39;employee\u0026#39; 这个例子涵盖了创建表、插入数据、查询数据和扫描表等操作，展示了如何使用HBase Shell进行较为复杂的数据存储和查询操作。\n","description":"Hbase优势和劣势 HBase是一个分布式、面向列的NoSQL数据库，它有一些优势和劣势。\n优势：\n高可靠性：HBase具有自动数据复制和故障转移功能，可以确保数据的高可靠性。 高性能：HBase能够处理大规模数据，并且支持快速的随机读/写操作。 水平扩展：HBase可以方便地通过添加更多的节点来进行水平扩展，以满足不断增长的数据需求。 灵活的数据模型：HBase支持灵活的数据模型，可以存储半结构化和非结构化数据。 紧密集成Hadoop：HBase与Hadoop生态系统紧密集成，可以方便地与Hadoop的其他组件进行交互。 劣势：\n不适合低延迟读取：相对于传统的关系型数据库，HBase在低延迟读取方面可能表现不佳。 学习成本高：HBase需要用户具备一定的分布式系统和大数据处理的知识，学习成本相对较高。 维护复杂：由于HBase是一个分布式系统，因此对其进行维护和管理可能相对复杂。 总的来说，HBase适合存储大规模的半结构化或非结构化数据，并且在需要高可靠性和高扩展性的场景下表现优异，但在一些特定的使用场景下可能存在一些限制。\n应用场景 HBase适合用于以下应用场景：\n大数据存储与分析：HBase适合存储大规模的半结构化或非结构化数据，特别是在需要进行实时分析和查询大数据集时。\n时序数据存储：对于需要按时间顺序存储和查询数据的场景，比如物联网（IoT）数据、日志数据等，HBase能够提供高效的支持。\n在线系统的实时访问：HBase可以用于支持在线系统的实时访问，特别是需要快速随机读/写操作的场景。\n大规模的用户数据存储：对于需要存储和管理大规模用户数据的应用，比如社交网络、在线游戏等，HBase能够提供高可靠性和高性能的支持。\n日志分析和监控：HBase可以用于存储和分析大量的日志数据，以及用于监控系统状态和性能。\n总的来说，HBase适合于需要存储大规模数据，并且需要高可靠性、高性能和水平扩展能力的应用场景。\nhabse的行键（RowKey） 在HBase中，RowKey是表中每行数据的唯一标识符，它在表中具有重要的作用。RowKey的设计需要根据实际业务需求和数据访问模式进行合理规划。\n设计RowKey时需要考虑以下几点：\n唯一性：RowKey应该能够唯一标识一行数据，确保不会出现重复的RowKey。\n访问模式：根据数据的访问模式，设计RowKey可以帮助提高查询效率。如果经常需要按照某个顺序进行范围查询或者特定的过滤，可以将这些需求考虑在内。\n数据分布：设计RowKey时需要考虑数据的均匀分布，避免热点数据集中在某个RegionServer上，导致负载不均衡。\n长度和编码：RowKey的长度应该尽量控制在合理范围内，避免过长的RowKey导致存储和查询效率下降。另外，RowKey的编码方式也需要根据实际情况选择，比如可以使用字典序、时间戳等方式进行编码。\n举例来说，如果我们需要存储用户的订单信息，并且经常需要按照订单号进行查询，可以将订单号作为RowKey。另外，如果需要按照时间范围查询订单，可以将时间戳作为RowKey的一部分，以便支持按时间范围进行快速查询。\n总的来说，设计RowKey需要综合考虑唯一性、访问模式、数据分布和长度编码等因素，以便在保证唯一性的前提下，提高数据的访问效率和系统的性能。\nhbase的底层数据结构 HBase的底层数据结构主要由HFile和MemStore组成。\nHFile：HFile是HBase中的一种存储文件格式，用于持久化存储数据。HFile采用了块索引（Block Index）和块缓存（Block Cache）的设计，能够提供高效的随机读取和范围扫描能力。HFile中的数据按行键（RowKey）排序存储，每个列族（Column Family）对应一个HFile文件。\nMemStore：MemStore是HBase中的内存数据存储组件，用于暂时存储写入的数据。当数据写入HBase时，首先会被写入MemStore中，然后根据一定的条件（如大小、时间等）将数据刷写到HFile中。这种设计能够提高写入性能，并且减少了频繁的磁盘写入操作。\n除了HFile和MemStore之外，HBase还依赖于Apache Hadoop中的HDFS（Hadoop Distributed File System）来存储数据文件，并且利用ZooKeeper来进行分布式协调和管理。\n总的来说，HBase的底层数据结构通过HFile和MemStore实现了高效的数据存储和管理，能够支持大规模数据的存储和高性能的读写操作。\nhbase的使用 使用HBase命令行进行数据的存储和查询操作需要通过HBase Shell来实现。以下是一个较为复杂的例子，包括数据的存储和查询操作：\n创建表：首先，我们需要创建一个名为\u0026quot;employee\u0026quot;的表，包括\u0026quot;info\u0026quot;列族和\u0026quot;salary\u0026quot;列族。 create \u0026#39;employee\u0026#39;, \u0026#39;info\u0026#39;, \u0026#39;salary\u0026#39; 插入数据：接下来，我们向表中插入一条员工记录，包括姓名、部门、工资等信息。 put \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, \u0026#39;info:name\u0026#39;, \u0026#39;John Doe\u0026#39; put \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, \u0026#39;info:department\u0026#39;, \u0026#39;IT\u0026#39; put \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, \u0026#39;salary:base\u0026#39;, \u0026#39;60000\u0026#39; put \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, \u0026#39;salary:bonus\u0026#39;, \u0026#39;10000\u0026#39; 查询数据：然后，我们可以查询刚刚插入的员工记录，以及特定列族或列的数值。 get \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39; get \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, {COLUMN =\u0026gt; \u0026#39;info\u0026#39;} get \u0026#39;employee\u0026#39;, \u0026#39;001\u0026#39;, {COLUMN =\u0026gt; \u0026#39;salary\u0026#39;, VERSIONS =\u0026gt; 1} 扫描表：我们还可以对整个表进行扫描，获取所有的员工记录。 scan \u0026#39;employee\u0026#39; 这个例子涵盖了创建表、插入数据、查询数据和扫描表等操作，展示了如何使用HBase Shell进行较为复杂的数据存储和查询操作。"},{"id":3,"href":"/hub/Data-Storage/NOSQL-DB/document-database/MongoDB/","title":"MongoDB","parent":"文档型数据库","content":"MongoDB作为一种NoSQL数据库系统，具有许多优势和劣势。下面是对MongoDB的优势和劣势进行分析：\n优势：\n灵活的数据模型：MongoDB采用文档存储模型，可以存储各种类型的数据，并支持嵌套文档和数组。这种灵活性使得MongoDB适用于需要频繁变化的数据结构的应用。\n高性能：MongoDB具有高性能的读写操作，支持水平扩展，可以通过横向扩展来提高系统的吞吐量和可扩展性。\n自动分片：MongoDB支持自动数据分片，可以将数据分布在多个节点上，实现数据的水平扩展，提高系统的负载能力。\n丰富的查询功能：MongoDB支持丰富的查询操作，包括文本搜索、地理空间查询等功能，可以满足不同类型的查询需求。\n高可用性：MongoDB支持副本集和自动故障转移，可以保证数据的高可用性和可靠性。\n劣势：\n事务支持不完善：相对于传统的关系型数据库，MongoDB在事务支持方面还不够成熟，不支持跨多个文档的事务操作。\n复杂的多表关联查询：对于复杂的多表关联查询，MongoDB的性能可能不如关系型数据库，因为MongoDB不支持传统的关系型数据库中的join操作。\n内存消耗较大：MongoDB在处理大规模数据时，可能需要较大的内存消耗，特别是在进行聚合操作或索引查询时。\n数据一致性：由于MongoDB采用最终一致性模型，可能会出现数据一致性的延迟，对一些需要强一致性的应用不太适合。\nMongoDB的查询方式 在MongoDB中，可以按照以下规则进行检索数据：\n基本查询：可以使用db.collection.find()方法进行基本查询，例如：\n检索所有文档：db.collection.find() 指定查询条件：db.collection.find({ key: value }) 范围查询：可以使用操作符进行范围查询，例如：\n大于：db.collection.find({ key: { $gt: value } }) 小于等于：db.collection.find({ key: { $lte: value } }) 逻辑查询：可以使用逻辑操作符进行逻辑查询，例如：\n与操作：db.collection.find({ $and: [{ key1: value1 }, { key2: value2 }] }) 或操作：db.collection.find({ $or: [{ key1: value1 }, { key2: value2 }] }) 文本搜索：可以使用全文搜索索引进行文本搜索，例如：\n创建文本索引：db.collection.createIndex({ key: \u0026quot;text\u0026quot; }) 文本搜索：db.collection.find({ $text: { $search: \u0026quot;keyword\u0026quot; } }) 聚合查询：可以使用聚合管道进行复杂的聚合查询，例如：\n分组统计：db.collection.aggregate([ { $group: { _id: \u0026quot;$key\u0026quot;, total: { $sum: 1 } } } ]) 聚合计算：db.collection.aggregate([ { $match: { key: value } }, { $group: { _id: \u0026quot;$key\u0026quot;, total: { $sum: \u0026quot;$value\u0026quot; } } } ]) 排序和限制：可以对查询结果进行排序和限制，例如：\n排序：db.collection.find().sort({ key: 1 })（升序）或db.collection.find().sort({ key: -1 })（降序） 限制数量：db.collection.find().limit(10) 通过这些规则，可以灵活地对MongoDB中的数据进行检索和查询，满足不同场景下的需求。\n案例 一个实际案例是电子商务网站的产品目录管理系统。在这个系统中，可以使用MongoDB存储产品信息，如产品名称、描述、价格、库存等。数据模型可以如下所示：\n{ \u0026#34;_id\u0026#34;: ObjectId(\u0026#34;609c7b6c39e1a7e2b6d3b4f2\u0026#34;), \u0026#34;product_name\u0026#34;: \u0026#34;iPhone 12\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A powerful phone with 5G capability\u0026#34;, \u0026#34;price\u0026#34;: 999.99, \u0026#34;stock\u0026#34;: 100, \u0026#34;category\u0026#34;: \u0026#34;Electronics\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;Black\u0026#34;, \u0026#34;storage\u0026#34;: \u0026#34;256GB\u0026#34;, \u0026#34;RAM\u0026#34;: \u0026#34;6GB\u0026#34; }, \u0026#34;reviews\u0026#34;: [ { \u0026#34;user\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;rating\u0026#34;: 5, \u0026#34;comment\u0026#34;: \u0026#34;Great phone!\u0026#34; }, { \u0026#34;user\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;rating\u0026#34;: 4, \u0026#34;comment\u0026#34;: \u0026#34;Good value for money\u0026#34; } ] } 查询 comment中包含\u0026quot;Good\u0026quot;的记录，可以使用如下查询语句： db.collection.find({ \u0026#34;reviews.comment\u0026#34;: { $regex: /Good/i } }) 或者使用MongoDB的全文检索加速查询操作\ndb.collection.createIndex({ \u0026#34;reviews.comment\u0026#34;: \u0026#34;text\u0026#34; }) db.collection.find({ $text: { $search: \u0026#34;Good\u0026#34; } }) ","description":"MongoDB作为一种NoSQL数据库系统，具有许多优势和劣势。下面是对MongoDB的优势和劣势进行分析：\n优势：\n灵活的数据模型：MongoDB采用文档存储模型，可以存储各种类型的数据，并支持嵌套文档和数组。这种灵活性使得MongoDB适用于需要频繁变化的数据结构的应用。\n高性能：MongoDB具有高性能的读写操作，支持水平扩展，可以通过横向扩展来提高系统的吞吐量和可扩展性。\n自动分片：MongoDB支持自动数据分片，可以将数据分布在多个节点上，实现数据的水平扩展，提高系统的负载能力。\n丰富的查询功能：MongoDB支持丰富的查询操作，包括文本搜索、地理空间查询等功能，可以满足不同类型的查询需求。\n高可用性：MongoDB支持副本集和自动故障转移，可以保证数据的高可用性和可靠性。\n劣势：\n事务支持不完善：相对于传统的关系型数据库，MongoDB在事务支持方面还不够成熟，不支持跨多个文档的事务操作。\n复杂的多表关联查询：对于复杂的多表关联查询，MongoDB的性能可能不如关系型数据库，因为MongoDB不支持传统的关系型数据库中的join操作。\n内存消耗较大：MongoDB在处理大规模数据时，可能需要较大的内存消耗，特别是在进行聚合操作或索引查询时。\n数据一致性：由于MongoDB采用最终一致性模型，可能会出现数据一致性的延迟，对一些需要强一致性的应用不太适合。\nMongoDB的查询方式 在MongoDB中，可以按照以下规则进行检索数据：\n基本查询：可以使用db.collection.find()方法进行基本查询，例如：\n检索所有文档：db.collection.find() 指定查询条件：db.collection.find({ key: value }) 范围查询：可以使用操作符进行范围查询，例如：\n大于：db.collection.find({ key: { $gt: value } }) 小于等于：db.collection.find({ key: { $lte: value } }) 逻辑查询：可以使用逻辑操作符进行逻辑查询，例如：\n与操作：db.collection.find({ $and: [{ key1: value1 }, { key2: value2 }] }) 或操作：db.collection.find({ $or: [{ key1: value1 }, { key2: value2 }] }) 文本搜索：可以使用全文搜索索引进行文本搜索，例如：\n创建文本索引：db.collection.createIndex({ key: \u0026quot;text\u0026quot; }) 文本搜索：db.collection.find({ $text: { $search: \u0026quot;keyword\u0026quot; } }) 聚合查询：可以使用聚合管道进行复杂的聚合查询，例如："},{"id":4,"href":"/hub/Data-Storage/NOSQL-DB/","title":"NOSQL数据库","parent":"数据存储","content":" 列存储数据库 hbase Couchbase Column-oriented Database 图数据库 Graph Database 文档型数据库 MongoDB elasticsearch Amazon DynamoDB 其他 ClickHouse Timeseries Database 键值对数据库 Redis ","description":" 列存储数据库 hbase Couchbase Column-oriented Database 图数据库 Graph Database 文档型数据库 MongoDB elasticsearch Amazon DynamoDB 其他 ClickHouse Timeseries Database 键值对数据库 Redis "},{"id":5,"href":"/hub/Data-Visualization/Power-BI/","title":"Power BI","parent":"数据可视化","content":"Microsoft\u0026rsquo;s Power BI is a unified, scalable platform for self-service and enterprise business intelligence (BI). Connect to and visualize any data, and seamlessly infuse the visuals into the apps you use every day.\nPower BI Official Documentation https://learn.microsoft.com/en-us/power-bi/\nPower BI Advantages #placeholder/description\nPower BI Disadvantages #placeholder/description\n","description":"Microsoft\u0026rsquo;s Power BI is a unified, scalable platform for self-service and enterprise business intelligence (BI). Connect to and visualize any data, and seamlessly infuse the visuals into the apps you use every day.\nPower BI Official Documentation https://learn.microsoft.com/en-us/power-bi/\nPower BI Advantages #placeholder/description\nPower BI Disadvantages #placeholder/description"},{"id":6,"href":"/hub/Fundamentals/languages/python/","title":"Python","parent":"编程语言","content":"Python在大数据领域有着广泛的应用，Python也有许多常用的框架和库，例如：\npandas：用于数据处理和分析的强大库，提供了数据结构和数据操作工具。 NumPy：用于数值计算的库，支持多维数组和矩阵运算。 Matplotlib：用于绘制数据可视化图表的库，支持各种类型的图表。 SciPy：基于NumPy的科学计算库，提供了更多的科学计算工具和算法。 scikit-learn：用于机器学习的库，包含了各种机器学习算法和工具。 Python适合于数据类的开发的原因包括：\n语法简洁清晰：Python的语法简洁易读，使得数据处理和分析的代码编写更加高效。 丰富的库和框架：Python拥有庞大的生态系统，提供了丰富的数据处理、机器学习和可视化工具。 强大的社区支持：Python拥有庞大的开发者社区，可以快速获取帮助和解决问题。 Python最常用的语言特性包括：\n动态类型：Python是一种动态类型语言，不需要显式声明变量的类型。 高级数据结构：Python提供了丰富的高级数据结构，如列表、字典和集合，方便进行数据处理和操作。 面向对象：Python是一种面向对象的语言，支持面向对象编程的特性，如封装、继承和多态。 Python相关工具 ","description":"Python在大数据领域有着广泛的应用，Python也有许多常用的框架和库，例如：\npandas：用于数据处理和分析的强大库，提供了数据结构和数据操作工具。 NumPy：用于数值计算的库，支持多维数组和矩阵运算。 Matplotlib：用于绘制数据可视化图表的库，支持各种类型的图表。 SciPy：基于NumPy的科学计算库，提供了更多的科学计算工具和算法。 scikit-learn：用于机器学习的库，包含了各种机器学习算法和工具。 Python适合于数据类的开发的原因包括：\n语法简洁清晰：Python的语法简洁易读，使得数据处理和分析的代码编写更加高效。 丰富的库和框架：Python拥有庞大的生态系统，提供了丰富的数据处理、机器学习和可视化工具。 强大的社区支持：Python拥有庞大的开发者社区，可以快速获取帮助和解决问题。 Python最常用的语言特性包括：\n动态类型：Python是一种动态类型语言，不需要显式声明变量的类型。 高级数据结构：Python提供了丰富的高级数据结构，如列表、字典和集合，方便进行数据处理和操作。 面向对象：Python是一种面向对象的语言，支持面向对象编程的特性，如封装、继承和多态。 Python相关工具 "},{"id":7,"href":"/hub/Data-Warehouse/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/","title":"什么是数据仓库","parent":"数据仓库","content":"数据仓库是用于报告和分析的数据的中央存储库。 数据通常从[[在线事务处理|事务系统]]、关系数据库或[[数据湖|其他来源]]进入数据仓库。 然后，业务分析师、数据工程师、数据科学家和决策者通过[[商业智能|商业智能]]工具、[[SQL]]客户端和其他分析应用程序访问数据。 由于数据仓库的主要用例围绕分析，因此它们通常使用[[在线分析处理|OLAP]]技术来提高性能。\n数据仓库的优势 将多个数据源的数据整合为一个“事实来源” 针对读取访问进行了优化，这使得生成报告比使用源事务系统进行报告更快 存储和分析大量历史数据 数据仓库的缺点 投入大量时间和资源来正确构建 不是为实时摄取数据而设计的（尽管它们通常可以近乎实时地处理） 何时使用数据仓库 数据仓库是为大型数据集的复杂查询而设计的。 如果您出于性能原因希望将历史数据与当前事务分开，则应该考虑使用数据仓库。\n流行的数据仓库 Amazon Redshift Azure Synapse Analytics Google BigQuery Snowflake 数据仓库基准 1 TB：[2020 - Redshift、Snowflake、Presto 和 BigQuery](https:// Fivetran.com/blog/warehouse-benchmark) 30 TB：2019 年 - Redshift、Azure SQL 数据仓库、BigQuery、Snowflake ","description":"数据仓库是用于报告和分析的数据的中央存储库。 数据通常从[[在线事务处理|事务系统]]、关系数据库或[[数据湖|其他来源]]进入数据仓库。 然后，业务分析师、数据工程师、数据科学家和决策者通过[[商业智能|商业智能]]工具、[[SQL]]客户端和其他分析应用程序访问数据。 由于数据仓库的主要用例围绕分析，因此它们通常使用[[在线分析处理|OLAP]]技术来提高性能。\n数据仓库的优势 将多个数据源的数据整合为一个“事实来源” 针对读取访问进行了优化，这使得生成报告比使用源事务系统进行报告更快 存储和分析大量历史数据 数据仓库的缺点 投入大量时间和资源来正确构建 不是为实时摄取数据而设计的（尽管它们通常可以近乎实时地处理） 何时使用数据仓库 数据仓库是为大型数据集的复杂查询而设计的。 如果您出于性能原因希望将历史数据与当前事务分开，则应该考虑使用数据仓库。\n流行的数据仓库 Amazon Redshift Azure Synapse Analytics Google BigQuery Snowflake 数据仓库基准 1 TB：[2020 - Redshift、Snowflake、Presto 和 BigQuery](https:// Fivetran.com/blog/warehouse-benchmark) 30 TB：2019 年 - Redshift、Azure SQL 数据仓库、BigQuery、Snowflake "},{"id":8,"href":"/hub/Data-Storage/","title":"数据存储","parent":"数据工程知识库","content":" SQL数据库 事务性数据库 Amazon RDS MySQL PostgreSQL Microsoft SQL Server 分析型数据库 Azure Synapse Analytics Amazon Redshift Google BigQuery 文件格式 Avro Apache ORC Apache Parquet JSON yaml格式 CSV Delta Lake Protocol Buffers NOSQL数据库 列存储数据库 hbase Couchbase Column-oriented Database 图数据库 Graph Database 文档型数据库 MongoDB elasticsearch Amazon DynamoDB 其他 ClickHouse Timeseries Database 键值对数据库 Redis 分布式文件存储 分布式文件存储 对象存储 Amazon S3 Glacier Amazon S3 ","description":" SQL数据库 事务性数据库 Amazon RDS MySQL PostgreSQL Microsoft SQL Server 分析型数据库 Azure Synapse Analytics Amazon Redshift Google BigQuery 文件格式 Avro Apache ORC Apache Parquet JSON yaml格式 CSV Delta Lake Protocol Buffers NOSQL数据库 列存储数据库 hbase Couchbase Column-oriented Database 图数据库 Graph Database 文档型数据库 MongoDB elasticsearch Amazon DynamoDB 其他 ClickHouse Timeseries Database 键值对数据库 Redis 分布式文件存储 分布式文件存储 对象存储 Amazon S3 Glacier Amazon S3 "},{"id":9,"href":"/hub/Data-Mining/Logistic_Regression/","title":"逻辑回归案例","parent":"数据挖掘","content":"我们可以使用一个开源的数据集来实现垃圾评论过滤。以下是一个示例，我们将使用 YouTube 评论数据集 来训练一个逻辑回归模型来识别垃圾评论。\n数据集：\n我们将使用 YouTube 评论数据集，其中包含了一些标记为垃圾（1）或非垃圾（0）的评论。这个数据集相对较小，大约有2000个样本¹²。 你可以从这个网站下载数据集：YouTube Spam Collection。 步骤：\n读取数据集并准备特征和目标变量。 使用 TF-IDF 向量化文本特征。 将数据分为训练集和测试集。 初始化 Logistic Regression 模型并在训练数据上进行训练。 在测试数据上进行预测并计算准确率。 示例代码：\nimport pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, classification_report # 读取垃圾评论数据集（示例数据，你可以替换为实际数据集） data = pd.read_csv(\u0026#34;YouTube_Spam_Collection.csv\u0026#34;) # 分割特征和目标变量 X = data[\u0026#34;comment_text\u0026#34;] y = data[\u0026#34;is_spam\u0026#34;] # 使用 TF-IDF 向量化文本特征 vectorizer = TfidfVectorizer(max_features=1000) X_tfidf = vectorizer.fit_transform(X) # 将数据分为训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42) # 初始化 Logistic Regression 模型 model = LogisticRegression() # 在训练数据上训练模型 model.fit(X_train, y_train) # 在测试数据上进行预测 y_pred = model.predict(X_test) # 计算准确率和其他评估指标 accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) print(f\u0026#34;准确率：{accuracy:.2f}\u0026#34;) print(\u0026#34;分类报告：\u0026#34;) print(report) 运行结果\n分类报告： precision recall f1-score support 0 0.96 0.96 0.96 27 1 0.98 0.98 0.98 43 accuracy 0.97 70 macro avg 0.97 0.97 0.97 70 weighted avg 0.97 0.97 0.97 70 结果解释\n这是一个**二分类模型**的分类报告，用于评估模型的性能。让我们来解释一下报告中的各项指标： - **Precision（精确率）**：表示模型预测为正类的样本中，实际为正类的比例。对于类别 0，精确率为 0.96，对于类别 1，精确率为 0.98。这意味着模型在预测为正类时，有很高的准确性。 - **Recall（召回率）**：表示实际为正类的样本中，被模型正确预测为正类的比例。对于类别 0，召回率为 0.96，对于类别 1，召回率为 0.98。这意味着模型能够有效地捕捉到正类样本。 - **F1-score（F1 分数）**：综合考虑了精确率和召回率，是一个综合性能指标。对于类别 0，F1 分数为 0.96，对于类别 1，F1 分数为 0.98。这个指标越接近 1，说明模型的性能越好。 - **Accuracy（准确率）**：表示模型预测正确的样本占总样本数的比例。整体准确率为 0.97，说明模型在整体上表现良好。 - **Macro avg 和 weighted avg**：分别是宏平均和加权平均。宏平均计算了每个类别的指标的平均值，而加权平均考虑了每个类别的样本数量。在这里，宏平均和加权平均的值都接近 0.97，说明模型在两个类别上都有很好的表现。 总之，这个模型在分类任务上表现出色，具有很高的准确性、召回率和 F1 分数。如果你有其他问题或需要更详细的解释，请随时告知！🌟 ","description":"我们可以使用一个开源的数据集来实现垃圾评论过滤。以下是一个示例，我们将使用 YouTube 评论数据集 来训练一个逻辑回归模型来识别垃圾评论。\n数据集：\n我们将使用 YouTube 评论数据集，其中包含了一些标记为垃圾（1）或非垃圾（0）的评论。这个数据集相对较小，大约有2000个样本¹²。 你可以从这个网站下载数据集：YouTube Spam Collection。 步骤：\n读取数据集并准备特征和目标变量。 使用 TF-IDF 向量化文本特征。 将数据分为训练集和测试集。 初始化 Logistic Regression 模型并在训练数据上进行训练。 在测试数据上进行预测并计算准确率。 示例代码：\nimport pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, classification_report # 读取垃圾评论数据集（示例数据，你可以替换为实际数据集） data = pd.read_csv(\u0026#34;YouTube_Spam_Collection.csv\u0026#34;) # 分割特征和目标变量 X = data[\u0026#34;comment_text\u0026#34;] y = data[\u0026#34;is_spam\u0026#34;] # 使用 TF-IDF 向量化文本特征 vectorizer = TfidfVectorizer(max_features=1000) X_tfidf = vectorizer.fit_transform(X) # 将数据分为训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0."},{"id":10,"href":"/hub/Data-Storage/DFS/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/","title":"分布式文件存储","parent":"分布式文件存储","content":"在HDFS（Hadoop Distributed File System）中，分布式文件存储的架构和存储特点如下：\n架构：\nNameNode：负责存储文件系统的元数据（文件名、目录结构、文件权限等），以及文件与数据块的映射关系。 DataNode：负责实际存储数据块，并定期向NameNode汇报自身存储的数据块信息。 Client：与NameNode和DataNode通信，负责文件的读写操作。 存储特点：\n数据冗余：HDFS通过数据块的冗余存储（通常默认为3个副本）来提高数据的容错性和可靠性。 水平扩展：HDFS可以在集群中添加新的DataNode来扩展存储容量，实现水平扩展。 高可靠性：通过数据冗余和检测机制，HDFS能够保证数据的高可靠性，即使某个DataNode发生故障也不会丢失数据。 高吞吐量：HDFS适合存储大文件和进行大规模数据处理，能够提供高吞吐量的数据读写能力。 流式数据访问：HDFS的设计适合大数据处理场景，支持高效的流式数据访问，适用于MapReduce等计算框架的需求。 总之，HDFS作为一种典型的分布式文件存储系统，具有高可靠性、高扩展性和高吞吐量等特点，适合用于大规模数据存储和处理的场景。\n","description":"在HDFS（Hadoop Distributed File System）中，分布式文件存储的架构和存储特点如下：\n架构：\nNameNode：负责存储文件系统的元数据（文件名、目录结构、文件权限等），以及文件与数据块的映射关系。 DataNode：负责实际存储数据块，并定期向NameNode汇报自身存储的数据块信息。 Client：与NameNode和DataNode通信，负责文件的读写操作。 存储特点：\n数据冗余：HDFS通过数据块的冗余存储（通常默认为3个副本）来提高数据的容错性和可靠性。 水平扩展：HDFS可以在集群中添加新的DataNode来扩展存储容量，实现水平扩展。 高可靠性：通过数据冗余和检测机制，HDFS能够保证数据的高可靠性，即使某个DataNode发生故障也不会丢失数据。 高吞吐量：HDFS适合存储大文件和进行大规模数据处理，能够提供高吞吐量的数据读写能力。 流式数据访问：HDFS的设计适合大数据处理场景，支持高效的流式数据访问，适用于MapReduce等计算框架的需求。 总之，HDFS作为一种典型的分布式文件存储系统，具有高可靠性、高扩展性和高吞吐量等特点，适合用于大规模数据存储和处理的场景。"},{"id":11,"href":"/hub/Data-Storage/format/ORC/","title":"Apache ORC","parent":"文件格式","content":"ORC（Optimized Row Columnar）是一种用于存储大规模数据的列式存储文件格式，常用于Apache Hive等数据处理工具中。下面是ORC格式的优势和劣势：\n优势：\n高压缩比：ORC文件格式支持多种压缩算法，能够有效地减小存储空间，节省存储成本。 高性能：由于采用列式存储，ORC文件格式在数据扫描和查询时能够实现更高的性能，特别是在大规模数据处理场景下。 支持复杂数据类型：ORC格式支持复杂的数据类型，如结构体、数组、映射等，能够更灵活地存储和查询数据。 列式存储：列式存储可以只读取需要的列，减少IO开销，提高查询效率。 劣势：\n不适合小规模数据：由于ORC格式的优势主要体现在大规模数据的存储和查询上，对于小规模数据可能会存在一定的性能损失。 不支持追加操作：和Avro一样，ORC文件格式一般用于静态数据存储，不支持在文件末尾追加数据，需要重新写入整个文件来更新数据。 不易于人类阅读：由于ORC文件格式是二进制存储的，不适合人类直接阅读，需要专门的工具来解析和查看文件内容。 需要额外的解析工具：为了读取和处理ORC文件，需要使用特定的工具或库，可能需要一定的学习成本。 ","description":"ORC（Optimized Row Columnar）是一种用于存储大规模数据的列式存储文件格式，常用于Apache Hive等数据处理工具中。下面是ORC格式的优势和劣势：\n优势：\n高压缩比：ORC文件格式支持多种压缩算法，能够有效地减小存储空间，节省存储成本。 高性能：由于采用列式存储，ORC文件格式在数据扫描和查询时能够实现更高的性能，特别是在大规模数据处理场景下。 支持复杂数据类型：ORC格式支持复杂的数据类型，如结构体、数组、映射等，能够更灵活地存储和查询数据。 列式存储：列式存储可以只读取需要的列，减少IO开销，提高查询效率。 劣势：\n不适合小规模数据：由于ORC格式的优势主要体现在大规模数据的存储和查询上，对于小规模数据可能会存在一定的性能损失。 不支持追加操作：和Avro一样，ORC文件格式一般用于静态数据存储，不支持在文件末尾追加数据，需要重新写入整个文件来更新数据。 不易于人类阅读：由于ORC文件格式是二进制存储的，不适合人类直接阅读，需要专门的工具来解析和查看文件内容。 需要额外的解析工具：为了读取和处理ORC文件，需要使用特定的工具或库，可能需要一定的学习成本。 "},{"id":12,"href":"/hub/Data-Storage/NOSQL-DB/document-database/elasticsearch/","title":"elasticsearch","parent":"文档型数据库","content":" ElasticSearch Elasticsearch 存储数据的格式是以 JSON 格式为基础的结构。每个文档（document）都是一个 JSON 对象，包含了字段（field）和对应的值。在 Elasticsearch 中，多个文档组成一个索引（index），而每个索引可以包含多种类型的文档。\n在 Elasticsearch 中，数据的存储格式如下所示：\n索引（Index）：类似于数据库中的表，用于存储一组相关的文档。 类型（Type）：在 Elasticsearch 6.x 及以上版本中，一个索引只能包含一个类型。类型用于对文档进行分类，但在将来的版本中可能会被废弃。 文档（Document）：每个文档都是一个 JSON 对象，包含了字段和对应的值。 字段（Field）：文档中的属性，类似于数据库表中的列。 值（Value）：字段对应的具体数值或文本。 总的来说，Elasticsearch 中的数据结构是基于 JSON 格式的文档存储，每个文档可以包含不同的字段和值，而这些文档组成了索引，用于快速检索和分析数据。\n案例 利用LogStash 将Mysql数据批量导入到ES 要将 MySQL 中的表 article 加入到 Elasticsearch 中，并对其中的 content、comment 和 date 进行全文索引，一种常见的方法是使用 Logstash。Logstash 是 Elastic Stack 中的一个组件，用于数据收集、转换和传输。通过 Logstash，你可以轻松地将 MySQL 中的数据导入到 Elasticsearch 中，并进行必要的数据转换和索引设置。\n以下是一个简单的示例配置文件，用于将 MySQL 表 article 中的数据导入到 Elasticsearch：\ninput { jdbc { jdbc_driver_library =\u0026gt; \u0026#34;path/to/mysql-connector-java.jar\u0026#34; jdbc_driver_class =\u0026gt; \u0026#34;com.mysql.cj.jdbc.Driver\u0026#34; jdbc_connection_string =\u0026gt; \u0026#34;jdbc:mysql://localhost:3306/your_database\u0026#34; jdbc_user =\u0026gt; \u0026#34;your_username\u0026#34; jdbc_password =\u0026gt; \u0026#34;your_password\u0026#34; statement =\u0026gt; \u0026#34;SELECT id, content, comment, date FROM article\u0026#34; } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;localhost:9200\u0026#34;] index =\u0026gt; \u0026#34;articles\u0026#34; document_id =\u0026gt; \u0026#34;%{id}\u0026#34; } } 在这个配置文件中：\ninput 部分使用 JDBC 插件连接到 MySQL 数据库，并执行 SQL 查询来获取 article 表中的数据。 output 部分将获取的数据发送到 Elasticsearch 中的 index \u0026ldquo;articles\u0026rdquo; 中，并指定了文档的 id 为数据库表中的 id 字段。 要运行这个配置文件，你需要安装和配置 Logstash，并确保 MySQL 数据库和 Elasticsearch 服务正常运行。执行以下命令来启动 Logstash 并运行配置文件：\nbin/logstash -f your_config_file.conf 通过这种方式，你可以将 MySQL 中的数据导入到 Elasticsearch 中，并对 content、comment 和 date 字段进行全文索引。Logstash 提供了丰富的插件和配置选项，可以帮助简化数据导入和转换的工作。\nLogstash支持Mysql数据导到ES 的增量更新 Logstash 提供了一个名为 JDBC Input 插件，可以实现监控 MySQL 数据库中数据变化并将新数据实时索引到 Elasticsearch 中的功能。这个功能通常称为 Incremental Data Import（增量数据导入）。\n通过配置 Logstash 的 JDBC Input 插件，并使用 schedule 参数来定时执行 SQL 查询，Logstash 可以定期轮询 MySQL 数据库，检测新的数据变化，并将新数据索引到 Elasticsearch 中。这样就实现了自动感知 MySQL 数据变化并实时更新 Elasticsearch 索引的功能。\n以下是一个简单的示例配置文件，演示如何使用 Logstash 实现增量数据导入：\ninput { jdbc { jdbc_driver_library =\u0026gt; \u0026#34;path/to/mysql-connector-java.jar\u0026#34; jdbc_driver_class =\u0026gt; \u0026#34;com.mysql.cj.jdbc.Driver\u0026#34; jdbc_connection_string =\u0026gt; \u0026#34;jdbc:mysql://localhost:3306/your_database\u0026#34; jdbc_user =\u0026gt; \u0026#34;your_username\u0026#34; jdbc_password =\u0026gt; \u0026#34;your_password\u0026#34; statement =\u0026gt; \u0026#34;SELECT id, content, comment, date FROM article WHERE date \u0026gt; :sql_last_value\u0026#34; use_column_value =\u0026gt; true tracking_column =\u0026gt; \u0026#34;date\u0026#34; schedule =\u0026gt; \u0026#34;* * * * *\u0026#34; } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;localhost:9200\u0026#34;] index =\u0026gt; \u0026#34;articles\u0026#34; document_id =\u0026gt; \u0026#34;%{id}\u0026#34; } } 在这个配置文件中：\nstatement 参数中的 :sql_last_value 变量表示上一次查询的最后一个值，用于实现增量查询。 tracking_column 参数指定了用于跟踪数据变化的列，这里使用了 \u0026ldquo;date\u0026rdquo; 字段。 schedule 参数用于设置定时执行查询的时间间隔，这里表示每分钟执行一次查询。 通过这样的配置，Logstash 将定期检测 MySQL 数据库中 \u0026ldquo;date\u0026rdquo; 字段的变化，将新数据索引到 Elasticsearch 中，实现了自动感知数据变化并实时更新索引的功能。。\n官网 https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html\n","description":"ElasticSearch Elasticsearch 存储数据的格式是以 JSON 格式为基础的结构。每个文档（document）都是一个 JSON 对象，包含了字段（field）和对应的值。在 Elasticsearch 中，多个文档组成一个索引（index），而每个索引可以包含多种类型的文档。\n在 Elasticsearch 中，数据的存储格式如下所示：\n索引（Index）：类似于数据库中的表，用于存储一组相关的文档。 类型（Type）：在 Elasticsearch 6.x 及以上版本中，一个索引只能包含一个类型。类型用于对文档进行分类，但在将来的版本中可能会被废弃。 文档（Document）：每个文档都是一个 JSON 对象，包含了字段和对应的值。 字段（Field）：文档中的属性，类似于数据库表中的列。 值（Value）：字段对应的具体数值或文本。 总的来说，Elasticsearch 中的数据结构是基于 JSON 格式的文档存储，每个文档可以包含不同的字段和值，而这些文档组成了索引，用于快速检索和分析数据。\n案例 利用LogStash 将Mysql数据批量导入到ES 要将 MySQL 中的表 article 加入到 Elasticsearch 中，并对其中的 content、comment 和 date 进行全文索引，一种常见的方法是使用 Logstash。Logstash 是 Elastic Stack 中的一个组件，用于数据收集、转换和传输。通过 Logstash，你可以轻松地将 MySQL 中的数据导入到 Elasticsearch 中，并进行必要的数据转换和索引设置。\n以下是一个简单的示例配置文件，用于将 MySQL 表 article 中的数据导入到 Elasticsearch：\ninput { jdbc { jdbc_driver_library =\u0026gt; \u0026#34;path/to/mysql-connector-java.jar\u0026#34; jdbc_driver_class =\u0026gt; \u0026#34;com.mysql.cj.jdbc.Driver\u0026#34; jdbc_connection_string =\u0026gt; \u0026#34;jdbc:mysql://localhost:3306/your_database\u0026#34; jdbc_user =\u0026gt; \u0026#34;your_username\u0026#34; jdbc_password =\u0026gt; \u0026#34;your_password\u0026#34; statement =\u0026gt; \u0026#34;SELECT id, content, comment, date FROM article\u0026#34; } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;localhost:9200\u0026#34;] index =\u0026gt; \u0026#34;articles\u0026#34; document_id =\u0026gt; \u0026#34;%{id}\u0026#34; } } 在这个配置文件中："},{"id":13,"href":"/hub/Fundamentals/languages/java/","title":"Java","parent":"编程语言","content":"Java语言在大数据领域有着广泛的应用，由Java语言开发的常用框架包括：\nApache Hadoop：用于分布式存储和处理大规模数据集。 Apache Spark：提供快速、通用的大规模数据处理引擎。 Apache Flink：用于流式数据处理和批处理的分布式数据处理框架。 Apache Kafka：用于构建实时数据流平台的分布式流处理平台。 Apache Storm：用于实时流数据处理的分布式计算系统。 Java语言适合于大数据领域的开发的原因包括：\n跨平台性：Java程序可以在各种操作系统上运行，适应大数据处理的多样化环境。 成熟的生态系统：Java拥有丰富的第三方库和框架，支持大数据处理所需的各种功能。 强大的并发性：Java具有良好的并发处理机制，适合处理大规模数据并发任务。 Java语言最常用的高级特性包括：\n泛型：通过泛型可以实现类型安全的数据结构和算法。 多线程：Java提供多线程支持，可以更高效地处理大规模数据并发任务。 反射：反射机制允许在运行时检查类、方法和属性，为大数据处理提供了灵活性和扩展性。 Java相关工具 ","description":"Java语言在大数据领域有着广泛的应用，由Java语言开发的常用框架包括：\nApache Hadoop：用于分布式存储和处理大规模数据集。 Apache Spark：提供快速、通用的大规模数据处理引擎。 Apache Flink：用于流式数据处理和批处理的分布式数据处理框架。 Apache Kafka：用于构建实时数据流平台的分布式流处理平台。 Apache Storm：用于实时流数据处理的分布式计算系统。 Java语言适合于大数据领域的开发的原因包括：\n跨平台性：Java程序可以在各种操作系统上运行，适应大数据处理的多样化环境。 成熟的生态系统：Java拥有丰富的第三方库和框架，支持大数据处理所需的各种功能。 强大的并发性：Java具有良好的并发处理机制，适合处理大规模数据并发任务。 Java语言最常用的高级特性包括：\n泛型：通过泛型可以实现类型安全的数据结构和算法。 多线程：Java提供多线程支持，可以更高效地处理大规模数据并发任务。 反射：反射机制允许在运行时检查类、方法和属性，为大数据处理提供了灵活性和扩展性。 Java相关工具 "},{"id":14,"href":"/hub/Fundamentals/languages/python/Python%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/","title":"Python相关工具","parent":"Python","content":"","description":""},{"id":15,"href":"/hub/Data-Visualization/Qlikview/","title":"Qlikview","parent":"数据可视化","content":"","description":""},{"id":16,"href":"/hub/Data-Warehouse/%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/","title":"数据模型","parent":"数据仓库","content":"数据建模是规划信息系统以及多个部分如何连接的过程。 数据模型通常用关系数据库的实体关系图来说明\n数据建模的好处 数据建模使开发人员和其他利益相关者可以更轻松地查看和理解数据库或数据仓库中数据之间的关系。 一个好的数据模型还可以带来以下好处：\n减少软件和数据库开发中的错误。 提高整个企业文档和系统设计的一致性。 提高应用程序和数据库性能。 简化整个组织的数据映射。 改善开发人员和商业智能团队之间的沟通。 简化并加速概念、逻辑和物理层面的数据库设计过程。 数据模型的类型 概念模型 提供系统将包含什么、如何组织以及涉及哪些业务规则的总体视图。 概念模型通常是作为收集初始项目需求过程的一部分创建的。 通常，它们包括实体类（定义对业务在数据模型中表示的重要事物的类型）、它们的特征和约束、它们之间的关系以及相关的安全性和数据完整性要求\n逻辑模型 它们不那么抽象，并提供了有关所考虑领域中的概念和关系的更多细节。 遵循几种形式数据建模符号系统之一。 这些指示数据属性，例如数据类型及其相应的长度，并显示实体之间的关系。 逻辑数据模型不指定任何技术系统要求。 在敏捷或 DevOps 实践中，这个阶段经常被省略。 逻辑数据模型在高度程序化的实施环境中非常有用，或者对于本质上面向数据的项目（例如数据仓库设计或报告系统开发）非常有用。\n物理模型 它们提供了如何在数据库中物理存储数据的模式。 因此，它们是最不抽象的。 他们提供了可以作为关系数据库实现的最终设计，包括说明实体之间关系的关联表以及用于维护这些关系的主键和外键。 物理数据模型可以包括数据库管理系统（DBMS）特定的属性，包括性能调整。\n","description":"数据建模是规划信息系统以及多个部分如何连接的过程。 数据模型通常用关系数据库的实体关系图来说明\n数据建模的好处 数据建模使开发人员和其他利益相关者可以更轻松地查看和理解数据库或数据仓库中数据之间的关系。 一个好的数据模型还可以带来以下好处：\n减少软件和数据库开发中的错误。 提高整个企业文档和系统设计的一致性。 提高应用程序和数据库性能。 简化整个组织的数据映射。 改善开发人员和商业智能团队之间的沟通。 简化并加速概念、逻辑和物理层面的数据库设计过程。 数据模型的类型 概念模型 提供系统将包含什么、如何组织以及涉及哪些业务规则的总体视图。 概念模型通常是作为收集初始项目需求过程的一部分创建的。 通常，它们包括实体类（定义对业务在数据模型中表示的重要事物的类型）、它们的特征和约束、它们之间的关系以及相关的安全性和数据完整性要求\n逻辑模型 它们不那么抽象，并提供了有关所考虑领域中的概念和关系的更多细节。 遵循几种形式数据建模符号系统之一。 这些指示数据属性，例如数据类型及其相应的长度，并显示实体之间的关系。 逻辑数据模型不指定任何技术系统要求。 在敏捷或 DevOps 实践中，这个阶段经常被省略。 逻辑数据模型在高度程序化的实施环境中非常有用，或者对于本质上面向数据的项目（例如数据仓库设计或报告系统开发）非常有用。\n物理模型 它们提供了如何在数据库中物理存储数据的模式。 因此，它们是最不抽象的。 他们提供了可以作为关系数据库实现的最终设计，包括说明实体之间关系的关联表以及用于维护这些关系的主键和外键。 物理数据模型可以包括数据库管理系统（DBMS）特定的属性，包括性能调整。"},{"id":17,"href":"/hub/Data-Storage/NOSQL-DB/document-database/Amazon-DynamoDB/","title":"Amazon DynamoDB","parent":"文档型数据库","content":"","description":""},{"id":18,"href":"/hub/Data-Storage/format/Parquet/","title":"Apache Parquet","parent":"文件格式","content":"Parquet 文件格式是一种列式存储格式，它将数据按列存储在文件中，而不是按行存储。这种存储方式有助于提高查询和分析性能。\n优势和劣势： 优势：\n高效的压缩：parquet 使用高效的压缩算法，可以显著减小存储空间。 列式存储：parquet 文件以列的方式存储数据，这样可以更快地进行列级别的操作和查询。 高效的扫描性能：parquet 文件格式支持更快的扫描性能，特别是在大数据量下。 处理复杂数据类型：parquet 文件格式支持复杂的数据类型，如嵌套数据结构和数组。 跨平台兼容性：parquet 文件格式可以被多种不同的处理框架支持，如Hadoop, Spark, Hive等。 劣势：\n不适合小数据量：对于小数据量的情况，parquet 文件格式可能会有一些额外的开销，不如其他格式高效。 不适合频繁更新：parquet 文件格式不适合频繁更新的场景，因为它是一种列式存储格式，更新操作可能会比较慢。 总的来说，parquet 文件格式适合大规模的数据存储和分析场景，特别是对于需要高效压缩和快速扫描的情况。\nParquet 存储数据的结构： 文件结构：Parquet 文件由多个数据块（Row Groups）组成，每个数据块包含一组行数据。\n列式存储：每个数据块中的数据按列存储，即所有行的某一列值连续存储在一起。这种存储方式有助于减少 I/O 操作，提高查询效率。\n数据页：每个列值被划分为多个数据页（Data Pages），数据页存储了列值的实际数据。数据页可以使用不同的压缩算法进行压缩，以减小存储空间。\n元数据：Parquet 文件中还包含元数据（Metadata），用于描述文件的结构和存储信息，包括列的数据类型、编码方式、压缩方式等。\n统计信息：Parquet 文件中还可以包含统计信息（Statistics），用于帮助查询优化器进行查询计划的生成，例如最小值、最大值、空值个数等。\n总的来说，Parquet 文件格式通过列式存储、数据页和元数据的结构，实现了高效的数据存储和查询功能，适合大规模数据分析和处理场景。\n以下是一个使用Python实现写入和读取Parquet文件的示例：\nimport pandas as pd import pyarrow as pa import pyarrow.parquet as pq # 创建示例数据 data = {\u0026#39;name\u0026#39;: [\u0026#39;Alice\u0026#39;, \u0026#39;Bob\u0026#39;, \u0026#39;Charlie\u0026#39;], \u0026#39;age\u0026#39;: [30, 25, 35]} df = pd.DataFrame(data) # 将Pandas DataFrame 转换为 PyArrow Table table = pa.Table.from_pandas(df) # 写入Parquet文件 pq.write_table(table, \u0026#39;users.parquet\u0026#39;) # 读取Parquet文件 table_read = pq.read_table(\u0026#39;users.parquet\u0026#39;) df_read = table_read.to_pandas() print(df_read) 在这个示例中，首先创建了一个简单的Pandas DataFrame，并将其转换为PyArrow Table。然后使用pq.write_table将Table写入到Parquet文件users.parquet中。接着使用pq.read_table读取Parquet文件，并将其转换为Pandas DataFrame进行打印。你可以根据实际需求修改数据内容和文件名。\n","description":"Parquet 文件格式是一种列式存储格式，它将数据按列存储在文件中，而不是按行存储。这种存储方式有助于提高查询和分析性能。\n优势和劣势： 优势：\n高效的压缩：parquet 使用高效的压缩算法，可以显著减小存储空间。 列式存储：parquet 文件以列的方式存储数据，这样可以更快地进行列级别的操作和查询。 高效的扫描性能：parquet 文件格式支持更快的扫描性能，特别是在大数据量下。 处理复杂数据类型：parquet 文件格式支持复杂的数据类型，如嵌套数据结构和数组。 跨平台兼容性：parquet 文件格式可以被多种不同的处理框架支持，如Hadoop, Spark, Hive等。 劣势：\n不适合小数据量：对于小数据量的情况，parquet 文件格式可能会有一些额外的开销，不如其他格式高效。 不适合频繁更新：parquet 文件格式不适合频繁更新的场景，因为它是一种列式存储格式，更新操作可能会比较慢。 总的来说，parquet 文件格式适合大规模的数据存储和分析场景，特别是对于需要高效压缩和快速扫描的情况。\nParquet 存储数据的结构： 文件结构：Parquet 文件由多个数据块（Row Groups）组成，每个数据块包含一组行数据。\n列式存储：每个数据块中的数据按列存储，即所有行的某一列值连续存储在一起。这种存储方式有助于减少 I/O 操作，提高查询效率。\n数据页：每个列值被划分为多个数据页（Data Pages），数据页存储了列值的实际数据。数据页可以使用不同的压缩算法进行压缩，以减小存储空间。\n元数据：Parquet 文件中还包含元数据（Metadata），用于描述文件的结构和存储信息，包括列的数据类型、编码方式、压缩方式等。\n统计信息：Parquet 文件中还可以包含统计信息（Statistics），用于帮助查询优化器进行查询计划的生成，例如最小值、最大值、空值个数等。\n总的来说，Parquet 文件格式通过列式存储、数据页和元数据的结构，实现了高效的数据存储和查询功能，适合大规模数据分析和处理场景。\n以下是一个使用Python实现写入和读取Parquet文件的示例：\nimport pandas as pd import pyarrow as pa import pyarrow.parquet as pq # 创建示例数据 data = {\u0026#39;name\u0026#39;: [\u0026#39;Alice\u0026#39;, \u0026#39;Bob\u0026#39;, \u0026#39;Charlie\u0026#39;], \u0026#39;age\u0026#39;: [30, 25, 35]} df = pd.DataFrame(data) # 将Pandas DataFrame 转换为 PyArrow Table table = pa."},{"id":19,"href":"/hub/Data-Governance/Choosing-your-optimal-messaging-service/","title":"Choosing your optimal messaging service","parent":"数据治理","content":" Overview A short guide on choosing which messaging service(s) to use.\nAWS %%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph TD A((Start)) --\u0026gt; B{Fan-out} B --\u0026gt;|Yes| C{Rate limit} C --\u0026gt;|Yes| D[SNS \u0026#43; SQS] C --\u0026gt;|No| E[SNS] B --\u0026gt;|No| F{Rate limit} F --\u0026gt;|Yes| G[SQS] F --\u0026gt;|No| H[Lambda Direct Invoke] class B internal-link; Source: AWS re:Invent 2020: Scalable serverless event-driven architectures with SNS, SQS \u0026amp; Lambda\nAzure #placeholder/description\nGCP #placeholder/description\n","description":"Overview A short guide on choosing which messaging service(s) to use.\nAWS %%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph TD A((Start)) --\u0026gt; B{Fan-out} B --\u0026gt;|Yes| C{Rate limit} C --\u0026gt;|Yes| D[SNS \u0026#43; SQS] C --\u0026gt;|No| E[SNS] B --\u0026gt;|No| F{Rate limit} F --\u0026gt;|Yes| G[SQS] F --\u0026gt;|No| H[Lambda Direct Invoke] class B internal-link; Source: AWS re:Invent 2020: Scalable serverless event-driven architectures with SNS, SQS \u0026amp; Lambda\nAzure #placeholder/description\nGCP #placeholder/description"},{"id":20,"href":"/hub/Data-Ingestion/consume_all_messages/","title":"Consume all messages","parent":"数据摄取","content":"在 Kafka 中，消费者组中的不同消费者默认会分别消费主题中的不同分区数据。但如果你希望每个消费者都消费全量数据，可以采用以下方法：\n手动分配分区：在创建消费者时，你可以手动分配分区给每个消费者。这样，每个消费者将负责处理指定的分区，从而确保每个消费者都能消费全量数据。这种方式需要你显式地管理分区分配，但可以精确控制消费者的数据处理.\n消费者组协调器：Kafka 的消费者组协调器会自动分配分区给消费者。如果你希望每个消费者都消费全量数据，可以将每个消费者订阅的主题分区数设置为与主题的分区总数相等。这样，每个消费者将负责处理所有分区，从而实现全量数据的消费.\nfrom kafka import KafkaConsumer, TopicPartition # 配置 Kafka 服务器和主题 bootstrap_servers = \u0026#39;your_kafka_broker\u0026#39; topic = \u0026#39;your_topic\u0026#39; # 创建消费者 consumer = KafkaConsumer( topic, group_id=\u0026#39;your_consumer_group\u0026#39;, bootstrap_servers=bootstrap_servers, auto_offset_reset=\u0026#39;earliest\u0026#39;, # 从最早的消息开始消费 enable_auto_commit=False, # 禁用自动提交偏移量 ) # 获取主题的分区列表 partitions = consumer.partitions_for_topic(topic) # 手动分配分区给消费者 for partition in partitions: tp = TopicPartition(topic, partition) consumer.assign([tp]) # 消费消息 for message in consumer: print(f\u0026#34;Received message: {message.value.decode(\u0026#39;utf-8\u0026#39;)}\u0026#34;) # 关闭消费者 consumer.close() ","description":"在 Kafka 中，消费者组中的不同消费者默认会分别消费主题中的不同分区数据。但如果你希望每个消费者都消费全量数据，可以采用以下方法：\n手动分配分区：在创建消费者时，你可以手动分配分区给每个消费者。这样，每个消费者将负责处理指定的分区，从而确保每个消费者都能消费全量数据。这种方式需要你显式地管理分区分配，但可以精确控制消费者的数据处理.\n消费者组协调器：Kafka 的消费者组协调器会自动分配分区给消费者。如果你希望每个消费者都消费全量数据，可以将每个消费者订阅的主题分区数设置为与主题的分区总数相等。这样，每个消费者将负责处理所有分区，从而实现全量数据的消费.\nfrom kafka import KafkaConsumer, TopicPartition # 配置 Kafka 服务器和主题 bootstrap_servers = \u0026#39;your_kafka_broker\u0026#39; topic = \u0026#39;your_topic\u0026#39; # 创建消费者 consumer = KafkaConsumer( topic, group_id=\u0026#39;your_consumer_group\u0026#39;, bootstrap_servers=bootstrap_servers, auto_offset_reset=\u0026#39;earliest\u0026#39;, # 从最早的消息开始消费 enable_auto_commit=False, # 禁用自动提交偏移量 ) # 获取主题的分区列表 partitions = consumer.partitions_for_topic(topic) # 手动分配分区给消费者 for partition in partitions: tp = TopicPartition(topic, partition) consumer.assign([tp]) # 消费消息 for message in consumer: print(f\u0026#34;Received message: {message.value.decode(\u0026#39;utf-8\u0026#39;)}\u0026#34;) # 关闭消费者 consumer.close() "},{"id":21,"href":"/hub/Fundamentals/languages/java/Java%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/","title":"Java相关工具","parent":"Java","content":"","description":""},{"id":22,"href":"/hub/Data-Ingestion/Kafka_Connect/","title":"Kafka Connect示例","parent":"数据摄取","content":" 使用kafka connect需要将一个日志目录文件数据进行导入，然后导出到mysql的数据库，中间涉及到对日志文件做一些转换，比如按照空格进行切分，选取其中的前三列存入mysql对应的表中。这个配置该如何写，相应的代码如何编写? 要实现这样的数据流转，您可以使用Kafka Connect的Source Connector来读取日志文件数据，然后使用Sink Connector将数据写入MySQL数据库。您需要编写两个配置文件，一个用于Source Connector，另一个用于Sink Connector。 以下是一个示例配置，假设您的日志文件中每行数据都是以空格分隔的：\nSource Connector 配置文件 (source.properties) name=source-connector connector.class=FileStreamSource tasks.max=1 file=/path/to/your/log/file.log topic=log-topic key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.storage.StringConverter Sink Connector 配置文件 (sink.properties) name=sink-connector connector.class=io.confluent.connect.jdbc.JdbcSinkConnector tasks.max=1 topics=log-topic connection.url=jdbc:mysql://localhost:3306/your_database connection.user=your_username connection.password=your_password auto.create=true insert.mode=upsert pk.mode=record_key pk.fields=your_primary_key_field fields.whitelist=col1,col2,col3 在这两个配置文件中，您需要根据实际情况替换文件路径、数据库连接信息、表字段等。\n然后，您可以使用以下命令启动Kafka Connect，分别加载这两个配置文件：\n启动 Source Connector $ bin/connect-standalone.sh config/connect-standalone.properties source.properties 启动 Sink Connector $ bin/connect-standalone.sh config/connect-standalone.properties sink.properties 这样配置完成后，Kafka Connect会将日志文件中的数据按行读取，然后根据空格拆分数据，选取前三列，最后将这些数据写入到MySQL数据库中。\n","description":"使用kafka connect需要将一个日志目录文件数据进行导入，然后导出到mysql的数据库，中间涉及到对日志文件做一些转换，比如按照空格进行切分，选取其中的前三列存入mysql对应的表中。这个配置该如何写，相应的代码如何编写? 要实现这样的数据流转，您可以使用Kafka Connect的Source Connector来读取日志文件数据，然后使用Sink Connector将数据写入MySQL数据库。您需要编写两个配置文件，一个用于Source Connector，另一个用于Sink Connector。 以下是一个示例配置，假设您的日志文件中每行数据都是以空格分隔的：\nSource Connector 配置文件 (source.properties) name=source-connector connector.class=FileStreamSource tasks.max=1 file=/path/to/your/log/file.log topic=log-topic key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.storage.StringConverter Sink Connector 配置文件 (sink.properties) name=sink-connector connector.class=io.confluent.connect.jdbc.JdbcSinkConnector tasks.max=1 topics=log-topic connection.url=jdbc:mysql://localhost:3306/your_database connection.user=your_username connection.password=your_password auto.create=true insert.mode=upsert pk.mode=record_key pk.fields=your_primary_key_field fields.whitelist=col1,col2,col3 在这两个配置文件中，您需要根据实际情况替换文件路径、数据库连接信息、表字段等。\n然后，您可以使用以下命令启动Kafka Connect，分别加载这两个配置文件：\n启动 Source Connector $ bin/connect-standalone.sh config/connect-standalone.properties source.properties 启动 Sink Connector $ bin/connect-standalone.sh config/connect-standalone.properties sink.properties 这样配置完成后，Kafka Connect会将日志文件中的数据按行读取，然后根据空格拆分数据，选取前三列，最后将这些数据写入到MySQL数据库中。"},{"id":23,"href":"/hub/Data-Visualization/Kibana/","title":"Kibana","parent":"数据可视化","content":"","description":""},{"id":24,"href":"/hub/Fundamentals/languages/scala/","title":"Scala","parent":"编程语言","content":"在大数据领域，Scala也有许多常用的框架和库，例如：\nApache Spark：Scala是Spark的主要支持语言，用于大规模数据处理和分析。 Apache Flink：Scala也广泛用于Flink的开发，支持流式数据处理和批处理。 Apache Kafka：Scala常用于Kafka的开发，用于构建实时数据流平台。 ScalaZ：提供了一系列函数式编程库，用于数据处理和函数式编程。 Breeze：用于数值计算和科学计算的库，支持线性代数和统计计算。 Scala适合于数据类的开发的原因包括：\n函数式编程支持：Scala是一种函数式编程语言，支持高阶函数、不可变数据结构等特性，适合数据处理和分析。 并发性能：Scala提供了Actor模型和并发库，可以更好地处理大规模数据并发任务。 表达力强：Scala具有强大的表达力和灵活性，可以编写简洁、优雅的数据处理代码。 Scala最常用的语言特性包括：\n函数式编程：Scala支持函数作为一等公民，提供了丰富的函数式编程特性。 模式匹配：Scala提供了强大的模式匹配功能，可以方便地处理复杂的数据结构。 类型推断：Scala具有类型推断功能，可以减少代码中的类型声明，提高代码的简洁性和可读性。 Scala相关工具 ","description":"在大数据领域，Scala也有许多常用的框架和库，例如：\nApache Spark：Scala是Spark的主要支持语言，用于大规模数据处理和分析。 Apache Flink：Scala也广泛用于Flink的开发，支持流式数据处理和批处理。 Apache Kafka：Scala常用于Kafka的开发，用于构建实时数据流平台。 ScalaZ：提供了一系列函数式编程库，用于数据处理和函数式编程。 Breeze：用于数值计算和科学计算的库，支持线性代数和统计计算。 Scala适合于数据类的开发的原因包括：\n函数式编程支持：Scala是一种函数式编程语言，支持高阶函数、不可变数据结构等特性，适合数据处理和分析。 并发性能：Scala提供了Actor模型和并发库，可以更好地处理大规模数据并发任务。 表达力强：Scala具有强大的表达力和灵活性，可以编写简洁、优雅的数据处理代码。 Scala最常用的语言特性包括：\n函数式编程：Scala支持函数作为一等公民，提供了丰富的函数式编程特性。 模式匹配：Scala提供了强大的模式匹配功能，可以方便地处理复杂的数据结构。 类型推断：Scala具有类型推断功能，可以减少代码中的类型声明，提高代码的简洁性和可读性。 Scala相关工具 "},{"id":25,"href":"/hub/Data-Storage/DFS/","title":"分布式文件存储","parent":"数据存储","content":" 分布式文件存储 ","description":" 分布式文件存储 "},{"id":26,"href":"/hub/Data-Storage/OSS/","title":"对象存储","parent":"数据存储","content":" Amazon S3 Glacier Amazon S3 ","description":" Amazon S3 Glacier Amazon S3 "},{"id":27,"href":"/hub/Data-Warehouse/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F/","title":"数仓建模的方式","parent":"数据仓库","content":" 维度建模 dimensional modeling （星型和雪花型）\n关系建模 relational modeling：关系模式是将数据仓库中的所有数据都存储在规范化的表中，遵循数据库范式化的设计原则。这种模式适用于需要保持数据一致性和避免数据冗余的场景。然而，关系模式可能导致查询性能较差，因为需要进行多表关联来获取数据。\n混合建模：混合模式是维度建模和标准化模式的结合，既包含维度建模中的维度表和事实表，又包含标准化模式中的规范化表。这种模式可以根据具体的业务需求和数据特点来灵活设计数据仓库结构。\n时间变化维度：时间变化维度模式用于处理维度数据随时间变化的情况，例如维度属性的历史记录、维度属性的变化等。常见的时间变化维度模式包括类型1、类型2和类型3维度变化。\n","description":"维度建模 dimensional modeling （星型和雪花型）\n关系建模 relational modeling：关系模式是将数据仓库中的所有数据都存储在规范化的表中，遵循数据库范式化的设计原则。这种模式适用于需要保持数据一致性和避免数据冗余的场景。然而，关系模式可能导致查询性能较差，因为需要进行多表关联来获取数据。\n混合建模：混合模式是维度建模和标准化模式的结合，既包含维度建模中的维度表和事实表，又包含标准化模式中的规范化表。这种模式可以根据具体的业务需求和数据特点来灵活设计数据仓库结构。\n时间变化维度：时间变化维度模式用于处理维度数据随时间变化的情况，例如维度属性的历史记录、维度属性的变化等。常见的时间变化维度模式包括类型1、类型2和类型3维度变化。"},{"id":28,"href":"/hub/Data-Visualization/Grafana/","title":"Grafana","parent":"数据可视化","content":"Grafana 架构：\nGrafana 是一个开源的数据可视化工具，其架构主要包括以下几个组件：\n前端界面：用户通过 Web 界面与 Grafana 进行交互，可以创建仪表盘、查询数据、设置警报等操作。\n后端服务：负责处理前端界面的请求，包括数据查询、数据存储、用户认证等功能。\n数据源插件：Grafana 支持多种数据源，如 Prometheus、InfluxDB、MySQL 等，用户可以通过数据源插件连接不同的数据源。\n图表插件：Grafana 提供了丰富的图表类型和可视化功能，用户可以选择合适的图表插件来展示数据。\n警报插件：用户可以设置警报规则，并通过警报插件发送通知，如邮件、Slack 消息等。\nGrafana 使用场景：\n监控和报警：Grafana 可以将监控数据可视化展示在仪表盘上，帮助用户实时监控系统性能、应用程序运行状态等，并设置相应的警报规则。\n数据分析：通过 Grafana 可以对存储在不同数据源中的数据进行查询和分析，帮助用户发现数据之间的关联和趋势。\n可视化报告：Grafana 提供了丰富的图表类型和布局选项，用户可以定制化生成各种数据可视化报告，用于展示给团队或客户。\n容器监控：Grafana 结合 Prometheus 等监控工具，可以对容器化环境中的资源利用率、性能指标等进行监控和可视化展示。\n总的来说，Grafana 是一个功能强大且灵活的数据可视化工具，适用于各种场景下的监控、数据分析和报告生成。\n","description":"Grafana 架构：\nGrafana 是一个开源的数据可视化工具，其架构主要包括以下几个组件：\n前端界面：用户通过 Web 界面与 Grafana 进行交互，可以创建仪表盘、查询数据、设置警报等操作。\n后端服务：负责处理前端界面的请求，包括数据查询、数据存储、用户认证等功能。\n数据源插件：Grafana 支持多种数据源，如 Prometheus、InfluxDB、MySQL 等，用户可以通过数据源插件连接不同的数据源。\n图表插件：Grafana 提供了丰富的图表类型和可视化功能，用户可以选择合适的图表插件来展示数据。\n警报插件：用户可以设置警报规则，并通过警报插件发送通知，如邮件、Slack 消息等。\nGrafana 使用场景：\n监控和报警：Grafana 可以将监控数据可视化展示在仪表盘上，帮助用户实时监控系统性能、应用程序运行状态等，并设置相应的警报规则。\n数据分析：通过 Grafana 可以对存储在不同数据源中的数据进行查询和分析，帮助用户发现数据之间的关联和趋势。\n可视化报告：Grafana 提供了丰富的图表类型和布局选项，用户可以定制化生成各种数据可视化报告，用于展示给团队或客户。\n容器监控：Grafana 结合 Prometheus 等监控工具，可以对容器化环境中的资源利用率、性能指标等进行监控和可视化展示。\n总的来说，Grafana 是一个功能强大且灵活的数据可视化工具，适用于各种场景下的监控、数据分析和报告生成。"},{"id":29,"href":"/hub/Data-Governance/Cost-Optimization-in-the-Cloud/","title":"Cost Optimization in the Cloud","parent":"数据治理","content":" Overview This guide provides general guidance for strategies to optimize various assets in the cloud. When talking about the cloud we will be using the most popular cloud providers as examples ([[Amazon Web Services|AWS]], [[Google Cloud Platform|GCP]], and [[Microsoft Azure|Azure]]).\nGeneral Compute General compute refers to servers that can be used to handle a large variety of general purpose work in the cloud. Typically, this kind of compute is used for transforming data or hosting a service. General compute services range from fully customizable to managed services where you have less control over the environment and settings.\nExamples of general compute services:\nAWS: EC2, Fargate, Batch Azure: Virtual Machine, Container Instances, Batch GCP: Compute Engine, Cloud Run, Batch on GKE Turn on metrics monitoring Before you can optimize anything, you need to turn on metrics to monitor the performance of your service. This monitoring is usually an additional expense but reasonable. If you don\u0026rsquo;t believe you\u0026rsquo;ll need it long term you can turn it on while you optimize and then turn it off later.\nExamples of metrics monitoring services:\nAWS: CloudWatch Azure Monitor GCP: Cloud Monitoring Datadog Once monitoring is turned on, focus on understanding your workload patterns and assessing whether your current usage is over-provisioned or under-provisioned. If you realize at this point that your workload is unpredictable, you may want to consider switching to a serverless service.\nRightsize resources Rightsizing is a term that means identifying and adjusting specific resources to increase resource utilization and potentially save costs. This adjustment usually happens when there\u0026rsquo;s an over-provisioning situation. Now that you\u0026rsquo;ve activated metric monitoring and gathered data on your resource usage, ensure that your instance size is suitable. This is the point where you\u0026rsquo;ll fine-tune the instance size to match the CPU and memory requirements of your workload.\nEnable Autoscaling After rightsizing your compute service, you can typically enable autoscaling to dynamically adjust resources up and down based on demand in your workload. This means that if demand is low, autoscaling will reduce the amount of resources provisioned allowing you to save money. Along with autoscaling, you will typically set high and low thresholds which should be based around your typical workload.\nSavings plans Finally, after exploring the above options, you can usually get significant savings by purchasing savings plans which are typically longer range commitments to use a predetermined amount of a resource. These are great when you know that your workload is relatively steady and predictable. Savings plans are a great high impact and low effort option for saving money.\nAWS Savings plans Azure savings plans for compute GCP Committed use discounts Databases #placeholder\n","description":"Overview This guide provides general guidance for strategies to optimize various assets in the cloud. When talking about the cloud we will be using the most popular cloud providers as examples ([[Amazon Web Services|AWS]], [[Google Cloud Platform|GCP]], and [[Microsoft Azure|Azure]]).\nGeneral Compute General compute refers to servers that can be used to handle a large variety of general purpose work in the cloud. Typically, this kind of compute is used for transforming data or hosting a service."},{"id":30,"href":"/hub/Data-Storage/format/JSON/","title":"JSON","parent":"文件格式","content":" JSON格式规范 JSON（JavaScript Object Notation）是一种轻量级的数据交换格式，常用于前端和后端之间的数据传输。下面是JSON格式的一些规范：\n数据类型：JSON支持以下数据类型：\n字符串（必须用双引号括起来） 数字 布尔值（true或false） 数组（用方括号[]表示） 对象（用花括号{}表示） null 键值对：JSON数据由键值对组成，键和值之间使用冒号分隔，不同键值对之间使用逗号分隔。键必须是字符串，值可以是任意合法的JSON数据类型。\n对象：JSON对象由一对花括号{}包裹，里面包含零个或多个键值对。\n数组：JSON数组由一对方括号[]包裹，里面包含零个或多个值，值之间使用逗号分隔。\n嵌套：JSON支持嵌套结构，即在对象或数组中可以包含更深层次的对象或数组。\n空白字符：JSON中可以包含空格、制表符、换行符等空白字符，用于提高可读性，但在解析时会忽略这些空白字符。\n字符编码：JSON文本必须使用UTF-8编码。\n注释：JSON不支持注释，不能在JSON数据中添加注释。\n字符转义：特殊字符如双引号、反斜杠等需要进行转义，使用反斜杠进行转义。\n总的来说，JSON格式的规范相对简单，易于理解和使用，适合用于数据的序列化和反序列化。\n优势和劣势 JSON格式作为一种轻量级的数据交换格式，具有以下优势和劣势：\n优势：\n简洁易读：JSON格式采用文本形式表示数据，易于人类阅读和编写，具有较好的可读性。 跨平台兼容：JSON格式是一种通用的数据格式，几乎所有编程语言和平台都支持JSON的解析和生成，具有良好的跨平台兼容性。 数据结构清晰：JSON支持嵌套结构，可以表示复杂的数据关系，适用于各种数据结构的表示。 支持多种数据类型：JSON支持字符串、数字、布尔值、数组、对象等多种数据类型，灵活性较高。 数据传输效率高：JSON格式相对于一些XML等格式来说，数据量较小，传输效率较高。 劣势：\n不适合大规模数据：对于大规模数据集，JSON格式可能会占用较大的存储空间，不如二进制格式节省空间。 不支持注释：JSON格式不支持注释，这在某些场景下可能会限制开发人员对数据的描述和解释。 可读性较差：当JSON数据结构嵌套层次较深时，可读性可能会降低，不如XML等格式具有良好的结构化。 不支持循环引用：JSON不支持循环引用，即一个对象中不能包含对自身的引用，这可能会限制某些数据模型的表示。 综上所述，JSON格式在简洁、易读、跨平台兼容等方面具有优势，适合用于数据交换和存储，但在处理大规模数据、需要注释、循环引用等特殊场景下可能存在一些限制。\n","description":"JSON格式规范 JSON（JavaScript Object Notation）是一种轻量级的数据交换格式，常用于前端和后端之间的数据传输。下面是JSON格式的一些规范：\n数据类型：JSON支持以下数据类型：\n字符串（必须用双引号括起来） 数字 布尔值（true或false） 数组（用方括号[]表示） 对象（用花括号{}表示） null 键值对：JSON数据由键值对组成，键和值之间使用冒号分隔，不同键值对之间使用逗号分隔。键必须是字符串，值可以是任意合法的JSON数据类型。\n对象：JSON对象由一对花括号{}包裹，里面包含零个或多个键值对。\n数组：JSON数组由一对方括号[]包裹，里面包含零个或多个值，值之间使用逗号分隔。\n嵌套：JSON支持嵌套结构，即在对象或数组中可以包含更深层次的对象或数组。\n空白字符：JSON中可以包含空格、制表符、换行符等空白字符，用于提高可读性，但在解析时会忽略这些空白字符。\n字符编码：JSON文本必须使用UTF-8编码。\n注释：JSON不支持注释，不能在JSON数据中添加注释。\n字符转义：特殊字符如双引号、反斜杠等需要进行转义，使用反斜杠进行转义。\n总的来说，JSON格式的规范相对简单，易于理解和使用，适合用于数据的序列化和反序列化。\n优势和劣势 JSON格式作为一种轻量级的数据交换格式，具有以下优势和劣势：\n优势：\n简洁易读：JSON格式采用文本形式表示数据，易于人类阅读和编写，具有较好的可读性。 跨平台兼容：JSON格式是一种通用的数据格式，几乎所有编程语言和平台都支持JSON的解析和生成，具有良好的跨平台兼容性。 数据结构清晰：JSON支持嵌套结构，可以表示复杂的数据关系，适用于各种数据结构的表示。 支持多种数据类型：JSON支持字符串、数字、布尔值、数组、对象等多种数据类型，灵活性较高。 数据传输效率高：JSON格式相对于一些XML等格式来说，数据量较小，传输效率较高。 劣势：\n不适合大规模数据：对于大规模数据集，JSON格式可能会占用较大的存储空间，不如二进制格式节省空间。 不支持注释：JSON格式不支持注释，这在某些场景下可能会限制开发人员对数据的描述和解释。 可读性较差：当JSON数据结构嵌套层次较深时，可读性可能会降低，不如XML等格式具有良好的结构化。 不支持循环引用：JSON不支持循环引用，即一个对象中不能包含对自身的引用，这可能会限制某些数据模型的表示。 综上所述，JSON格式在简洁、易读、跨平台兼容等方面具有优势，适合用于数据交换和存储，但在处理大规模数据、需要注释、循环引用等特殊场景下可能存在一些限制。"},{"id":31,"href":"/hub/Fundamentals/languages/scala/Scala%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/","title":"Scala相关工具","parent":"Scala","content":"","description":""},{"id":32,"href":"/hub/Data-Process/","title":"数据处理","parent":"数据工程知识库","content":" Spark问题 Spark 的架构 spark 提交作业的流程 spark wordcount ，分别用python，java ，scala来表示 spark DataSet 和 DataStream 的差异 Spark 流处理的API，一个简单的Spark 流处理的例子 Spark 流处理 如何集合Spark ML预测交易欺诈 Spark 图处理的典型利用案例 Spark 在处理大量数据时候有哪些优化手段 Flink问题 Flink 和Spark 的优劣分析和对比 Flink作业提交的流程 Flink的wordcount ，分别用python，java，scala来表示 Flink流处理的API Flink SQL的简单案例 Flink CDC的简单案例 Flink on yarn startup procedure flink wordcount test mermaid Spark Join的类型和使用 spark概念和架构 spark面试题 spark shuffle 详解 spark structured streaming案例 Flink案例 ","description":" Spark问题 Spark 的架构 spark 提交作业的流程 spark wordcount ，分别用python，java ，scala来表示 spark DataSet 和 DataStream 的差异 Spark 流处理的API，一个简单的Spark 流处理的例子 Spark 流处理 如何集合Spark ML预测交易欺诈 Spark 图处理的典型利用案例 Spark 在处理大量数据时候有哪些优化手段 Flink问题 Flink 和Spark 的优劣分析和对比 Flink作业提交的流程 Flink的wordcount ，分别用python，java，scala来表示 Flink流处理的API Flink SQL的简单案例 Flink CDC的简单案例 Flink on yarn startup procedure flink wordcount test mermaid Spark Join的类型和使用 spark概念和架构 spark面试题 spark shuffle 详解 spark structured streaming案例 Flink案例 "},{"id":33,"href":"/hub/Data-Visualization/prometheus/","title":"Prometheus","parent":"数据可视化","content":"Prometheus 收集和存储数据的工作流程如下：\n采集数据：Prometheus 通过配置 job 和 target 来定期抓取监控目标的指标数据。这些监控目标可以是服务器、应用程序、数据库等。Prometheus 支持多种方式采集数据，包括通过 HTTP 协议的 Pull 模式和客户端库的 Push 模式。\n存储数据：Prometheus 将采集到的指标数据存储在本地的时间序列数据库中。默认情况下，Prometheus 使用本地存储来保存数据，可以配置存储周期和数据保留策略。\n查询数据：用户可以通过 Prometheus 提供的 PromQL 查询语言来查询存储在数据库中的数据。PromQL 支持丰富的操作符和函数，可以进行灵活的数据查询和分析。\n通过以上工作流程，Prometheus 实现了高效的数据采集、存储和查询功能，为用户提供了强大的监控和分析能力。\n","description":"Prometheus 收集和存储数据的工作流程如下：\n采集数据：Prometheus 通过配置 job 和 target 来定期抓取监控目标的指标数据。这些监控目标可以是服务器、应用程序、数据库等。Prometheus 支持多种方式采集数据，包括通过 HTTP 协议的 Pull 模式和客户端库的 Push 模式。\n存储数据：Prometheus 将采集到的指标数据存储在本地的时间序列数据库中。默认情况下，Prometheus 使用本地存储来保存数据，可以配置存储周期和数据保留策略。\n查询数据：用户可以通过 Prometheus 提供的 PromQL 查询语言来查询存储在数据库中的数据。PromQL 支持丰富的操作符和函数，可以进行灵活的数据查询和分析。\n通过以上工作流程，Prometheus 实现了高效的数据采集、存储和查询功能，为用户提供了强大的监控和分析能力。"},{"id":34,"href":"/hub/Data-Warehouse/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%88%86%E5%B1%82%E8%AE%BE%E8%AE%A1/","title":"数据仓库分层设计","parent":"数据仓库","content":"数据仓库的分层设计是关键的环节，它有助于整理和管理数据，使其更易于理解和使用。让我们深入了解数据仓库的分层原理。\n数据仓库分层 数据运营层（ODS）：\nOperation Data Store，也称为贴源层。 数据从源系统抽取、清洗、传输到这一层。 主要功能： 为数据仓库层提供原始数据。 减少对业务系统的影响。 不建议在此层进行过多的数据清洗，应保持原始数据。 数据仓库层：\n从上到下分为三个层次：数据细节层（DWD）、数据中间层（DWM）、数据服务层（DWS）。 数据细节层（DWD）： 保持与贴源层相同的数据颗粒度。 对原始数据进行清洗和规范化，如去除空数据、脏数据、离群值。 通常采用维度退化方法，将维度退化至事实表中，减少关联操作。 数据中间层（DWM）： 在DWD层基础上，进行轻微的聚合操作，生成中间结果表。 提升公共指标的复用性，减少重复加工。 数据服务层（DWS）： 整合DWM上的基础数据，形成宽表。 用于提供后续的业务查询、OLAP分析、数据分发等。 数据应用层（ADS）：\n存放供数据产品和数据分析使用的数据。 可存放在ES、Redis、PostgreSQL等系统中，也可能存放在Hive或Druid中。 用于线上系统或数据分析和挖掘。 总结 数据仓库的分层设计有助于清晰地组织数据、简化复杂问题、统一数据口径，并减少重复开发。每个公司可以根据业务需求自定义不同的层次。¹³⁴⁵\nSource: Conversation with Bing, 3/31/2024 (1) 详解数据仓库分层 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/377978194. (2) 数据仓库——分层原理-百度开发者中心. https://developer.baidu.com/article/detail.html?id=2860353. (3) 漫谈数据仓库：如何优雅地设计数据分层-百度开发者中心. https://developer.baidu.com/article/detail.html?id=2902581.\n","description":"数据仓库的分层设计是关键的环节，它有助于整理和管理数据，使其更易于理解和使用。让我们深入了解数据仓库的分层原理。\n数据仓库分层 数据运营层（ODS）：\nOperation Data Store，也称为贴源层。 数据从源系统抽取、清洗、传输到这一层。 主要功能： 为数据仓库层提供原始数据。 减少对业务系统的影响。 不建议在此层进行过多的数据清洗，应保持原始数据。 数据仓库层：\n从上到下分为三个层次：数据细节层（DWD）、数据中间层（DWM）、数据服务层（DWS）。 数据细节层（DWD）： 保持与贴源层相同的数据颗粒度。 对原始数据进行清洗和规范化，如去除空数据、脏数据、离群值。 通常采用维度退化方法，将维度退化至事实表中，减少关联操作。 数据中间层（DWM）： 在DWD层基础上，进行轻微的聚合操作，生成中间结果表。 提升公共指标的复用性，减少重复加工。 数据服务层（DWS）： 整合DWM上的基础数据，形成宽表。 用于提供后续的业务查询、OLAP分析、数据分发等。 数据应用层（ADS）：\n存放供数据产品和数据分析使用的数据。 可存放在ES、Redis、PostgreSQL等系统中，也可能存放在Hive或Druid中。 用于线上系统或数据分析和挖掘。 总结 数据仓库的分层设计有助于清晰地组织数据、简化复杂问题、统一数据口径，并减少重复开发。每个公司可以根据业务需求自定义不同的层次。¹³⁴⁵\nSource: Conversation with Bing, 3/31/2024 (1) 详解数据仓库分层 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/377978194. (2) 数据仓库——分层原理-百度开发者中心. https://developer.baidu.com/article/detail.html?id=2860353. (3) 漫谈数据仓库：如何优雅地设计数据分层-百度开发者中心. https://developer.baidu.com/article/detail.html?id=2902581."},{"id":35,"href":"/hub/Data-Ingestion/Flink_cdc/","title":"Flink CDC","parent":"数据摄取","content":" 什么是CDC 广义概念上，能够捕获数据变更的技术统称为 CDC（Change Data Capture）。通常我们说的 CDC 主要面向数据库的变更，是一种用于捕获数据库中数据变化的技术。\nCDC的用途 CDC 的主要应用有三个方面：\n数据同步，通过 CDC 将数据同步到其他存储位置来进行异地灾备或备份。 数据分发，通过 CDC 将数据从一个数据源抽取出来后分发给下游各个业务方做数据处理和变换。 数据采集，使用 CDC 将源端数据库中的数据读取出来后，经过 ETL 写入数据仓库或数据湖。 CDC的实现途径 按照实现机制，CDC 可以分为两种类型：\n基于查询和基于日志的 CDC。基于查询的 CDC 通过定时调度离线任务的方式实现，一般为批处理模式，无法保证数据的实时性，数据一致性也会受到影响。 基于日志的 CDC 通过实时消费数据库里的日志变化实现，如通过连接器直接读取 MySQL 的 binlog 捕获变更。这种流处理模式可以做到低延迟，因此更好地保障了数据的实时性和一致性。 Flink CDC和其他常见CDC技术的比较 在上图中，我们比较了几种常见的 CDC 方案。相比于其他方案，Flink CDC 在功能上集成了许多优势：\n在实现机制方面，Flink CDC 通过直接读取数据库日志捕获数据变更，保障了数据实时性和一致性。 在同步能力方面，Flink CDC 支持全量和增量两种读取模式，并且可以做到无缝切换。 在数据连续性方面，Flink CDC 充分利用了 Apache Flink 的 checkpoint 机制，提供了断点续传功能，当作业出现故障重启后可以从中断的位置直接启动恢复。 在架构方面，Flink CDC 的分布式设计使得用户可以启动多个并发来消费源库中的数据。 在数据变换方面，Flink CDC 将从数据库中读取出来后，可以通过 DataStream、SQL 等进行各种复杂计算和数据处理。 在生态方面，Flink CDC 依托于强大的 Flink 生态和众多的 connector 种类，可以将实时数据对接至多种外部系统。\nFlink CDC 示例 ################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: \u0026#34;\u0026#34; table.create.properties.light_schema_change: true table.create.properties.replication_num: 1 pipeline: name: Sync MySQL Database to Doris parallelism: 2 运行\nbin/flink-cdc.sh mysql-to-doris.yaml ","description":"什么是CDC 广义概念上，能够捕获数据变更的技术统称为 CDC（Change Data Capture）。通常我们说的 CDC 主要面向数据库的变更，是一种用于捕获数据库中数据变化的技术。\nCDC的用途 CDC 的主要应用有三个方面：\n数据同步，通过 CDC 将数据同步到其他存储位置来进行异地灾备或备份。 数据分发，通过 CDC 将数据从一个数据源抽取出来后分发给下游各个业务方做数据处理和变换。 数据采集，使用 CDC 将源端数据库中的数据读取出来后，经过 ETL 写入数据仓库或数据湖。 CDC的实现途径 按照实现机制，CDC 可以分为两种类型：\n基于查询和基于日志的 CDC。基于查询的 CDC 通过定时调度离线任务的方式实现，一般为批处理模式，无法保证数据的实时性，数据一致性也会受到影响。 基于日志的 CDC 通过实时消费数据库里的日志变化实现，如通过连接器直接读取 MySQL 的 binlog 捕获变更。这种流处理模式可以做到低延迟，因此更好地保障了数据的实时性和一致性。 Flink CDC和其他常见CDC技术的比较 在上图中，我们比较了几种常见的 CDC 方案。相比于其他方案，Flink CDC 在功能上集成了许多优势：\n在实现机制方面，Flink CDC 通过直接读取数据库日志捕获数据变更，保障了数据实时性和一致性。 在同步能力方面，Flink CDC 支持全量和增量两种读取模式，并且可以做到无缝切换。 在数据连续性方面，Flink CDC 充分利用了 Apache Flink 的 checkpoint 机制，提供了断点续传功能，当作业出现故障重启后可以从中断的位置直接启动恢复。 在架构方面，Flink CDC 的分布式设计使得用户可以启动多个并发来消费源库中的数据。 在数据变换方面，Flink CDC 将从数据库中读取出来后，可以通过 DataStream、SQL 等进行各种复杂计算和数据处理。 在生态方面，Flink CDC 依托于强大的 Flink 生态和众多的 connector 种类，可以将实时数据对接至多种外部系统。"},{"id":36,"href":"/hub/Data-Governance/SQL-Guide/","title":"SQL Guide","parent":"数据治理","content":" Overview This guide is intended to be a general [[SQL]] reference for data engineers. It is not specific to any particular [[SQL#SQL Variants|variant of SQL]]. It also does not cover every concept or feature of SQL - only the most important or commonly used ones in data engineering.\n[!info]- ### SQL Learning Resources ![[Learning Resources#SQL Learning Resources]]\n1. Beginner SQL Order of Operations SQL executes each clause in a query in a defined order.\nFROM, including JOINs WHERE GROUP BY HAVING WINDOW functions SELECT DISTINCT UNION ORDER BY LIMIT and OFFSET Basic Commands SELECT\nUsed to select data from a database The data returned is stored in a result table, called the result-set. FROM\nUsed to specify which table to select or delete data from. WHERE\nUsed to filter records. It is used to extract only those records that fulfill a specified condition. ORDER BY\nUsed to sort the result set in ascending or descending order. Sorts the result set in ascending order by default. To sort the records in descending order, use the DESC keyword. Joins INNER JOIN\nReturns only those records or rows that have matching values and is used to retrieve data that appears in both tables. LEFT JOIN\nGives the output of the matching rows between both tables. In case, no records match from the left table, it shows those records with null values. RIGHT JOIN\nGives the output of the matching rows between both tables. In case, no records match from the right table, it shows those records with null values. FULL (OUTER) JOIN\nWill retrieve not only the matching rows but also the unmatched rows as well. CROSS (CARTESIAN) JOIN\nJoins every row from the first table with every row from the second table and its result comprises all combinations of records in two tables. SELF JOIN\nJoins a table to itself. UNION vs UNION ALL UNION and UNION ALL are both used to retrieve records from multiple tables. Both UNION and UNION ALL are known as set operators. In SQL, set operators combine the results of two or more queries into a single result.\nThere is one major difference:\nUNION only returns unique UNION ALL returns all records, including duplicates. Example: The columns in both SELECT statements are of the same or matching data types.\nSELECT column_1, column_2 FROM table_1 [WHERE condition] UNION [ALL] SELECT column_1, column_2 FROM table_2 [WHERE condition] Filtering Data Filtering data with SQL is useful for returning desired reults from a dataset or table. Filtering can be accomplsihed with the WHERE clause.\nExamples:\nFilter with logical operators AND operator\n# this will return rows where the City column has a value of \u0026#34;London\u0026#34; and the Country column has a value of \u0026#34;UK\u0026#34; SELECT * FROM Customers WHERE City = \u0026#34;London\u0026#34; AND Country = \u0026#34;UK\u0026#34; OR operator\n# this will return rows where the City column has a value of either \u0026#34;London\u0026#34; or \u0026#34;Paris\u0026#34; SELECT * FROM Customers WHERE City = \u0026#34;London\u0026#34; OR City = \u0026#34;Paris\u0026#34; BETWEEN operator\n# this will return rows where the Price column has values that are between 50 and 60 SELECT * FROM Products WHERE Price BETWEEN 50 AND 60 LIKE operator\n# this will return rows where the City column has values that start with \u0026#39;S\u0026#39; with no character limit SELECT * FROM Customers WHERE City LIKE \u0026#39;S%\u0026#39; \u0026gt;\u0026gt; Santiago, Sydney, San Antonio # the \u0026#39;%\u0026#39; wildcard can placed any where in a string to try and find a match SELECT * FROM Customers WHERE City LIKE \u0026#39;%ar%\u0026#39; \u0026gt;\u0026gt; Paris, Barcelona, Jakarta # the \u0026#39;_\u0026#39; wildcard can also be used in conjuction with \u0026#39;%\u0026#39; to find a single value # this will return rows where \u0026#39;a\u0026#39; has to be the second character and the rest of the characters can be anything SELECT * FROM Customers WHERE City LIKE \u0026#39;_a%\u0026#39; \u0026gt;\u0026gt; Lagos, Manila, Cairo Limit Used to specify the number of records to return. Different database systems use their own syntax:\nSQL Server = SELECT TOP 3 * FROM Customers; MySQL = SELECT * FROM Customers LIMIT 3; Oracle = SELECT * FROM Customers WHERE ROWNUM \u0026lt;= 3 Where vs Having WHERE Introduces a condition on individual rows. Use before GROUP BY clause. HAVING Introduces a condition on aggregations, i.e. results of selection where a single result, such as COUNT(), SUM(), MAX(), MIN() has been produced from multiple rows. Use after GROUP BY clause. 2. Intermediate SQL CTE vs Subquery CTE\nCan be used multiple times in the body of a query. Allows for recursive queries. Generally more readable. Subquery\nCan only be used once in a query. Can be used to filter results in the WHERE clause. Can be used as a column in your query. Table vs View vs Materialized View Table View Materialized View A table contains records (rows) with data values for specified columns (fields) and is stored physically in a database. Tables can be joined together to create reports, manipulated through SQL queries, and changed directly by updating or deleting individual rows or columns. A view is a virtual table based on the results of a SQL statement. It does not physically exist in the database and the fields in the view are fields from one or more real tables in the database. A materialized view is a snapshot of a query saved as a physical object within the database. Materialized views can be used instead of tables and support all operations available to real tables with some drawbacks such as maintenance costs. Data physically stored in database Data not physically stored Data physically stored in database Faster to query Slower to query Faster to query If underlying table is dropped then view will no longer work Materialized view will continue to work if underlying table is dropped No additional maintenance required No additional maintenance required Needs to be updated as new/updated data arrive in underlying table Case Statements Case statements are SQL\u0026rsquo;s version of an if-else logic. The CASE expression goes through conditions and returns a value when the first condition is met. Once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the ELSE clause.\nExample:\n--Sytanx CASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 WHEN conditionN THEN resultN ELSE result END; --Example SELECT OrderID, Quantity, CASE WHEN Quantity \u0026gt; 30 THEN \u0026#39;The quantity is greater than 30\u0026#39; WHEN Quantity = 30 THEN \u0026#39;The quantity is 30\u0026#39; ELSE \u0026#39;The quantity is under 30\u0026#39; END AS QuantityText FROM OrderDetails; DML vs DDL DML\nDML is short name of Data Manipulation Language which deals with data manipulation, and includes most common SQL statements such SELECT, INSERT, UPDATE, DELETE etc, and it is used to store, modify, retrieve, delete and update data in database.\nSELECT – retrieve data from one or more tables. INSERT – insert data into a table. UPDATE – updates existing data within a table. DELETE – delete all records from a table. MERGE – UPSERT operation (insert or update) CALL – call a PL/SQL or Java subprogram. EXPLAIN PLAN – interpretation of the data access path. LOCK TABLE – concurrency control. DDL\nDDL is short name of Data Definition Language, which deals with database schemas and descriptions, of how the data should reside in the database.\nCREATE – to create database and its objects like (table, index, views, store procedure, function and triggers). ALTER – alters the structure of the existing database. DROP – delete objects from the database. TRUNCATE – remove all records from a table; also, all spaces allocated for the records are removed. COMMENT – add comments to the data dictionary. RENAME – rename an object. (Source: StackOverflow)\nAggregate Functions An SQL aggregate function calculates on a set of values and returns a single value. For example, the average function (AVG) takes a list of values and returns the average.\nOther basic aggregate functions include:\nCOUNT() – returns the number of items in a set. MAX() – returns the maximum value in a set. MIN() – returns the minimum value in a set SUM() – returns the sum of all or distinct values in a set Note that aggregate functions do not work within in a WHERE clause due to the order of evaluation of clauses. Instead, GROUP BY and HAVING clases are used in place of a WHERE clause.\n3. Advanced SQL Window Functions Online Reference\nWindow functions perform calculations on a set of rows that are related together, but, unlike aggregate functions, windowing functions do not collapse the result of the rows into a single value. Instead, all the rows maintain their original identity and the calculated result is returned for every row.\n![[Assets/window-vs-aggregate-function.png]]\nCorrelated Subqueries ![[Correlated Subquery]]\n","description":"Overview This guide is intended to be a general [[SQL]] reference for data engineers. It is not specific to any particular [[SQL#SQL Variants|variant of SQL]]. It also does not cover every concept or feature of SQL - only the most important or commonly used ones in data engineering.\n[!info]- ### SQL Learning Resources ![[Learning Resources#SQL Learning Resources]]\n1. Beginner SQL Order of Operations SQL executes each clause in a query in a defined order."},{"id":37,"href":"/hub/Data-Storage/format/yaml/","title":"yaml格式","parent":"文件格式","content":" 格式规范 YAML（YAML Ain\u0026rsquo;t Markup Language）是一种人类可读的数据序列化格式，具有简洁、易读、易写的特点。以下是关于YAML格式的一些规范：\n缩进：YAML使用缩进来表示数据结构，缩进必须使用空格字符，不能使用制表符。通常使用2个空格或4个空格作为缩进。\n键值对：YAML中的键值对使用冒号(:)分隔，键值对之间不需要逗号分隔。\n列表：YAML中的列表使用短横线(-)表示，后面跟着一个空格。可以用列表表示一组值。\n对象：YAML中的对象使用冒号(:)和缩进表示，对象的键值对需要在同一缩进级别下。对象可以嵌套在其他对象或列表中。\n注释：YAML支持单行注释，使用井号(#)表示注释，井号后面的内容为注释内容。\n字符串：YAML中的字符串可以使用双引号(\u0026quot;\u0026quot;)或单引号(\u0026rsquo;\u0026rsquo;)括起来，也可以不使用引号。\n多行字符串：YAML支持多行字符串，在字符串内容前后使用竖线(|)表示保留换行符，使用大于符号(\u0026gt;)表示折叠换行符。\n特殊字符转义：如果字符串中包含特殊字符，可以使用转义字符进行转义。\n空值：YAML中的空值可以表示为null或~。\n总的来说，YAML格式具有简洁、易读、易写的特点，适合用于配置文件、数据序列化等场景。它与JSON格式相比，更强调人类可读性，适合于需要手动编辑的场景。\n","description":"格式规范 YAML（YAML Ain\u0026rsquo;t Markup Language）是一种人类可读的数据序列化格式，具有简洁、易读、易写的特点。以下是关于YAML格式的一些规范：\n缩进：YAML使用缩进来表示数据结构，缩进必须使用空格字符，不能使用制表符。通常使用2个空格或4个空格作为缩进。\n键值对：YAML中的键值对使用冒号(:)分隔，键值对之间不需要逗号分隔。\n列表：YAML中的列表使用短横线(-)表示，后面跟着一个空格。可以用列表表示一组值。\n对象：YAML中的对象使用冒号(:)和缩进表示，对象的键值对需要在同一缩进级别下。对象可以嵌套在其他对象或列表中。\n注释：YAML支持单行注释，使用井号(#)表示注释，井号后面的内容为注释内容。\n字符串：YAML中的字符串可以使用双引号(\u0026quot;\u0026quot;)或单引号(\u0026rsquo;\u0026rsquo;)括起来，也可以不使用引号。\n多行字符串：YAML支持多行字符串，在字符串内容前后使用竖线(|)表示保留换行符，使用大于符号(\u0026gt;)表示折叠换行符。\n特殊字符转义：如果字符串中包含特殊字符，可以使用转义字符进行转义。\n空值：YAML中的空值可以表示为null或~。\n总的来说，YAML格式具有简洁、易读、易写的特点，适合用于配置文件、数据序列化等场景。它与JSON格式相比，更强调人类可读性，适合于需要手动编辑的场景。"},{"id":38,"href":"/hub/Data-Ingestion/","title":"数据摄取","parent":"数据工程知识库","content":"数据摄取是指从不同数据源中提取数据并将其加载到数据存储或数据仓库中的过程。以下是一些常用的开源和商业数据摄取技术：\n开源技术：\nApache Kafka：用于实时数据流处理的分布式流平台，可以用于数据摄取和数据流处理。 Apache NiFi：一个易于使用、强大且可靠的数据自动化系统，可用于数据摄取、转换和传输。 Flume：Apache基金会的另一个项目，用于高可靠性、分布式、可配置的数据采集系统。 Logstash：一个用于数据收集、处理和转发的开源工具，通常与Elasticsearch等工具一起使用。 Sqoop：用于在Apache Hadoop和传统关系型数据库之间进行数据传输的工具。 商业技术：\nInformatica PowerCenter：一款强大的企业数据集成工具，支持各种数据源之间的数据传输和转换。 Talend Data Integration：一个全面的数据集成平台，支持数据摄取、数据转换和数据加载等功能。 IBM DataStage：IBM提供的企业级数据集成工具，支持复杂的ETL（Extract, Transform, Load）操作。 Microsoft SQL Server Integration Services (SSIS)：微软提供的ETL工具，用于数据集成和数据转换。 Oracle Data Integrator (ODI)：甲骨文提供的数据集成工具，支持各种数据源之间的数据传输和转换。 以上列举的技术只是一部分，数据摄取领域有很多其他开源和商业工具可供选择，具体选择取决于项目需求、技术栈和预算等因素。\nApache Kafka详解 Consume all messages Kafka Connect示例 Flink CDC Flink CDC结合Debezium Data Build Tool ","description":"数据摄取是指从不同数据源中提取数据并将其加载到数据存储或数据仓库中的过程。以下是一些常用的开源和商业数据摄取技术：\n开源技术：\nApache Kafka：用于实时数据流处理的分布式流平台，可以用于数据摄取和数据流处理。 Apache NiFi：一个易于使用、强大且可靠的数据自动化系统，可用于数据摄取、转换和传输。 Flume：Apache基金会的另一个项目，用于高可靠性、分布式、可配置的数据采集系统。 Logstash：一个用于数据收集、处理和转发的开源工具，通常与Elasticsearch等工具一起使用。 Sqoop：用于在Apache Hadoop和传统关系型数据库之间进行数据传输的工具。 商业技术：\nInformatica PowerCenter：一款强大的企业数据集成工具，支持各种数据源之间的数据传输和转换。 Talend Data Integration：一个全面的数据集成平台，支持数据摄取、数据转换和数据加载等功能。 IBM DataStage：IBM提供的企业级数据集成工具，支持复杂的ETL（Extract, Transform, Load）操作。 Microsoft SQL Server Integration Services (SSIS)：微软提供的ETL工具，用于数据集成和数据转换。 Oracle Data Integrator (ODI)：甲骨文提供的数据集成工具，支持各种数据源之间的数据传输和转换。 以上列举的技术只是一部分，数据摄取领域有很多其他开源和商业工具可供选择，具体选择取决于项目需求、技术栈和预算等因素。\nApache Kafka详解 Consume all messages Kafka Connect示例 Flink CDC Flink CDC结合Debezium Data Build Tool "},{"id":39,"href":"/hub/Data-Visualization/prometheus%E5%92%8CGrafana%E7%9A%84%E9%9B%86%E6%88%90/","title":"Prometheus和Grafana的集成","parent":"数据可视化","content":"Grafana 和 Prometheus 是常用的监控工具，它们可以通过以下步骤进行集成：\nPrometheus 配置：首先需要在 Prometheus 的配置文件中添加需要监控的目标（targets），并配置相关的监控规则（rules）和警报规则（alerting rules）。\nGrafana 数据源配置：在 Grafana 中添加 Prometheus 作为数据源，需要提供 Prometheus 的 URL 和其他相关配置信息。\n创建仪表盘：在 Grafana 中创建仪表盘，并选择 Prometheus 作为数据源。可以通过 Grafana 提供的图表编辑功能，从 Prometheus 中查询数据并进行可视化展示。\n通过这种集成方式，用户可以借助 Prometheus 收集和存储监控数据，再通过 Grafana 进行数据的可视化展示和分析，实现全面的监控和分析功能。\n","description":"Grafana 和 Prometheus 是常用的监控工具，它们可以通过以下步骤进行集成：\nPrometheus 配置：首先需要在 Prometheus 的配置文件中添加需要监控的目标（targets），并配置相关的监控规则（rules）和警报规则（alerting rules）。\nGrafana 数据源配置：在 Grafana 中添加 Prometheus 作为数据源，需要提供 Prometheus 的 URL 和其他相关配置信息。\n创建仪表盘：在 Grafana 中创建仪表盘，并选择 Prometheus 作为数据源。可以通过 Grafana 提供的图表编辑功能，从 Prometheus 中查询数据并进行可视化展示。\n通过这种集成方式，用户可以借助 Prometheus 收集和存储监控数据，再通过 Grafana 进行数据的可视化展示和分析，实现全面的监控和分析功能。"},{"id":40,"href":"/hub/Data-Storage/format/CSV/","title":"CSV","parent":"文件格式","content":"CSV（Comma-Separated Values）是一种常用的文本文件格式，用于存储表格数据。下面是CSV格式的优势和劣势：\n优势：\n简单易用：CSV格式是一种纯文本格式，易于创建、编辑和阅读，几乎所有的文本编辑器和电子表格软件都支持CSV格式。 跨平台兼容：CSV格式是一种通用的数据交换格式，可以在不同操作系统和软件之间进行数据导入和导出，具有良好的跨平台兼容性。 节省空间：相对于一些二进制格式，CSV格式通常较为简洁，可以节省存储空间。 可读性强：由于是文本格式，CSV文件内容可以直接被人类读取和理解，便于数据的可视化和检查。 劣势：\n不适合大规模数据：对于大规模数据集，CSV格式可能会占用较大的存储空间，并且在处理和查询大数据量时性能较低。 不支持复杂数据类型：CSV格式通常只支持基本的数据类型（如整数、浮点数、字符串等），不适合存储复杂的数据结构。 不支持元数据：CSV格式本身不包含元数据信息，如数据类型、列名等，需要额外的约定或处理来确保数据的正确解析。 容易出现格式问题：由于CSV格式是基于文本的，如果数据中包含逗号、换行符等特殊字符，可能会导致解析错误，需要额外的处理来处理这些情况。 综上所述，CSV格式适合简单的表格数据存储和交换，但在处理大规模、复杂数据或需要保留元数据的情况下可能不太适用。\n","description":"CSV（Comma-Separated Values）是一种常用的文本文件格式，用于存储表格数据。下面是CSV格式的优势和劣势：\n优势：\n简单易用：CSV格式是一种纯文本格式，易于创建、编辑和阅读，几乎所有的文本编辑器和电子表格软件都支持CSV格式。 跨平台兼容：CSV格式是一种通用的数据交换格式，可以在不同操作系统和软件之间进行数据导入和导出，具有良好的跨平台兼容性。 节省空间：相对于一些二进制格式，CSV格式通常较为简洁，可以节省存储空间。 可读性强：由于是文本格式，CSV文件内容可以直接被人类读取和理解，便于数据的可视化和检查。 劣势：\n不适合大规模数据：对于大规模数据集，CSV格式可能会占用较大的存储空间，并且在处理和查询大数据量时性能较低。 不支持复杂数据类型：CSV格式通常只支持基本的数据类型（如整数、浮点数、字符串等），不适合存储复杂的数据结构。 不支持元数据：CSV格式本身不包含元数据信息，如数据类型、列名等，需要额外的约定或处理来确保数据的正确解析。 容易出现格式问题：由于CSV格式是基于文本的，如果数据中包含逗号、换行符等特殊字符，可能会导致解析错误，需要额外的处理来处理这些情况。 综上所述，CSV格式适合简单的表格数据存储和交换，但在处理大规模、复杂数据或需要保留元数据的情况下可能不太适用。"},{"id":41,"href":"/hub/Data-Storage/format/Delta-Lake/","title":"Delta Lake","parent":"文件格式","content":" Delta Lake is an open-source storage framework that enables building a\nLakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.\nDelta Lake is essentially a metadata layer on top of Parquet.\nThe file layout looks like:\n![[Assets/delta-lake-file-format.png|500]]\nDelta Lake Official Documentation\nhttps://docs.delta.io/latest/index.html\nDelta Lake Advantages (over plain [[Apache Parquet|Parquet]]) ACID transactions with optimistic concurrency control. Efficient streaming I/O. Caching. Time travel. Data layout optimization, e.g. Z-ordering. Schema enforcement \u0026amp; evolution. UPSERT \u0026amp; MERGE statements. Audit logging. Delta Lake Disadvantages Same Parquet disadvantages. Maintenance processes are required to maintain its performance, e.g. OPTIMIZE. There is a learning curve when using advanced features, e.g. VACUUM. ","description":"Delta Lake is an open-source storage framework that enables building a\nLakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.\nDelta Lake is essentially a metadata layer on top of Parquet.\nThe file layout looks like:\n![[Assets/delta-lake-file-format.png|500]]\nDelta Lake Official Documentation\nhttps://docs.delta.io/latest/index.html\nDelta Lake Advantages (over plain [[Apache Parquet|Parquet]]) ACID transactions with optimistic concurrency control. Efficient streaming I/O."},{"id":42,"href":"/hub/Data-Ingestion/Flink-CDC%E7%BB%93%E5%90%88Debezium/","title":"Flink CDC结合Debezium","parent":"数据摄取","content":"Debezium is an open-source log-based change data capture tool used for streaming changes from your database. It works by reading the Transaction Log of your database to capture INSERT/UPDATE/DELETE events and propagates those events to a consumer (most commonly Apache Kafka).\nDebezium Advantages Captures changes in a way that has minimal impact on the source Changes can be captured with very low latency 一个利用Flink CDC结合Debezium 监控mysql变更操作，并将变更数据发送kafka的例子 要实现利用Flink CDC动态监控MySQL的test表的变更操作，并将变更数据发送到Kafka中，可以按照以下步骤进行：\n部署Apache Flink集群：确保已经搭建好Apache Flink集群，并准备好运行Flink Job。\n部署Debezium MySQL Connector：将Debezium MySQL Connector部署到Flink集群中，配置连接MySQL数据库的信息。\n编写Flink Job：编写一个Flink应用程序，通过Debezium Connector监听MySQL的test表的变更数据，并将变更数据发送到Kafka中。以下是一个简单的Flink Job示例：\nimport org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer; public class MySQLChangeDataCaptureJob { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 添加MySQL CDC Source DataStream\u0026lt;String\u0026gt; mysqlDataStream = env.addSource(new MySQLDebeziumSourceFunction(\u0026#34;mysql-connector-properties\u0026#34;)); // 将变更数据发送到Kafka Sink mysqlDataStream.addSink(new FlinkKafkaProducer\u0026lt;\u0026gt;(\u0026#34;kafka-topic\u0026#34;, new SimpleStringSchema(), kafkaProperties)); env.execute(\u0026#34;MySQL Change Data Capture Job\u0026#34;); } } 提交Flink Job：将编写好的Flink Job提交到Flink集群中运行，监控MySQL的test表的变更操作，并将变更数据实时发送到Kafka中。\n监控和验证：监控Flink Job的运行状态，确保CDC任务正常工作。可以通过Kafka Consumer验证数据是否正常发送到Kafka中。\n通过以上步骤，您可以成功利用Flink CDC实现动态监控MySQL的test表的变更操作，并将变更数据发送到Kafka中。请根据实际情况调整配置和代码，以满足您的需求。\n","description":"Debezium is an open-source log-based change data capture tool used for streaming changes from your database. It works by reading the Transaction Log of your database to capture INSERT/UPDATE/DELETE events and propagates those events to a consumer (most commonly Apache Kafka).\nDebezium Advantages Captures changes in a way that has minimal impact on the source Changes can be captured with very low latency 一个利用Flink CDC结合Debezium 监控mysql变更操作，并将变更数据发送kafka的例子 要实现利用Flink CDC动态监控MySQL的test表的变更操作，并将变更数据发送到Kafka中，可以按照以下步骤进行：\n部署Apache Flink集群：确保已经搭建好Apache Flink集群，并准备好运行Flink Job。"},{"id":43,"href":"/hub/Data-Governance/Testing-Your-Data-Pipeline/","title":"Testing Your Data Pipeline","parent":"数据治理","content":" Why Is Testing Important? Writing tests have a few key benefits for data engineers. They can help you sleep better at night knowing you\u0026rsquo;re less likely to get called into an emergency for a data issue. They can be used to help yourself and stakeholders get on the same page and understand the data you\u0026rsquo;re working with. And one of the most important reasons is they help build stakeholders\u0026rsquo; trust in the data. If stakeholders can\u0026rsquo;t trust the data they are using then they will likely stop using it and build their own solutions which may duplicate efforts or lead to more problems in the future.\nThe Different Types of Tests There are two main types of tests data engineers use to test their data pipelines: unit tests and data quality tests.\nUnit tests are used to test code, such as the functions and classes that make up a data pipeline. Unit tests can help ensure that your code is doing what it should be doing and that any changes you make don\u0026rsquo;t break existing functionality.\n[!example] Let\u0026rsquo;s say you have a function that you use to transform some data in Python. You would write a unit test with various inputs and expected outputs to ensure that it would always transform data in the same way even if you make a change.\nData quality tests, on the other hand, are used to check the accuracy of data flowing through your pipeline. These types of tests can help identify potential issues with incoming or outgoing data before they become a problem for downstream systems. Data quality tests also provide an extra layer of assurance that all parts of the system are working together correctly and producing reliable results.\n[!example] A data quality test for a data pipeline might involve comparing the results of a SQL query against the expected results. For example, if you have a dataset containing customer orders and you want to make sure that each order contains at least one item, your data quality test could check that there are no orders with zero items.\n[!tip] Typically, data quality tests are used with a Write-Audit-Publish pattern to make sure unexpected data doesn\u0026rsquo;t interfere with stakeholder-facing tools and diminish their trust in the data.\nCreating a Testing Plan Step 1: Identify your most important pipelines As a data engineer, you will constantly need to balance competing priorities which is why we first start by identifying pipelines to prioritize adding testing to. Here are a few questions you can ask yourself to identify these pipelines:\nIs it directly related to something that makes money or involves money (i.e. the Finance department)? Does it power a core feature of the business? Is the data used by high-level decision makers? These are the pipelines that should be prioritized and tested the most rigorously.\nStep 2: Understand the architecture and decide what/how to test Data pipelines can look very different depending on the company you\u0026rsquo;re at. At some companies, they are entirely no-code, at others everything is built in-house, but most are a combination of code mixed with SaaS or open-source tools.\nGenerally speaking, if you have any custom code that extracts, loads, or transforms data you should write unit tests that run every time you make a change to this code. Priority should be given to any business logic and custom connectors to ensure data isn\u0026rsquo;t corrupted early in the pipeline. Popular tools for this are unittest for Python and GitHub Actions to automate running the tests.\n[!tip] New to unit testing? Here is a short guide for beginners: https://www.dataquest.io/blog/unit-tests-python/\nNow that you\u0026rsquo;re testing your code, you also need to test your data quality to catch any \u0026ldquo;silent failures\u0026rdquo; and ensure the data arrives as expected. Silent failure in this case refers to when a data pipeline doesn\u0026rsquo;t produce an error but the data output is wrong or missing. These data quality tests are run as part of your data pipeline. Here are several popular [[Data Unit Test#Data Unit Testing Tools|data quality testing tools]].\nData quality tests should be triggered early on in the pipeline and test core business logic. You\u0026rsquo;ll need to work with the person/department who is the owner of the business logic to help you build appropriate tests. It would be a mistake to not talk to other departments and spend time writing tests that you later find out aren\u0026rsquo;t valuable.\nSome common data quality tests are:\nCheck the count of rows Check if a column contains nulls Check if the unique values in a column are expected values Check if a number value falls into an expected range Other kinds of tests may be more common depending on the use case. For example, if your pipeline feeds data into a machine learning model, then it\u0026rsquo;s common to check and make sure the data population hasn\u0026rsquo;t drifted.\nStep 3: Planning for failure When a unit test fails it usually fails before the code makes it to production. But what do you do if a data quality test fails?\nGoing back to the Write-Audit-Publish pattern mentioned earlier, you\u0026rsquo;ll want to write the batch of data that failed your tests somewhere so you can inspect it and figure out what happened. This also prevents bad data from being used downstream in important dashboards or data products. In a batch pipeline, this may be just a separate table in your data warehouse or it could be written to object storage like an S3 bucket. For streaming pipelines, you might send this bad data to a Dead-Letter Queue. Again, you should be able to inspect, fix, and then re-run the data through the pipeline from wherever you temporarily store the failed data.\nAnother common practice for data engineering is to create a runbook for each pipeline. A runbook is simply documentation that outlines the steps to take to resolve an issue or perform a task. You can read more about runbooks here.\nContinuous Monitoring and Alerting We won\u0026rsquo;t get into the details of monitoring and alerting because it\u0026rsquo;s a separate topic but it\u0026rsquo;s worth mentioning here that you will need to make sure that if tests fail, the appropriate person is alerted and can fix it. If you\u0026rsquo;re using a workflow orchestrator to run your pipeline then you can use the built-in email/alerting functionality to send an email if a data quality test fails.\n","description":"Why Is Testing Important? Writing tests have a few key benefits for data engineers. They can help you sleep better at night knowing you\u0026rsquo;re less likely to get called into an emergency for a data issue. They can be used to help yourself and stakeholders get on the same page and understand the data you\u0026rsquo;re working with. And one of the most important reasons is they help build stakeholders\u0026rsquo; trust in the data."},{"id":44,"href":"/hub/Data-Mining/","title":"数据挖掘","parent":"数据工程知识库","content":" 逻辑回归案例 K_Means案例 用户画像 ","description":" 逻辑回归案例 K_Means案例 用户画像 "},{"id":45,"href":"/hub/Data-Warehouse/%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/","title":"维度建模","parent":"数据仓库","content":"Developed by Ralph Kimball, dimensional modeling is a popular technique used to model data for analytics. At it\u0026rsquo;s core, dimensional modeling revolves around organizing data into two types of datasets: fact tables and dimension tables. Facts are usually comprised of numerical values that can be aggregated while dimensions hold descriptive attributes of entities/objects. A key tradeoff the dimensional model makes is it [[Denormalization|denormalizes]] data (increases data redundancy) in order to speed up queries.\nWithin dimensional modeling there are a few different schema design patterns: star schema (recommended in most cases), snowflake schema, and galaxy schema.\nDimensional Modeling Advantages Intuitive to understand. Good query performance for analytics. Keeps track of historical changes easily. Dimensional Modeling Disadvantages Can be complicated to query sometimes. ","description":"Developed by Ralph Kimball, dimensional modeling is a popular technique used to model data for analytics. At it\u0026rsquo;s core, dimensional modeling revolves around organizing data into two types of datasets: fact tables and dimension tables. Facts are usually comprised of numerical values that can be aggregated while dimensions hold descriptive attributes of entities/objects. A key tradeoff the dimensional model makes is it [[Denormalization|denormalizes]] data (increases data redundancy) in order to speed up queries."},{"id":46,"href":"/hub/Data-Visualization/Apache-Superset/","title":"Apache Superset","parent":"数据可视化","content":"Apache Superset是一款现代化、企业级的商业智能Web应用程序。它快速、轻量级、直观，并且拥有许多选项，使得用户可以轻松地探索和可视化他们的数据，从简单的饼图到高度详细的deck.gl地理空间图。\nSuperset 提供： 直观的界面，可视化数据集和创建交互式仪表板 各种漂亮的可视化效果，展示您的数据 无需编写代码的可视化构建器，提取和展示数据集 世界一流的 SQL IDE，用于准备数据进行可视化，包括丰富的元数据浏览器 轻量级的语义层，赋予数据分析师快速定义自定义维度和度量的能力 对大多数支持 SQL 的数据库的开箱即用支持 无缝、内存中的异步缓存和查询 可扩展的安全模型，允许配置非常复杂的规则，确定谁可以访问哪些产品功能和数据集 与主要身份验证后端集成（数据库、OpenID、LDAP、OAuth、REMOTE_USER 等） 添加自定义可视化插件的能力 用于程序化定制的 API 从头开始设计的云原生架构，可实现规模化 ","description":"Apache Superset是一款现代化、企业级的商业智能Web应用程序。它快速、轻量级、直观，并且拥有许多选项，使得用户可以轻松地探索和可视化他们的数据，从简单的饼图到高度详细的deck.gl地理空间图。\nSuperset 提供： 直观的界面，可视化数据集和创建交互式仪表板 各种漂亮的可视化效果，展示您的数据 无需编写代码的可视化构建器，提取和展示数据集 世界一流的 SQL IDE，用于准备数据进行可视化，包括丰富的元数据浏览器 轻量级的语义层，赋予数据分析师快速定义自定义维度和度量的能力 对大多数支持 SQL 的数据库的开箱即用支持 无缝、内存中的异步缓存和查询 可扩展的安全模型，允许配置非常复杂的规则，确定谁可以访问哪些产品功能和数据集 与主要身份验证后端集成（数据库、OpenID、LDAP、OAuth、REMOTE_USER 等） 添加自定义可视化插件的能力 用于程序化定制的 API 从头开始设计的云原生架构，可实现规模化 "},{"id":47,"href":"/hub/Data-Ingestion/data_build_tool/","title":"Data Build Tool","parent":"数据摄取","content":"data build tool 的特点通常包括：\n支持多种数据源：可以连接和处理不同类型的数据源，如数据库、文件、API 等。\n可视化操作：提供图形化界面或者类似 SQL 的语法，方便用户进行操作和配置。\n支持数据转换：可以进行数据清洗、转换、合并等操作，以满足不同的数据处理需求。\n调度和监控：可以设置任务调度，监控任务执行情况，保证数据处理的准确性和及时性。\n利用 data build tool 进行 ETL 过程通常包括以下步骤：\nExtract（提取）：从不同的数据源中提取需要的数据，可以是数据库表、文件、API 接口等。\nTransform（转换）：对提取的数据进行清洗、转换、合并等操作，使其符合目标数据仓库的要求。\nLoad（加载）：将经过转换的数据加载到目标数据仓库中，可以是数据库表、数据湖等存储介质。\n案例：利用dbt 将 本地的mysql数据进行过滤掉一些id为敏感数据的记录，然后导入到云端的gcp的bigquery中，这个例子用dbt该如何实现呢？\n要利用 dbt 将本地的 MySQL 数据进行过滤并导入到 GCP 的 BigQuery 中，可以按照以下步骤进行：\n创建 dbt 项目：首先在本地初始化一个 dbt 项目，可以使用以下命令： dbt init my_project 配置数据源：在 dbt 项目目录下的 profiles.yml 文件中配置本地 MySQL 数据源和 GCP BigQuery 数据源的连接信息。\n编写模型：在 dbt 项目中创建一个模型（model），用于过滤敏感数据。在 models 目录下创建一个 .sql 文件，编写 SQL 查询语句，例如：\n-- models/filter_sensitive_data.sql with filtered_data as ( select * from {{ ref(\u0026#39;source_table\u0026#39;) }} where id not in (\u0026#39;sensitive_id_1\u0026#39;, \u0026#39;sensitive_id_2\u0026#39;) ) select * from filtered_data 配置模型依赖：在 schema.yml 文件中定义模型的依赖关系，指定数据源表和过滤模型之间的关系。\n运行 dbt：在终端中运行以下命令，执行 dbt 项目并生成目标数据：\ndbt run 导入到 BigQuery：将生成的数据集导入到 GCP 的 BigQuery 中，可以使用 BigQuery 的 Web 界面或者命令行工具进行导入操作。 通过以上步骤，可以利用 dbt 对本地 MySQL 数据进行过滤处理，并将处理后的数据导入到 GCP 的 BigQuery 中，实现数据迁移和处理的自动化流程。\n如果使用dbt cloud，可以无需步骤6 ，在 dbt 中，可以使用 dbt Cloud 或者 dbt Cloud CLI 来直接将生成的数据集导入到 BigQuery 中，而无需手动操作。 ","description":"data build tool 的特点通常包括：\n支持多种数据源：可以连接和处理不同类型的数据源，如数据库、文件、API 等。\n可视化操作：提供图形化界面或者类似 SQL 的语法，方便用户进行操作和配置。\n支持数据转换：可以进行数据清洗、转换、合并等操作，以满足不同的数据处理需求。\n调度和监控：可以设置任务调度，监控任务执行情况，保证数据处理的准确性和及时性。\n利用 data build tool 进行 ETL 过程通常包括以下步骤：\nExtract（提取）：从不同的数据源中提取需要的数据，可以是数据库表、文件、API 接口等。\nTransform（转换）：对提取的数据进行清洗、转换、合并等操作，使其符合目标数据仓库的要求。\nLoad（加载）：将经过转换的数据加载到目标数据仓库中，可以是数据库表、数据湖等存储介质。\n案例：利用dbt 将 本地的mysql数据进行过滤掉一些id为敏感数据的记录，然后导入到云端的gcp的bigquery中，这个例子用dbt该如何实现呢？\n要利用 dbt 将本地的 MySQL 数据进行过滤并导入到 GCP 的 BigQuery 中，可以按照以下步骤进行：\n创建 dbt 项目：首先在本地初始化一个 dbt 项目，可以使用以下命令： dbt init my_project 配置数据源：在 dbt 项目目录下的 profiles.yml 文件中配置本地 MySQL 数据源和 GCP BigQuery 数据源的连接信息。\n编写模型：在 dbt 项目中创建一个模型（model），用于过滤敏感数据。在 models 目录下创建一个 .sql 文件，编写 SQL 查询语句，例如：\n-- models/filter_sensitive_data.sql with filtered_data as ( select * from {{ ref(\u0026#39;source_table\u0026#39;) }} where id not in (\u0026#39;sensitive_id_1\u0026#39;, \u0026#39;sensitive_id_2\u0026#39;) ) select * from filtered_data 配置模型依赖：在 schema."},{"id":48,"href":"/hub/Data-Governance/Guides/","title":"Guides","parent":"数据治理","content":"[[Getting Started With Data Engineering]]\n","description":"[[Getting Started With Data Engineering]]"},{"id":49,"href":"/hub/Data-Storage/format/Protocol-Buffers/","title":"Protocol Buffers","parent":"文件格式","content":"Protocol buffers provide a serialization format for packets of typed, structured data that are up to a few megabytes in size. The format is suitable for both ephemeral network traffic and long-term data storage. Protocol buffers can be extended with new information without invalidating existing data or requiring code to be updated. They are the most commonly-used data format at Google.\nExtension: .proto\nProtocol Buffers Official Documentation https://developers.google.com/protocol-buffers/docs/overview\nProtocol Buffers Advantages Compact data storage Fast parsing Available in several programming languages Protocol Buffers Disadvantages Not suitable for data larger than a few megabytes Messages are not compressed. You can compress them but sometimes special-purpose compression algorithms (JPEG, PNG) will produce more optimal results. Not optimal for scientific and engineering use cases involving multi-dimensional arrays of floating point numbers. ","description":"Protocol buffers provide a serialization format for packets of typed, structured data that are up to a few megabytes in size. The format is suitable for both ephemeral network traffic and long-term data storage. Protocol buffers can be extended with new information without invalidating existing data or requiring code to be updated. They are the most commonly-used data format at Google.\nExtension: .proto\nProtocol Buffers Official Documentation https://developers.google.com/protocol-buffers/docs/overview\nProtocol Buffers Advantages Compact data storage Fast parsing Available in several programming languages Protocol Buffers Disadvantages Not suitable for data larger than a few megabytes Messages are not compressed."},{"id":50,"href":"/hub/Data-Visualization/","title":"数据可视化","parent":"数据工程知识库","content":" Power BI Qlikview Kibana Grafana Prometheus Prometheus和Grafana的集成 Apache Superset ","description":" Power BI Qlikview Kibana Grafana Prometheus Prometheus和Grafana的集成 Apache Superset "},{"id":51,"href":"/hub/Data-Governance/Data-Governance-Guide/","title":"Data Governance Guide","parent":"数据治理","content":"The emergence of [[data governance]] directly correlated with massively increased quantities of data that is now considered standard. Only a few decades ago, many companies managed a quantity of data that was reasonable to organize by a database administrator. But with the increased popularity of using different data sources and streaming data as well as the sheer quantity of data that is available nowadays, it is dangerous to leave data to a single person and expect them to properly handle everything.\nData governance is a philosophy of data management that focuses on establishing responsibility for data throughout the complete lifecycle of data. The important part here is that data governance is unfortunately not a \u0026ldquo;set it and forget it\u0026rdquo; system for managing data, at least during the beginning stages of implementation. Contrary to most of the philosophical cores of data engineering, completely automating data governance is not recommended. A quality data governance framework involves a system of rules, processes, procedures, and enforcement strategies to ensure that data is properly accounted for.\nDifferent kinds of data may require fine-tuned guidelines. For example, how one would manage HIPAA compliant data is very different than how one would handle a spreadsheet keeping track of lunch expenses for a team bowling party. Additionally, different teams and sub-organizations may have different cultures that necessitate the tweaking of any framework to ensure a customized fit. These challenges necessitate some manual guidance and buy-in from policymakers at the top of an organization. Without actual enforcement, data governance tends to be difficult to implement due to the natural high friction it tends to create.\nData use and management are particularly difficult when coordinating within and between different units to allow for better delivery on the business side and more accountability on the security side. There are typically multiple steps required to implement data governance effectively. A common five-pronged approach involves the following stages: inventory/mapping, planning, education, implementation, and enforcement.\nWhenever data governance is mentioned, a key piece that is misunderstood is what engaging in data governance entails. Even logging onto a computer or sending a work email is technically an example of data governance. The key is identifying what the end state of data governance looks like for a particular organization and then setting a formal practice for managing organizational information with consistency.\nInventory When compiling an inventory of assets, there are two pieces to keep in mind. A standard data inventory tends to be a completely documented repository of information resources that are owned by an organization, including the associated metadata. A data inventory is focused on understanding data and identifying risks that are posed due to any missing/broken dependencies or gaps. When tailoring an inventory for data governance, it can be helpful to have explicit data mappings associated with databases and data owners/subject matter experts. This allows for an open communication line between policymakers and gatekeepers of important data sets.\nThere are many use cases where data governance is being embraced by organizations of all different kinds. Larger companies often have different teams with completely different views on data. The analytics team might view it as a secret weapon while the security team might view it as a liability. This can lead to conflicts when it comes to how to best manage and assign ownership of data. Government institutions on the other hand traditionally deal with a high level of employee turnover, leading to entire databases and key institutional knowledge being lost in the offboarding process.\nDetailing, documenting, and accounting for risks and opportunities is a key piece of data inventory when preparing it for data governance.\nPlanning Once the data assets and personnel are identified for an organization, building trust with database owners, subject matter experts, and policymakers within the organization is key. Since this is the planning stage it is difficult to actually enforce data governance, and as such getting buy-in at this stage is crucial.\nA comprehensive plan requires interviews and meetings with key stakeholders and database owners/subject matter experts to understand what the needs and wants of all the sub-organizations require and how to reconcile these demands.\nSpecific processes that need to be ironed out should be included in this phase as well. For example, for a State level organization, data trusts tend to be very important. IT teams like to consolidate databases and implement uniform security.\nEducation In order for people in organizations to fully embrace data governance, a training and education program is necessary. Typically classroom-style lectures tend to not be effective. Instead, convincing business owners how data governance can bring value to the company and security admins that data governance increases the security and stability of the work.\nImplementation Actually implementing the process tends to be more of a practice of execution. As long as there\u0026rsquo;s been a quality plan and strategy produced and a training program that has started tackling some of the silos that have been built, progress will eventually be made. The biggest indicator of success for the implementation stage will be how interested and engaged key stakeholders are.\nThe main processes for data governance are the query engine, data catalog, and policy engine. Policymakers create a policy that is applied to the data catalog which is consumed through the query engine.\nEnforcement Enforcement is relatively simple if the infrastructure for the implementation is thorough. By applying the correct policies that restrict user roles, minimal enforcement is necessary. The main enforcement that is required during the process is making sure subject matter experts, policymakers, and stakeholders are all engaged. This is largely the job of Chief policymakers to create a plan of accountability.\nSummary Data governance is the practice of making data more of an asset rather than a liability. The ability to know and be able to search through all data assets an organization owns is transformative.\nData governance in modern architecture is centered around policymakers, data consumers, and subject matter experts. Subject matter experts own data resources, which must be mapped and consolidated under a data catalog with the associated metadata.\n","description":"The emergence of [[data governance]] directly correlated with massively increased quantities of data that is now considered standard. Only a few decades ago, many companies managed a quantity of data that was reasonable to organize by a database administrator. But with the increased popularity of using different data sources and streaming data as well as the sheer quantity of data that is available nowadays, it is dangerous to leave data to a single person and expect them to properly handle everything."},{"id":52,"href":"/hub/Data-Warehouse/%E4%BA%8B%E5%AE%9E%E8%A1%A8%E8%AE%BE%E8%AE%A1%E8%A7%84%E5%88%99/","title":"事实表的设计","parent":"数据仓库","content":"在数据仓库设计中，事实表是存储业务度量（事实）数据的核心表，用于支持分析和报告。以下是设计事实表时应该遵循的规范和最佳实践：\n清晰定义度量：明确定义每个事实表中包含的业务度量，确保每个度量都具有清晰的含义和计算方法。\n选择合适的粒度：确定事实表的粒度，即每条记录代表的业务事件的时间和范围。粒度应该能够满足用户的分析需求，同时避免数据冗余。\n遵循命名规范：为事实表、字段、约束等命名制定规范，使命名具有一致性、可读性和易于理解。\n设计合适的主键：选择合适的主键字段，确保每条记录都有唯一标识符，并且能够支持数据的快速检索和关联。\n避免冗余数据：避免在事实表中存储冗余数据，保持数据的一致性和准确性。\n设计适当的索引：为事实表设计合适的索引，以提高查询性能和数据访问效率。\n考虑数据质量：确保事实表中的数据质量高，包括准确性、完整性、一致性和及时性。\n考虑历史数据：如果业务需要追踪历史数据变化，可以考虑在事实表中设计支持历史数据的字段或采用缓慢变化维度技术。\n与维度表关联：确保事实表与维度表之间建立正确的关联关系，以支持多维分析和数据关联查询。\n性能优化：根据实际需求和数据量，设计事实表的物理存储结构和索引，以提高查询性能和数据加载效率。\n遵循上述规范和最佳实践可以确保事实表的设计符合业务需求，数据结构合理，同时支持数据分析和报告的高效进行。\n","description":"在数据仓库设计中，事实表是存储业务度量（事实）数据的核心表，用于支持分析和报告。以下是设计事实表时应该遵循的规范和最佳实践：\n清晰定义度量：明确定义每个事实表中包含的业务度量，确保每个度量都具有清晰的含义和计算方法。\n选择合适的粒度：确定事实表的粒度，即每条记录代表的业务事件的时间和范围。粒度应该能够满足用户的分析需求，同时避免数据冗余。\n遵循命名规范：为事实表、字段、约束等命名制定规范，使命名具有一致性、可读性和易于理解。\n设计合适的主键：选择合适的主键字段，确保每条记录都有唯一标识符，并且能够支持数据的快速检索和关联。\n避免冗余数据：避免在事实表中存储冗余数据，保持数据的一致性和准确性。\n设计适当的索引：为事实表设计合适的索引，以提高查询性能和数据访问效率。\n考虑数据质量：确保事实表中的数据质量高，包括准确性、完整性、一致性和及时性。\n考虑历史数据：如果业务需要追踪历史数据变化，可以考虑在事实表中设计支持历史数据的字段或采用缓慢变化维度技术。\n与维度表关联：确保事实表与维度表之间建立正确的关联关系，以支持多维分析和数据关联查询。\n性能优化：根据实际需求和数据量，设计事实表的物理存储结构和索引，以提高查询性能和数据加载效率。\n遵循上述规范和最佳实践可以确保事实表的设计符合业务需求，数据结构合理，同时支持数据分析和报告的高效进行。"},{"id":53,"href":"/hub/Data-Pipeline/","title":"数据管道","parent":"数据工程知识库","content":" Airflow 问题 talend 问题 perfect 问题 Apache Airflow ","description":" Airflow 问题 talend 问题 perfect 问题 Apache Airflow "},{"id":54,"href":"/hub/Data-Governance/Getting-Started-With-Data-Engineering/","title":"Getting Started With Data Engineering","parent":"数据治理","content":" Overview This guide is intended for people who are new to Data Engineering and aren\u0026rsquo;t sure where to start. The purpose of this guide isn\u0026rsquo;t to cover every single thing you need to know, but rather give you the working knowledge and the intuition to find answers later on in your Data Engineering journey.\nStep 1: Read the FAQ ![[FAQ]]\nStep 2: Learn Data Engineering core concepts ![[Concepts#Core Concepts]]\nStep 3: Learn the core tools ![[Tools#Core Tools]]\n","description":"Overview This guide is intended for people who are new to Data Engineering and aren\u0026rsquo;t sure where to start. The purpose of this guide isn\u0026rsquo;t to cover every single thing you need to know, but rather give you the working knowledge and the intuition to find answers later on in your Data Engineering journey.\nStep 1: Read the FAQ ![[FAQ]]\nStep 2: Learn Data Engineering core concepts ![[Concepts#Core Concepts]]"},{"id":55,"href":"/hub/Data-Governance/","title":"数据治理","parent":"数据工程知识库","content":" 数据合规 数据传输 数据加密 数据容灾 数据监控 数据血缘 数据质量 Soda Great Expectations Monte Carlo Deequ Choosing your optimal messaging service Cost Optimization in the Cloud SQL Guide Testing Your Data Pipeline Guides Data Governance Guide Getting Started With Data Engineering Data Pipeline Best Practices ","description":" 数据合规 数据传输 数据加密 数据容灾 数据监控 数据血缘 数据质量 Soda Great Expectations Monte Carlo Deequ Choosing your optimal messaging service Cost Optimization in the Cloud SQL Guide Testing Your Data Pipeline Guides Data Governance Guide Getting Started With Data Engineering Data Pipeline Best Practices "},{"id":56,"href":"/hub/Data-on-Cloud/Aliyun/","title":"Aliyun","parent":"数据上云","content":"","description":""},{"id":57,"href":"/hub/Data-Governance/Data-Pipeline-Best-Practices/","title":"Data Pipeline Best Practices","parent":"数据治理","content":" Overview A best practice guide for data pipelines compiled from data engineers in the community. Follow this guide to help you build more robust, scalable, and more performant data pipelines. These best practices are in no particular order but we\u0026rsquo;ve done our best to categorize them.\nGeneral Best Practices Verify your assumptions about the data. Document your pipelines. Add proper logging to your pipelines to make debugging easier. Use code and version control (git) for pipelines. Make your pipelines [[Idempotence|idempotent]]. Understand the tradeoff between fast data and accurate data. Data quality takes time. Have separate environments for development, staging, and production ideally. If you have separate environments, color code them and label them clearly. Use templates whenever possible. If you\u0026rsquo;re writing custom code, try to make it generic/modular. Avoid ingesting data from manually created data sources (e.g. Google Sheet/Excel file). If you have to do so, require strict protections on what can change at the source. Design Use Docker for dependency management. Prepare for intermittent or temporary failures. Use exponential back-off and retry strategies. Use CI to deploy pipelines to staging and production environments. Set up alerting on failures and pipeline run times. Surface all parameters and use configuration files/environment variables to change pipeline behavior vs updating the code. Optimization Don\u0026rsquo;t let file sizes become too large or too small. Large files (\u0026gt;1 GB) can require more resources and many small files can create a large overhead to process. ~250 MB is a good size that allows for better parallel processing. Use the [[Claim Check Pattern|claim check pattern]] to pass large amounts of data between tasks in your pipeline. Security Save credentials in a secrets manager and access them in your pipeline programmatically. Ideally, have secrets rotated automatically. Avoid logging any sensitive information like credentials or PII data. Testing Test data quality with [[Data Unit Test|data unit tests]] regularly. Test data pipeline code with regular unit tests. Set up a local environment to test pipelines locally first. (see Docker above) Re-define pipeline failures. If pipeline fails x times but data is still delivered on time then it was successful. ","description":"Overview A best practice guide for data pipelines compiled from data engineers in the community. Follow this guide to help you build more robust, scalable, and more performant data pipelines. These best practices are in no particular order but we\u0026rsquo;ve done our best to categorize them.\nGeneral Best Practices Verify your assumptions about the data. Document your pipelines. Add proper logging to your pipelines to make debugging easier. Use code and version control (git) for pipelines."},{"id":58,"href":"/hub/Data-Mining/K_Means/","title":"K_Means案例","parent":"数据挖掘","content":" K均值聚类算法实际案例 假设我们有一个数据集，其中包含300个数据点，这些数据点属于4个不同的类别。我们想要使用K均值算法将这些数据点分成4个簇。以下是实现的步骤：\n生成合成数据：我们首先生成一个合成数据集，其中包含4个簇。这可以通过make_blobs函数来实现。\n初始化K均值模型：我们使用KMeans类来初始化一个K均值模型，指定簇的数量为4。\n拟合模型：将数据拟合到K均值模型中。\n获取簇中心和标签：我们可以获取每个簇的中心点和数据点的标签。\n绘制结果：最后，我们绘制数据点和簇中心，以查看聚类效果。\n以下是Python代码实现：\nimport numpy as np import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.datasets import make_blobs # 生成合成数据 X, _ = make_blobs(n_samples=300, centers=4, random_state=42) # 初始化KMeans模型（指定4个簇） kmeans = KMeans(n_clusters=4, random_state=42) # 拟合模型 kmeans.fit(X) # 获取簇中心和标签 cluster_centers = kmeans.cluster_centers_ labels = kmeans.labels_ # 绘制数据点和簇中心 plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\u0026#39;viridis\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c=\u0026#39;red\u0026#39;, marker=\u0026#39;x\u0026#39;, s=200, label=\u0026#39;Cluster Centers\u0026#39;) plt.title(\u0026#34;KMeans Clustering\u0026#34;) plt.xlabel(\u0026#34;Feature 1\u0026#34;) plt.ylabel(\u0026#34;Feature 2\u0026#34;) plt.legend() plt.show() 这段代码将生成一个散点图，其中数据点被分成4个簇，并且红色的“x”表示每个簇的中心点。\n","description":"K均值聚类算法实际案例 假设我们有一个数据集，其中包含300个数据点，这些数据点属于4个不同的类别。我们想要使用K均值算法将这些数据点分成4个簇。以下是实现的步骤：\n生成合成数据：我们首先生成一个合成数据集，其中包含4个簇。这可以通过make_blobs函数来实现。\n初始化K均值模型：我们使用KMeans类来初始化一个K均值模型，指定簇的数量为4。\n拟合模型：将数据拟合到K均值模型中。\n获取簇中心和标签：我们可以获取每个簇的中心点和数据点的标签。\n绘制结果：最后，我们绘制数据点和簇中心，以查看聚类效果。\n以下是Python代码实现：\nimport numpy as np import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.datasets import make_blobs # 生成合成数据 X, _ = make_blobs(n_samples=300, centers=4, random_state=42) # 初始化KMeans模型（指定4个簇） kmeans = KMeans(n_clusters=4, random_state=42) # 拟合模型 kmeans.fit(X) # 获取簇中心和标签 cluster_centers = kmeans.cluster_centers_ labels = kmeans.labels_ # 绘制数据点和簇中心 plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\u0026#39;viridis\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c=\u0026#39;red\u0026#39;, marker=\u0026#39;x\u0026#39;, s=200, label=\u0026#39;Cluster Centers\u0026#39;) plt.title(\u0026#34;KMeans Clustering\u0026#34;) plt."},{"id":59,"href":"/hub/Data-Process/spark-join%E7%9A%84%E7%B1%BB%E5%9E%8B/","title":"Spark Join的类型和使用","parent":"数据处理","content":" spark join 的三种类型 在Spark中，有三种常见的连接方式：Shuffle Hash Join、Broadcast Hash Join和Sort Merge Join，它们各有特点，适合不同的场合：\nShuffle Hash Join（Shuffle哈希连接）：\n原理：Shuffle Hash Join会对两个数据集的连接键进行哈希分区，并将数据重新分布到不同的Executor上，然后进行连接操作。 适合场合：适合用于连接大规模数据集，数据分布较均匀的情况。由于需要进行Shuffle操作，适合用于连接大数据集时。 Broadcast Hash Join（广播哈希连接）：\n原理：Broadcast Hash Join会将一个数据集小的数据广播到所有Executor上，然后与另一个数据集进行连接操作。 适合场合：适合用于连接一个小数据集和一个大数据集，或者在一个数据集已经被缓存到内存中的情况下。适合用于连接小数据集时。 Sort Merge Join（排序合并连接）：\n原理：Sort Merge Join会对两个数据集的连接键进行排序，然后按顺序合并这两个有序数据集来进行连接操作。 适合场合：适合用于连接大数据集并且连接键有序的情况。当数据集已经按连接键有序时，Sort Merge Join可以避免Shuffle操作，适合用于连接有序数据集时。 在实际使用中，根据数据集的大小、数据分布情况、连接键的数据倾斜程度等因素来选择合适的连接方式：\n如果连接的是大规模数据集，且数据分布较均匀，可以考虑使用Shuffle Hash Join。 如果连接的是一个小数据集和一个大数据集，或者已经缓存到内存中的数据集，可以考虑使用Broadcast Hash Join。 如果连接的数据集已经按连接键有序，可以考虑使用Sort Merge Join。 综合考虑数据规模、数据分布、内存资源等因素，选择合适的连接方式可以提高Spark作业的性能和效率。\nboardcast hash join 的例子 假设有两个DataFrame，一个包含用户ID和用户名，另一个包含用户ID和用户所在城市，我们可以使用Broadcast Hash Join来将这两个DataFrame连接起来。在这个例子中，我们将用户ID作为连接键。\n首先，创建两个DataFrame：\nfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#34;BroadcastHashJoinExample\u0026#34;).getOrCreate() # 创建第一个DataFrame：用户ID和用户名 data1 = [(1, \u0026#34;Alice\u0026#34;), (2, \u0026#34;Bob\u0026#34;), (3, \u0026#34;Charlie\u0026#34;)] df1 = spark.createDataFrame(data1, [\u0026#34;user_id\u0026#34;, \u0026#34;username\u0026#34;]) # 创建第二个DataFrame：用户ID和用户所在城市 data2 = [(1, \u0026#34;New York\u0026#34;), (2, \u0026#34;San Francisco\u0026#34;), (3, \u0026#34;Los Angeles\u0026#34;)] df2 = spark.createDataFrame(data2, [\u0026#34;user_id\u0026#34;, \u0026#34;city\u0026#34;]) 然后，使用Broadcast Hash Join将这两个DataFrame连接起来：\nfrom pyspark.sql.functions import broadcast # 使用Broadcast Hash Join连接两个DataFrame joined_df = df1.join(broadcast(df2), \u0026#34;user_id\u0026#34;) # 显示连接后的DataFrame joined_df.show() 在这个例子中，我们使用broadcast(df2)将第二个DataFrame广播到所有Executor上，然后通过df1.join()进行Broadcast Hash Join操作。这样可以避免将较小的DataFrame进行Shuffle操作，提高连接操作的性能。\n通过使用Broadcast Hash Join，可以有效地处理连接一个小数据集和一个大数据集的情况，提高连接操作的效率。\n","description":"spark join 的三种类型 在Spark中，有三种常见的连接方式：Shuffle Hash Join、Broadcast Hash Join和Sort Merge Join，它们各有特点，适合不同的场合：\nShuffle Hash Join（Shuffle哈希连接）：\n原理：Shuffle Hash Join会对两个数据集的连接键进行哈希分区，并将数据重新分布到不同的Executor上，然后进行连接操作。 适合场合：适合用于连接大规模数据集，数据分布较均匀的情况。由于需要进行Shuffle操作，适合用于连接大数据集时。 Broadcast Hash Join（广播哈希连接）：\n原理：Broadcast Hash Join会将一个数据集小的数据广播到所有Executor上，然后与另一个数据集进行连接操作。 适合场合：适合用于连接一个小数据集和一个大数据集，或者在一个数据集已经被缓存到内存中的情况下。适合用于连接小数据集时。 Sort Merge Join（排序合并连接）：\n原理：Sort Merge Join会对两个数据集的连接键进行排序，然后按顺序合并这两个有序数据集来进行连接操作。 适合场合：适合用于连接大数据集并且连接键有序的情况。当数据集已经按连接键有序时，Sort Merge Join可以避免Shuffle操作，适合用于连接有序数据集时。 在实际使用中，根据数据集的大小、数据分布情况、连接键的数据倾斜程度等因素来选择合适的连接方式：\n如果连接的是大规模数据集，且数据分布较均匀，可以考虑使用Shuffle Hash Join。 如果连接的是一个小数据集和一个大数据集，或者已经缓存到内存中的数据集，可以考虑使用Broadcast Hash Join。 如果连接的数据集已经按连接键有序，可以考虑使用Sort Merge Join。 综合考虑数据规模、数据分布、内存资源等因素，选择合适的连接方式可以提高Spark作业的性能和效率。\nboardcast hash join 的例子 假设有两个DataFrame，一个包含用户ID和用户名，另一个包含用户ID和用户所在城市，我们可以使用Broadcast Hash Join来将这两个DataFrame连接起来。在这个例子中，我们将用户ID作为连接键。\n首先，创建两个DataFrame：\nfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#34;BroadcastHashJoinExample\u0026#34;).getOrCreate() # 创建第一个DataFrame：用户ID和用户名 data1 = [(1, \u0026#34;Alice\u0026#34;), (2, \u0026#34;Bob\u0026#34;), (3, \u0026#34;Charlie\u0026#34;)] df1 = spark."},{"id":60,"href":"/hub/Data-Process/Spark%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9E%B6%E6%9E%84/","title":"spark概念和架构","parent":"数据处理","content":"abc\n","description":"abc"},{"id":61,"href":"/hub/Data-Warehouse/","title":"数据仓库","parent":"数据工程知识库","content":" 什么是数据仓库 数据模型 数仓建模的方式 数据仓库分层设计 维度建模 事实表的设计 缓慢变化维 电商数据仓库设计 实时数仓架构 数据湖 增量更新 ","description":" 什么是数据仓库 数据模型 数仓建模的方式 数据仓库分层设计 维度建模 事实表的设计 缓慢变化维 电商数据仓库设计 实时数仓架构 数据湖 增量更新 "},{"id":62,"href":"/hub/Data-on-Cloud/AWS/AWS-dynanoDB/","title":"AWS DynamoDB","parent":"AWS","content":" Overview This tutorial will show you how to export a large MongoDB collection as a JSON file and upload it to AWS S3 using the AWS CLI. The AWS CLI is used for larger files because there is a file size limit when uploading files to S3 via the AWS management console.\nRequirements Install mongoexport Install the AWS CLI and configure it 1. Export the collection using mongoexport In the example below, replace the uri connection string with your own.\nmongoexport --uri=\u0026#34;mongodb+srv://username:password@example-mongodb.example.mongodb.net/\u0026#34; --db=example --collection=example --out=example.json You can also put your credentials into a separate configuration file and reference it.\nmongoexport --config=config.yaml --db=example --collection=example --type=json --out=example.json 2. Generate an md5 checksum After we have exported our file from MongoDB, we want to generate a hash which uniquely identifies the data. It will be used in the next step to verify that the data was uploaded correctly in S3.\nopenssl md5 -binary example.json | base64 The generated md5 hash should look similar to: t8oeOvMA7tKvxzZoEcYawQ==\n3. Initiate copy to S3 Using the S3 copy command below, specify your file, the destination you want to send it to in S3, and the md5 hash generated previously.\naws s3 cp example.json s3://example-bucket/example.json --metadata md5=\u0026#34;example\u0026#34; ","description":"Overview This tutorial will show you how to export a large MongoDB collection as a JSON file and upload it to AWS S3 using the AWS CLI. The AWS CLI is used for larger files because there is a file size limit when uploading files to S3 via the AWS management console.\nRequirements Install mongoexport Install the AWS CLI and configure it 1. Export the collection using mongoexport In the example below, replace the uri connection string with your own."},{"id":63,"href":"/hub/Data-Process/spark-%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"spark面试题","parent":"数据处理","content":" spark面试题 如何设计和实现一个具有高可靠性和容错性的Spark作业？ 在设计和实现具有高可靠性和容错性的Spark作业时，我通常会采取以下策略：\n设置合适的检查点（Checkpoint）：\n在作业中设置合适的检查点，以便在作业执行过程中将中间结果持久化到可靠的存储介质，以便在发生故障时能够快速恢复作业状态。 处理异常情况：\n在作业中加入异常处理逻辑，包括捕获和处理异常、记录错误日志、重试失败的任务等，以确保作业在遇到异常情况时能够正确处理。 数据丢失处理：\n使用合适的数据存储介质，如HDFS或云存储服务，以确保数据持久性。在作业中使用RDD持久化或将数据写入可靠的数据源，以防止数据丢失。 任务级别容错：\n在作业中使用Spark的容错机制，如RDD的血统（lineage）和任务重试机制，以确保任务在失败时能够重新执行，并保持数据一致性。 监控和日志：\n添加监控和日志功能，监控作业的执行状态和性能指标，记录作业执行的详细日志，以便在需要时进行故障排查和性能优化。 通过以上策略，我能够确保在实际项目中设计和实现具有高可靠性和容错性的Spark作业，以保证作业能够稳定运行并处理各种异常情况。\n","description":"spark面试题 如何设计和实现一个具有高可靠性和容错性的Spark作业？ 在设计和实现具有高可靠性和容错性的Spark作业时，我通常会采取以下策略：\n设置合适的检查点（Checkpoint）：\n在作业中设置合适的检查点，以便在作业执行过程中将中间结果持久化到可靠的存储介质，以便在发生故障时能够快速恢复作业状态。 处理异常情况：\n在作业中加入异常处理逻辑，包括捕获和处理异常、记录错误日志、重试失败的任务等，以确保作业在遇到异常情况时能够正确处理。 数据丢失处理：\n使用合适的数据存储介质，如HDFS或云存储服务，以确保数据持久性。在作业中使用RDD持久化或将数据写入可靠的数据源，以防止数据丢失。 任务级别容错：\n在作业中使用Spark的容错机制，如RDD的血统（lineage）和任务重试机制，以确保任务在失败时能够重新执行，并保持数据一致性。 监控和日志：\n添加监控和日志功能，监控作业的执行状态和性能指标，记录作业执行的详细日志，以便在需要时进行故障排查和性能优化。 通过以上策略，我能够确保在实际项目中设计和实现具有高可靠性和容错性的Spark作业，以保证作业能够稳定运行并处理各种异常情况。"},{"id":64,"href":"/hub/Data-on-Cloud/","title":"数据上云","parent":"数据工程知识库","content":" AWS AWS DynamoDB Amazon MSK Benthos Amazon Web Services Azure Microsoft Azure Google Cloud Platform Google Cloud Platform Aliyun ","description":" AWS AWS DynamoDB Amazon MSK Benthos Amazon Web Services Azure Microsoft Azure Google Cloud Platform Google Cloud Platform Aliyun "},{"id":65,"href":"/hub/Data-Warehouse/%E7%BC%93%E6%85%A2%E5%8F%98%E5%8C%96%E7%BB%B4%E5%BA%A6/","title":"缓慢变化维","parent":"数据仓库","content":"缓慢变化维（Slowly Changing Dimensions，SCDs）是数据仓库中用来处理维度数据变化的一种技术。在实际业务中，维度数据的属性可能会随着时间的推移而发生变化，例如产品名称、客户地址、员工职位等。SCDs旨在有效地处理这种维度数据变化的情况。\n常见的缓慢变化维度类型包括：\n类型1：覆盖型（Type 1 - Overwrite）：在这种情况下，新数据直接覆盖旧数据，不保留历史记录。这种方式简单直接，但无法追踪数据变化历史。\n类型2：增加型（Type 2 - Additive）：在这种情况下，新数据会被插入到维度表中，同时保留历史记录。每次维度数据变化都会生成一个新的记录，每条记录都有一个唯一标识符和有效时间范围。这样可以追踪数据变化历史，但会增加表的大小。\n类型3：历史型（Type 3 - Historical）：在这种情况下，维度表中只保留一部分历史数据，通常是最近的一次变化。旧数据会被更新，同时保留一个或多个历史属性字段，用于存储之前的值。这种方式可以平衡数据存储和查询性能，但无法完整追踪数据变化历史。\n根据具体业务需求和数据特点，可以选择适合的缓慢变化维度类型来处理维度数据的变化。SCDs在数据仓库中起到重要作用，确保数据的准确性和完整性，同时支持对历史数据的分析和追踪。\n","description":"缓慢变化维（Slowly Changing Dimensions，SCDs）是数据仓库中用来处理维度数据变化的一种技术。在实际业务中，维度数据的属性可能会随着时间的推移而发生变化，例如产品名称、客户地址、员工职位等。SCDs旨在有效地处理这种维度数据变化的情况。\n常见的缓慢变化维度类型包括：\n类型1：覆盖型（Type 1 - Overwrite）：在这种情况下，新数据直接覆盖旧数据，不保留历史记录。这种方式简单直接，但无法追踪数据变化历史。\n类型2：增加型（Type 2 - Additive）：在这种情况下，新数据会被插入到维度表中，同时保留历史记录。每次维度数据变化都会生成一个新的记录，每条记录都有一个唯一标识符和有效时间范围。这样可以追踪数据变化历史，但会增加表的大小。\n类型3：历史型（Type 3 - Historical）：在这种情况下，维度表中只保留一部分历史数据，通常是最近的一次变化。旧数据会被更新，同时保留一个或多个历史属性字段，用于存储之前的值。这种方式可以平衡数据存储和查询性能，但无法完整追踪数据变化历史。\n根据具体业务需求和数据特点，可以选择适合的缓慢变化维度类型来处理维度数据的变化。SCDs在数据仓库中起到重要作用，确保数据的准确性和完整性，同时支持对历史数据的分析和追踪。"},{"id":66,"href":"/hub/Data-Process/spark-shuffle%E8%AF%A6%E8%A7%A3/","title":"spark shuffle 详解","parent":"数据处理","content":"Spark中的Shuffle是指在数据重分区（Data Reshuffling）时发生的数据移动操作，通常在数据需要重新分布到不同的Executor节点上进行计算时发生。Shuffle是Spark作业中性能开销比较大的部分之一，因此了解Shuffle的过程对于优化Spark作业至关重要。\n下面是Spark Shuffle的详细过程：\nMap阶段：\n在Map阶段，每个Executor节点会根据数据的分区规则对数据进行处理，生成中间结果。这些中间结果通常会按照Key-Value的形式存储在内存中。 Shuffle阶段：\n当需要对中间结果进行聚合或Join操作时，Spark会触发Shuffle操作。在Shuffle阶段，Spark会将数据根据Key重新分区，并将相同Key的数据发送到同一个Reducer节点上进行合并。 Map端Shuffle：\n在Map端Shuffle过程中，每个Map任务会将自己的输出数据按照Partition规则划分成多个分区，并写入本地磁盘中的文件中。同时，Map任务会将每个分区的元数据信息（包括Partition ID、数据大小等）发送给Reduce任务。 Reduce端Shuffle：\n在Reduce端Shuffle过程中，Reduce任务会从各个Map任务所在的节点上拉取数据分区，并进行合并操作。Reduce任务根据Partition ID来确定从哪个节点上拉取数据，然后将数据进行合并，最终生成最终的计算结果。 数据传输：\n在Shuffle过程中，数据的传输是通过网络进行的。数据会在Executor节点之间进行传输，可能会经过多次网络传输，这也是Shuffle操作的性能瓶颈之一。 磁盘和内存使用：\nShuffle过程中会涉及到大量的磁盘读写和内存使用。在Map端，数据会写入磁盘文件中；在Reduce端，数据会从磁盘读取到内存中进行合并操作。 通过了解Spark Shuffle的过程，可以更好地理解Spark作业中的性能瓶颈所在，有针对性地进行优化，提高作业的执行效率。\n","description":"Spark中的Shuffle是指在数据重分区（Data Reshuffling）时发生的数据移动操作，通常在数据需要重新分布到不同的Executor节点上进行计算时发生。Shuffle是Spark作业中性能开销比较大的部分之一，因此了解Shuffle的过程对于优化Spark作业至关重要。\n下面是Spark Shuffle的详细过程：\nMap阶段：\n在Map阶段，每个Executor节点会根据数据的分区规则对数据进行处理，生成中间结果。这些中间结果通常会按照Key-Value的形式存储在内存中。 Shuffle阶段：\n当需要对中间结果进行聚合或Join操作时，Spark会触发Shuffle操作。在Shuffle阶段，Spark会将数据根据Key重新分区，并将相同Key的数据发送到同一个Reducer节点上进行合并。 Map端Shuffle：\n在Map端Shuffle过程中，每个Map任务会将自己的输出数据按照Partition规则划分成多个分区，并写入本地磁盘中的文件中。同时，Map任务会将每个分区的元数据信息（包括Partition ID、数据大小等）发送给Reduce任务。 Reduce端Shuffle：\n在Reduce端Shuffle过程中，Reduce任务会从各个Map任务所在的节点上拉取数据分区，并进行合并操作。Reduce任务根据Partition ID来确定从哪个节点上拉取数据，然后将数据进行合并，最终生成最终的计算结果。 数据传输：\n在Shuffle过程中，数据的传输是通过网络进行的。数据会在Executor节点之间进行传输，可能会经过多次网络传输，这也是Shuffle操作的性能瓶颈之一。 磁盘和内存使用：\nShuffle过程中会涉及到大量的磁盘读写和内存使用。在Map端，数据会写入磁盘文件中；在Reduce端，数据会从磁盘读取到内存中进行合并操作。 通过了解Spark Shuffle的过程，可以更好地理解Spark作业中的性能瓶颈所在，有针对性地进行优化，提高作业的执行效率。"},{"id":67,"href":"/hub/Data-Architecture/","title":"数据架构","parent":"数据工程知识库","content":" Kappa架构 CAP不可能三角原则 Fan-out Claim Check Pattern Lambda Architecture 云厂商数据架构 ","description":" Kappa架构 CAP不可能三角原则 Fan-out Claim Check Pattern Lambda Architecture 云厂商数据架构 "},{"id":68,"href":"/hub/Data-Warehouse/%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E8%AE%BE%E8%AE%A1/","title":"电商数据仓库设计","parent":"数据仓库","content":"案例：电商数据仓库\n数据运营层（ODS）：\n在这一层，我们从电商网站的各个业务系统（如订单系统、库存系统、用户系统）中抽取原始数据。 数据包括订单信息、商品信息、用户信息等。 使用 Apache Kafka 作为消息系统，将数据传输到 ODS 层。 数据仓库层：\n数据细节层（DWD）： 在 DWD 层，我们对原始数据进行清洗和规范化。 例如，去除无效数据、处理日期格式、将维度退化至事实表中。 生成订单事实表、商品维度表、用户维度表等。 数据中间层（DWM）： 在 DWM 层，我们进行轻微的聚合操作，以提高查询性能。 例如，计算每日销售额、商品库存量。 生成每日销售汇总表、商品库存汇总表。 数据服务层（DWS）： 在 DWS 层，我们整合 DWM 上的基础数据，形成宽表。 例如，生成用于报表和仪表盘的数据集。 提供给数据分析师、业务用户查询使用。 数据应用层（ADS）：\n在 ADS 层，我们存放供数据产品和数据分析使用的数据。 例如，将数据存放在 Elasticsearch 中，用于支持实时搜索和商品推荐。 或者存放在 Redis 中，用于支持用户登录状态的缓存。 这样的分层设计使得数据仓库更具可维护性、可扩展性和易用性。不同层次的数据服务不同的业务需求，同时保持数据的一致性和准确性。\n","description":"案例：电商数据仓库\n数据运营层（ODS）：\n在这一层，我们从电商网站的各个业务系统（如订单系统、库存系统、用户系统）中抽取原始数据。 数据包括订单信息、商品信息、用户信息等。 使用 Apache Kafka 作为消息系统，将数据传输到 ODS 层。 数据仓库层：\n数据细节层（DWD）： 在 DWD 层，我们对原始数据进行清洗和规范化。 例如，去除无效数据、处理日期格式、将维度退化至事实表中。 生成订单事实表、商品维度表、用户维度表等。 数据中间层（DWM）： 在 DWM 层，我们进行轻微的聚合操作，以提高查询性能。 例如，计算每日销售额、商品库存量。 生成每日销售汇总表、商品库存汇总表。 数据服务层（DWS）： 在 DWS 层，我们整合 DWM 上的基础数据，形成宽表。 例如，生成用于报表和仪表盘的数据集。 提供给数据分析师、业务用户查询使用。 数据应用层（ADS）：\n在 ADS 层，我们存放供数据产品和数据分析使用的数据。 例如，将数据存放在 Elasticsearch 中，用于支持实时搜索和商品推荐。 或者存放在 Redis 中，用于支持用户登录状态的缓存。 这样的分层设计使得数据仓库更具可维护性、可扩展性和易用性。不同层次的数据服务不同的业务需求，同时保持数据的一致性和准确性。"},{"id":69,"href":"/hub/Data-on-Cloud/AWS/Amazon-MSK/","title":"Amazon MSK","parent":"AWS","content":"AWS Managed Streaming for Apache Kafka is a fully managed tool to securely stream data with\nAmazon MSK Official Documentation https://aws.amazon.com/msk/\n","description":"AWS Managed Streaming for Apache Kafka is a fully managed tool to securely stream data with\nAmazon MSK Official Documentation https://aws.amazon.com/msk/"},{"id":70,"href":"/hub/Data-on-Cloud/GCP/Google-Cloud-Platform/","title":"Google Cloud Platform","parent":"Google Cloud Platform","content":"GCP is Google\u0026rsquo;s cloud computing platform. It boasts advanced machine learning and analytics capabilities as well as the \u0026ldquo;cleanest cloud\u0026rdquo; in the industry (aka net carbon-neutral).\n","description":"GCP is Google\u0026rsquo;s cloud computing platform. It boasts advanced machine learning and analytics capabilities as well as the \u0026ldquo;cleanest cloud\u0026rdquo; in the industry (aka net carbon-neutral)."},{"id":71,"href":"/hub/Data-on-Cloud/Azure/Microsoft-Azure/","title":"Microsoft Azure","parent":"Azure","content":"Azure is Microsoft\u0026rsquo;s cloud computing platform that has over 200 products and cloud services. It is especially popular among businesses that already use Microsoft data products like [[Microsoft SQL Server]] as they move to a hybrid model of having on-prem servers and cloud servers.\n","description":"Azure is Microsoft\u0026rsquo;s cloud computing platform that has over 200 products and cloud services. It is especially popular among businesses that already use Microsoft data products like [[Microsoft SQL Server]] as they move to a hybrid model of having on-prem servers and cloud servers."},{"id":72,"href":"/hub/Data-Warehouse/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%9E%B6%E6%9E%84/","title":"实时数仓架构","parent":"数据仓库","content":"实时数仓的架构因应不同需求和场景而有所不同，但主要有两种常见的技术架构：Lambda 和 Kappa。\nLambda 架构：\nLambda 架构是一种较早的实时数仓架构，它结合了批处理和流处理。 下面是 Lambda 架构的主要组件和流程： 数据源通过 Kafka、Flume 等组件进行收集。 数据分成两条线进行计算： 一条线进入流式计算平台（例如 Storm、Flink 或 Spark Streaming），计算实时指标。 另一条线进入批量数据处理离线计算平台（例如 MapReduce、Hive、Spark SQL），计算 T+1 的相关业务指标，这些指标需要隔夜才能查看。 Lambda 架构之所以分成两条线计算，是为了解决用户等待计算结果的延迟问题。如果只有一个批处理层，用户可能需要等待几个小时才能获取计算结果。 Lambda 架构的优点包括稳定性和可控的实时计算成本。然而，它也存在一些缺点，例如维护两套复杂的分布式系统和开发周期较长。 Kappa 架构：\nKappa 架构是在 Lambda 架构的基础上简化而来，它只关注流式计算，去除了离线批处理部分。 Kappa 架构的核心思想是直接使用流处理引擎（如 Apache Flink、Kafka Streams）来满足实时计算需求。 Kappa 架构的优点包括简单性、实时性更好以及所需的计算资源较少。 尽管 Kappa 架构适用于某些场景，但并不意味着它可以完全取代 Lambda 架构。两者各有适用领域，具体情况需根据业务需求来选择¹²³⁴. Source: Conversation with Bing, 3/31/2024 (1) 数仓建设 | 图文详解实时数仓的两种技术架构Lambda和Kappa - 知乎. https://zhuanlan.zhihu.com/p/539247209. (2) 实时数仓架构 - 知乎 - 知乎专栏. https://bing.com/search?q=%e5%ae%9e%e6%97%b6%e6%95%b0%e4%bb%93%e7%9a%84%e6%9e%b6%e6%9e%84. (3) 离线和实时数仓技术架构梳理 - 百度智能云. https://cloud.baidu.com/article/2860688. (4) 实时数仓架构 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/412232339. (5) 实时数仓当前主流架构（精简总结收藏！！） - CSDN博客. https://blog.csdn.net/CLKTOY/article/details/120433175.\n","description":"实时数仓的架构因应不同需求和场景而有所不同，但主要有两种常见的技术架构：Lambda 和 Kappa。\nLambda 架构：\nLambda 架构是一种较早的实时数仓架构，它结合了批处理和流处理。 下面是 Lambda 架构的主要组件和流程： 数据源通过 Kafka、Flume 等组件进行收集。 数据分成两条线进行计算： 一条线进入流式计算平台（例如 Storm、Flink 或 Spark Streaming），计算实时指标。 另一条线进入批量数据处理离线计算平台（例如 MapReduce、Hive、Spark SQL），计算 T+1 的相关业务指标，这些指标需要隔夜才能查看。 Lambda 架构之所以分成两条线计算，是为了解决用户等待计算结果的延迟问题。如果只有一个批处理层，用户可能需要等待几个小时才能获取计算结果。 Lambda 架构的优点包括稳定性和可控的实时计算成本。然而，它也存在一些缺点，例如维护两套复杂的分布式系统和开发周期较长。 Kappa 架构：\nKappa 架构是在 Lambda 架构的基础上简化而来，它只关注流式计算，去除了离线批处理部分。 Kappa 架构的核心思想是直接使用流处理引擎（如 Apache Flink、Kafka Streams）来满足实时计算需求。 Kappa 架构的优点包括简单性、实时性更好以及所需的计算资源较少。 尽管 Kappa 架构适用于某些场景，但并不意味着它可以完全取代 Lambda 架构。两者各有适用领域，具体情况需根据业务需求来选择¹²³⁴. Source: Conversation with Bing, 3/31/2024 (1) 数仓建设 | 图文详解实时数仓的两种技术架构Lambda和Kappa - 知乎. https://zhuanlan.zhihu.com/p/539247209. (2) 实时数仓架构 - 知乎 - 知乎专栏. https://bing.com/search?q=%e5%ae%9e%e6%97%b6%e6%95%b0%e4%bb%93%e7%9a%84%e6%9e%b6%e6%9e%84. (3) 离线和实时数仓技术架构梳理 - 百度智能云. https://cloud.baidu.com/article/2860688. (4) 实时数仓架构 - 知乎 - 知乎专栏."},{"id":73,"href":"/hub/Data-Process/spark-structured-streaming-%E6%A1%88%E4%BE%8B/","title":"spark structured streaming案例","parent":"数据处理","content":"假如有三个数据源，分别是提供温度，湿度，紫外线强度三个数据的接口，这三个接口都是通过gPRC来提供Json数据格式。\n其返回的数据给是分别是这样的\n{ \u0026#34;datetime\u0026#34;:\u0026#34;2024-01-01 12:00:00\u0026#34;, \u0026#34;temparature\u0026#34;:\u0026#34;30\u0026#34;, \u0026#34;location\u0026#34;:\u0026#34;F1D1\u0026#34;, } { \u0026#34;datetime\u0026#34;:\u0026#34;2024-01-01 12:00:00\u0026#34;, \u0026#34;humidity\u0026#34;:\u0026#34;30\u0026#34;, \u0026#34;location\u0026#34;:\u0026#34;F1D1\u0026#34;, } { \u0026#34;datetime\u0026#34;:\u0026#34;2024-01-01 12:00:00\u0026#34;, \u0026#34;ultraviolet\u0026#34;:\u0026#34;30\u0026#34;, \u0026#34;location\u0026#34;:\u0026#34;F1D1\u0026#34;, } 这些数据接口每隔1秒钟就会更新一次数据 现在需要设计一个流处理引擎，实时处理这些数据，需求如下，以十秒钟的数据为一个时间窗口，如果这个时间窗口的数据中，某个地点的温度，以及湿度，以及紫外线强度的值 都超过某一个设定的阈值，并且连续超过一分钟，则记录一个状态值为true，则进行报警，把报警信息打印出来，如果这三个指标只要有一个没有达到阈值，则重置这个状态值为false。 使用spark structured streaming框架，分别用python和java实现之\nPython from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * spark = SparkSession.builder.appName(\u0026#34;RealTimeDataProcessing\u0026#34;).getOrCreate() # Define the schema for temperature data temperature_schema = StructType([ StructField(\u0026#34;datetime\u0026#34;, StringType(), True), StructField(\u0026#34;temperature\u0026#34;, IntegerType(), True), StructField(\u0026#34;location\u0026#34;, StringType(), True) ]) # Define the schema for humidity data humidity_schema = StructType([ StructField(\u0026#34;datetime\u0026#34;, StringType(), True), StructField(\u0026#34;humidity\u0026#34;, IntegerType(), True), StructField(\u0026#34;location\u0026#34;, StringType(), True) ]) # Define the schema for ultraviolet data ultraviolet_schema = StructType([ StructField(\u0026#34;datetime\u0026#34;, StringType(), True), StructField(\u0026#34;ultraviolet\u0026#34;, IntegerType(), True), StructField(\u0026#34;location\u0026#34;, StringType(), True) ]) # Read data from gRPC sources temperature_df = spark.readStream.format(\u0026#34;grpc\u0026#34;).option(\u0026#34;host\u0026#34;, \u0026#34;temperature_server\u0026#34;).load() humidity_df = spark.readStream.format(\u0026#34;grpc\u0026#34;).option(\u0026#34;host\u0026#34;, \u0026#34;humidity_server\u0026#34;).load() ultraviolet_df = spark.readStream.format(\u0026#34;grpc\u0026#34;).option(\u0026#34;host\u0026#34;, \u0026#34;ultraviolet_server\u0026#34;).load() # Define threshold values temperature_threshold = 30 humidity_threshold = 80 ultraviolet_threshold = 8 # Join the dataframes on \u0026#39;location\u0026#39; and \u0026#39;datetime\u0026#39; columns joined_df = temperature_df.join(humidity_df, [\u0026#34;location\u0026#34;, \u0026#34;datetime\u0026#34;]).join(ultraviolet_df, [\u0026#34;location\u0026#34;, \u0026#34;datetime\u0026#34;]) # Define the condition for triggering an alert alert_condition = (col(\u0026#34;temperature\u0026#34;) \u0026gt;= temperature_threshold) \u0026amp; (col(\u0026#34;humidity\u0026#34;) \u0026gt;= humidity_threshold) \u0026amp; (col(\u0026#34;ultraviolet\u0026#34;) \u0026gt;= ultraviolet_threshold) # Define the window duration and slide duration windowed_df = joined_df.withWatermark(\u0026#34;datetime\u0026#34;, \u0026#34;10 seconds\u0026#34;).groupBy(window(\u0026#34;datetime\u0026#34;, \u0026#34;10 seconds\u0026#34;)).agg(collect_list(\u0026#34;location\u0026#34;).alias(\u0026#34;locations\u0026#34;), max(when(alert_condition, 1).otherwise(0)).alias(\u0026#34;alert\u0026#34;)) # Filter the windowed dataframe to find consecutive alerts for a minute consecutive_alerts_df = windowed_df.filter((col(\u0026#34;alert\u0026#34;) == 1)).groupBy(\u0026#34;window\u0026#34;).agg(count(\u0026#34;alert\u0026#34;).alias(\u0026#34;consecutive_alerts\u0026#34;)).filter(col(\u0026#34;consecutive_alerts\u0026#34;) \u0026gt;= 6) # Start the streaming query to monitor consecutive alerts query = consecutive_alerts_df.writeStream.outputMode(\u0026#34;complete\u0026#34;).format(\u0026#34;console\u0026#34;).start() query.awaitTermination() Java import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; import org.apache.spark.sql.SparkSession; import org.apache.spark.sql.streaming.StreamingQuery; import static org.apache.spark.sql.functions.*; public class RealTimeDataProcessing { public static void main(String[] args) throws Exception { SparkSession spark = SparkSession.builder() .appName(\u0026#34;RealTimeDataProcessing\u0026#34;) .getOrCreate(); // Read data from gRPC sources Dataset\u0026lt;Row\u0026gt; temperatureDF = spark .readStream() .format(\u0026#34;grpc\u0026#34;) .option(\u0026#34;host\u0026#34;, \u0026#34;temperature_server\u0026#34;) .load(); Dataset\u0026lt;Row\u0026gt; humidityDF = spark .readStream() .format(\u0026#34;grpc\u0026#34;) .option(\u0026#34;host\u0026#34;, \u0026#34;humidity_server\u0026#34;) .load(); Dataset\u0026lt;Row\u0026gt; ultravioletDF = spark .readStream() .format(\u0026#34;grpc\u0026#34;) .option(\u0026#34;host\u0026#34;, \u0026#34;ultraviolet_server\u0026#34;) .load(); // Define threshold values int temperatureThreshold = 30; int humidityThreshold = 80; int ultravioletThreshold = 8; // Join the dataframes on \u0026#39;location\u0026#39; and \u0026#39;datetime\u0026#39; columns Dataset\u0026lt;Row\u0026gt; joinedDF = temperatureDF.join(humidityDF, \u0026#34;location\u0026#34;) .join(ultravioletDF, \u0026#34;location\u0026#34;); // Define the condition for triggering an alert Column alertCondition = col(\u0026#34;temperature\u0026#34;).geq(temperatureThreshold) .and(col(\u0026#34;humidity\u0026#34;).geq(humidityThreshold)) .and(col(\u0026#34;ultraviolet\u0026#34;).geq(ultravioletThreshold)); // Define the window duration and slide duration Dataset\u0026lt;Row\u0026gt; windowedDF = joinedDF.withWatermark(\u0026#34;datetime\u0026#34;, \u0026#34;10 seconds\u0026#34;) .groupBy(window(\u0026#34;datetime\u0026#34;, \u0026#34;10 seconds\u0026#34;)) .agg(collect_list(\u0026#34;location\u0026#34;).as(\u0026#34;locations\u0026#34;), max(when(alertCondition, 1)).otherwise(0).as(\u0026#34;alert\u0026#34;)); // Filter the windowed dataframe to find consecutive alerts for a minute Dataset\u0026lt;Row\u0026gt; consecutiveAlertsDF = windowedDF.filter(col(\u0026#34;alert\u0026#34;).equalTo(1)) .groupBy(\u0026#34;window\u0026#34;) .agg(count(\u0026#34;alert\u0026#34;).as(\u0026#34;consecutive_alerts\u0026#34;)) .filter(col(\u0026#34;consecutive_alerts\u0026#34;).geq(6)); // Start the streaming query to monitor consecutive alerts StreamingQuery query = consecutiveAlertsDF.writeStream() .outputMode(\u0026#34;complete\u0026#34;) .format(\u0026#34;console\u0026#34;) .start(); query.awaitTermination(); } } ","description":"假如有三个数据源，分别是提供温度，湿度，紫外线强度三个数据的接口，这三个接口都是通过gPRC来提供Json数据格式。\n其返回的数据给是分别是这样的\n{ \u0026#34;datetime\u0026#34;:\u0026#34;2024-01-01 12:00:00\u0026#34;, \u0026#34;temparature\u0026#34;:\u0026#34;30\u0026#34;, \u0026#34;location\u0026#34;:\u0026#34;F1D1\u0026#34;, } { \u0026#34;datetime\u0026#34;:\u0026#34;2024-01-01 12:00:00\u0026#34;, \u0026#34;humidity\u0026#34;:\u0026#34;30\u0026#34;, \u0026#34;location\u0026#34;:\u0026#34;F1D1\u0026#34;, } { \u0026#34;datetime\u0026#34;:\u0026#34;2024-01-01 12:00:00\u0026#34;, \u0026#34;ultraviolet\u0026#34;:\u0026#34;30\u0026#34;, \u0026#34;location\u0026#34;:\u0026#34;F1D1\u0026#34;, } 这些数据接口每隔1秒钟就会更新一次数据 现在需要设计一个流处理引擎，实时处理这些数据，需求如下，以十秒钟的数据为一个时间窗口，如果这个时间窗口的数据中，某个地点的温度，以及湿度，以及紫外线强度的值 都超过某一个设定的阈值，并且连续超过一分钟，则记录一个状态值为true，则进行报警，把报警信息打印出来，如果这三个指标只要有一个没有达到阈值，则重置这个状态值为false。 使用spark structured streaming框架，分别用python和java实现之\nPython from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * spark = SparkSession.builder.appName(\u0026#34;RealTimeDataProcessing\u0026#34;).getOrCreate() # Define the schema for temperature data temperature_schema = StructType([ StructField(\u0026#34;datetime\u0026#34;, StringType(), True), StructField(\u0026#34;temperature\u0026#34;, IntegerType(), True), StructField(\u0026#34;location\u0026#34;, StringType(), True) ]) # Define the schema for humidity data humidity_schema = StructType([ StructField(\u0026#34;datetime\u0026#34;, StringType(), True), StructField(\u0026#34;humidity\u0026#34;, IntegerType(), True), StructField(\u0026#34;location\u0026#34;, StringType(), True) ]) # Define the schema for ultraviolet data ultraviolet_schema = StructType([ StructField(\u0026#34;datetime\u0026#34;, StringType(), True), StructField(\u0026#34;ultraviolet\u0026#34;, IntegerType(), True), StructField(\u0026#34;location\u0026#34;, StringType(), True) ]) # Read data from gRPC sources temperature_df = spark."},{"id":74,"href":"/hub/Data-on-Cloud/AWS/Benthos/","title":"Benthos","parent":"AWS","content":"![[Assets/benthos.png|100]]\nBenthos is a free and open source data streaming engine written entirely in Go and packaged as a static single-binary command line tool. It comes with a wide range of connectors and is totally data agnostic.\nData transformations can be expressed using a high-level DSL called Bloblang, or blobl for short, which is a language designed for mapping data of a wide variety of forms.\nIt also exposes a Go API which allows users to import Benthos as a library and extend it through custom plugins.\nBenthos Studio is an application that provides visual editing and testing capabilities for Benthos pipelines.\nThe Community page contains links to various places where you can reach out and ask for help with Benthos.\nBenthos Advantages Can be used for both realtime and batch processing use cases Provides a unified framework for building data streaming pipelines Simple and easy to deploy Well-maintained and organized codebase written in pure Go Stateless Popular use cases Reading to / Writing from [[Apache Kafka|Kafka]] and similar event streaming platforms Windowed Processing ","description":"![[Assets/benthos.png|100]]\nBenthos is a free and open source data streaming engine written entirely in Go and packaged as a static single-binary command line tool. It comes with a wide range of connectors and is totally data agnostic.\nData transformations can be expressed using a high-level DSL called Bloblang, or blobl for short, which is a language designed for mapping data of a wide variety of forms.\nIt also exposes a Go API which allows users to import Benthos as a library and extend it through custom plugins."},{"id":75,"href":"/hub/Data-Process/Flink%E6%A1%88%E4%BE%8B/","title":"Flink案例","parent":"数据处理","content":"给定一个流数据，输入的数据结构是一个Transaction对象，里面包含\ntxid string, amount int, name string 三个字段，需要用flink对该transaction的流数据进行实时检测。 检测规则如下，如果对于某一个name，连续有三次的amount超过100，则进行报警，报警信息在控制台显示出来，如果是不是连续的三次，则无需报警。 这样的一个程序请用java代码来实现\nJava import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; public class TransactionDetection { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Transaction\u0026gt; transactions = ... // 从数据源获取Transaction流数据 transactions .keyBy(Transaction::getName) .flatMap(new TransactionAlertFunction()) .print(); env.execute(\u0026#34;Transaction Detection Job\u0026#34;); } public static class TransactionAlertFunction implements FlatMapFunction\u0026lt;Transaction, String\u0026gt; { private static final int THRESHOLD = 100; private static final int CONSECUTIVE_COUNT = 3; @Override public void flatMap(Transaction transaction, Collector\u0026lt;String\u0026gt; out) { if (transaction.getAmount() \u0026gt; THRESHOLD) { // 检测到金额超过阈值 TransactionState state = TransactionState.getOrDefault(transaction.getName()); state.incrementCount(); if (state.getCount() \u0026gt;= CONSECUTIVE_COUNT) { out.collect(\u0026#34;ALERT: Detected 3 consecutive transactions exceeding 100 for \u0026#34; + transaction.getName()); state.resetCount(); } } else { // 金额未超过阈值，重置计数 TransactionState.resetCount(transaction.getName()); } } } public static class TransactionState { private String name; private int count; private static final Map\u0026lt;String, TransactionState\u0026gt; stateMap = new HashMap\u0026lt;\u0026gt;(); public static TransactionState getOrDefault(String name) { return stateMap.computeIfAbsent(name, k -\u0026gt; new TransactionState(name, 0)); } public static void resetCount(String name) { TransactionState state = stateMap.get(name); if (state != null) { state.count = 0; } } public void incrementCount() { count++; } public int getCount() { return count; } private TransactionState(String name, int count) { this.name = name; this.count = count; } } } Java import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.util.Collector; public class FlinkRealTimeAlert { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Transaction\u0026gt; transactionStream = ... // 从数据源获取Transaction流数据 DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; alertStream = transactionStream .keyBy(Transaction::getName) .process(new AlertProcessFunction()); alertStream.print(); env.execute(\u0026#34;Flink Real-time Alert Job\u0026#34;); } public static class AlertProcessFunction extends KeyedProcessFunction\u0026lt;String, Transaction, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { private static final int THRESHOLD = 100; private static final int ALERT_COUNT = 3; private ValueState\u0026lt;Integer\u0026gt; countState; @Override public void open(Configuration parameters) throws Exception { ValueStateDescriptor\u0026lt;Integer\u0026gt; countDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;countState\u0026#34;, Integer.class); countState = getRuntimeContext().getState(countDescriptor); } @Override public void processElement(Transaction transaction, Context context, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) throws Exception { Integer count = countState.value(); if (count == null) { count = 0; } if (transaction.getAmount() \u0026gt; THRESHOLD) { count++; countState.update(count); } else { // do nothing, keep count as it is } if (count \u0026gt;= ALERT_COUNT) { out.collect(new Tuple2\u0026lt;\u0026gt;(transaction.getName(), transaction.getAmount())); } } } public static class Transaction { private String txid; private int amount; private String name; public Transaction(String txid, int amount, String name) { this.txid = txid; this.amount = amount; this.name = name; } public String getTxid() { return txid; } public int getAmount() { return amount; } public String getName() { return name; } } } ","description":"给定一个流数据，输入的数据结构是一个Transaction对象，里面包含\ntxid string, amount int, name string 三个字段，需要用flink对该transaction的流数据进行实时检测。 检测规则如下，如果对于某一个name，连续有三次的amount超过100，则进行报警，报警信息在控制台显示出来，如果是不是连续的三次，则无需报警。 这样的一个程序请用java代码来实现\nJava import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; public class TransactionDetection { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Transaction\u0026gt; transactions = ... // 从数据源获取Transaction流数据 transactions .keyBy(Transaction::getName) .flatMap(new TransactionAlertFunction()) .print(); env.execute(\u0026#34;Transaction Detection Job\u0026#34;); } public static class TransactionAlertFunction implements FlatMapFunction\u0026lt;Transaction, String\u0026gt; { private static final int THRESHOLD = 100; private static final int CONSECUTIVE_COUNT = 3; @Override public void flatMap(Transaction transaction, Collector\u0026lt;String\u0026gt; out) { if (transaction."},{"id":76,"href":"/hub/Data-on-Cloud/AWS/Amazon-Web-Services/","title":"Amazon Web Services","parent":"AWS","content":"AWS is a suite of cloud-based tools revolving around applications, networking, infrastructure, data processing, and data storage. One of it\u0026rsquo;s main attributes is serverless and managed products.\nDatabases [[Tools/Databases/Amazon Redshift|Amazon Redshift]] [[Tools/Databases/Amazon RDS|Amazon RDS]] [[Tools/Databases/Amazon DynamoDB|Amazon DynamoDB]] [[Tools/Databases/Amazon Aurora|Amazon Aurora]] [[Tools/Databases/Amazon DocumentDB|Amazon DocumentDB]] File Storage [[Amazon S3]] [[Amazon S3 Glacier]] Computation [[AWS Lambda]] [[Amazon EC2]] [[Amazon ECS]] [[AWS Fargate]] [[Amazon EMR]] [[AWS Batch]] Scheduling and Workflow Orchestration [[AWS Step Functions]] [[Amazon MWAA]] Data Migration and Streaming [[Amazon DMS]] [[Amazon MSK]] [[Amazon Kinesis]] ","description":"AWS is a suite of cloud-based tools revolving around applications, networking, infrastructure, data processing, and data storage. One of it\u0026rsquo;s main attributes is serverless and managed products.\nDatabases [[Tools/Databases/Amazon Redshift|Amazon Redshift]] [[Tools/Databases/Amazon RDS|Amazon RDS]] [[Tools/Databases/Amazon DynamoDB|Amazon DynamoDB]] [[Tools/Databases/Amazon Aurora|Amazon Aurora]] [[Tools/Databases/Amazon DocumentDB|Amazon DocumentDB]] File Storage [[Amazon S3]] [[Amazon S3 Glacier]] Computation [[AWS Lambda]] [[Amazon EC2]] [[Amazon ECS]] [[AWS Fargate]] [[Amazon EMR]] [[AWS Batch]] Scheduling and Workflow Orchestration [[AWS Step Functions]] [[Amazon MWAA]] Data Migration and Streaming [[Amazon DMS]] [[Amazon MSK]] [[Amazon Kinesis]] "},{"id":77,"href":"/hub/Data-Governance/data-quality/Soda/","title":"Soda","parent":"数据质量","content":"Soda is an open-source data quality testing platform that makes it easy for you to test data quality early and often in development and production pipelines. Soda catches data problems far upstream, before they can impact your business.\nSoda Official Documentation https://docs.soda.io/\nSoda Advantages #placeholder/description\nSoda Disadvantages #placeholder/description\n","description":"Soda is an open-source data quality testing platform that makes it easy for you to test data quality early and often in development and production pipelines. Soda catches data problems far upstream, before they can impact your business.\nSoda Official Documentation https://docs.soda.io/\nSoda Advantages #placeholder/description\nSoda Disadvantages #placeholder/description"},{"id":78,"href":"/hub/Data-Governance/data-quality/Great-Expectations/","title":"Great Expectations","parent":"数据质量","content":"Great Expectations is a [[Python]] library for creating [[Data Unit Test|data unit tests]] that can be used in your [[Data Pipeline|data pipelines]].\nSummary Great Expectations operates off of the principal that data engineering pipelines tend towards entropy over time, a term that they dub \u0026ldquo;pipeline debt\u0026rdquo;. Great Expectations aims to provide a testing and evaluation suite to help data engineering teams clean up their pipelines and increase their confidence in working on them in a collaborative setting.\nGreat Expectations calls each unit test segment an \u0026ldquo;Expectation\u0026rdquo;, due to expecting the output of a test to be a certain value. Rather than other testing and evaluation suites, Great Expectations encourage testing at batch time (when new data arrives). This is in contrast to compile or deploy times. By testing at batch time, teams can be confident that there is a safety net should code behave unexpectedly for new data, and pinpoint the root cause as soon as possible.\nWorkflow Introduce the expectation early into the process, perhaps even before you\u0026rsquo;ve built a pipeline. Show it to the stakeholder and have them validate the assumptions. Implement it into your pipeline. Continuously update tests as data changes by iterating with the stakeholder. ","description":"Great Expectations is a [[Python]] library for creating [[Data Unit Test|data unit tests]] that can be used in your [[Data Pipeline|data pipelines]].\nSummary Great Expectations operates off of the principal that data engineering pipelines tend towards entropy over time, a term that they dub \u0026ldquo;pipeline debt\u0026rdquo;. Great Expectations aims to provide a testing and evaluation suite to help data engineering teams clean up their pipelines and increase their confidence in working on them in a collaborative setting."},{"id":79,"href":"/hub/Data-Governance/data-quality/Monte-Carlo/","title":"Monte Carlo","parent":"数据质量","content":"Monte Carlo is an end-to-end [[data observability]] tool that monitors and alerts you of data quality issues throughout the data lifecycle.\nMonte Carlo Official Documentation https://docs.getmontecarlo.com/\nMonte Carlo Advantages #placeholder/description\nMonte Carlo Disadvantages #placeholder/description\n","description":"Monte Carlo is an end-to-end [[data observability]] tool that monitors and alerts you of data quality issues throughout the data lifecycle.\nMonte Carlo Official Documentation https://docs.getmontecarlo.com/\nMonte Carlo Advantages #placeholder/description\nMonte Carlo Disadvantages #placeholder/description"},{"id":80,"href":"/hub/Data-Governance/data-quality/Deequ/","title":"Deequ","parent":"数据质量","content":"Deequ is a library built on top of Apache Spark for defining \u0026ldquo;unit tests for data\u0026rdquo;, which measure data quality in large datasets. Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on GitHub, readthedocs, and PyPI.\nDeequ Official Documentation https://github.com/awslabs/deequ/tree/master\nDeequ Advantages Built on [[Apache Spark]] for large datasets Deequ Disadvantages #placeholder/description\n","description":"Deequ is a library built on top of Apache Spark for defining \u0026ldquo;unit tests for data\u0026rdquo;, which measure data quality in large datasets. Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on GitHub, readthedocs, and PyPI.\nDeequ Official Documentation https://github.com/awslabs/deequ/tree/master\nDeequ Advantages Built on [[Apache Spark]] for large datasets Deequ Disadvantages #placeholder/description"},{"id":81,"href":"/hub/Data-Storage/OSS/Amazon-S3-Glacier/","title":"Amazon S3 Glacier","parent":"对象存储","content":"Glacier is a related service to S3 where data is archived. With extremely cheap data storage, Glacier’s main downside is that its cost-optimized for infrequent data access.\nAmazon S3 Glacier Official Documentation https://aws.amazon.com/s3/storage-classes/glacier/\n","description":"Glacier is a related service to S3 where data is archived. With extremely cheap data storage, Glacier’s main downside is that its cost-optimized for infrequent data access.\nAmazon S3 Glacier Official Documentation https://aws.amazon.com/s3/storage-classes/glacier/"},{"id":82,"href":"/hub/Data-Storage/OSS/Amazon-S3/","title":"Amazon S3","parent":"对象存储","content":"S3 is a blob storage platform that is different in nature than standard flat file system storage. By treating individual files as objects, S3 offers a way to securely and reliably store large quantities of data.\nAmazon S3 Official Documentation https://aws.amazon.com/s3/\n","description":"S3 is a blob storage platform that is different in nature than standard flat file system storage. By treating individual files as objects, S3 offers a way to securely and reliably store large quantities of data.\nAmazon S3 Official Documentation https://aws.amazon.com/s3/"},{"id":83,"href":"/hub/Data-Storage/SQL-DB/OLAP/Azure-Synapse-Analytics/","title":"Azure Synapse Analytics","parent":"分析型数据库","content":"Azure Synapse Analytics is Microsoft\u0026rsquo;s [[Data Warehouse|enterprise data warehouse]] offering. It offers serverless and dedicated options and is designed for big data analytics and machine learning.\nAzure Synapse Analytics Official Documentation https://learn.microsoft.com/en-us/azure/synapse-analytics/overview-what-is\nAzure Synapse Analytics Learning Resources https://azure.microsoft.com/en-us/resources/developers/synapse-analytics-for-data-engineers/#overview\n","description":"Azure Synapse Analytics is Microsoft\u0026rsquo;s [[Data Warehouse|enterprise data warehouse]] offering. It offers serverless and dedicated options and is designed for big data analytics and machine learning.\nAzure Synapse Analytics Official Documentation https://learn.microsoft.com/en-us/azure/synapse-analytics/overview-what-is\nAzure Synapse Analytics Learning Resources https://azure.microsoft.com/en-us/resources/developers/synapse-analytics-for-data-engineers/#overview"},{"id":84,"href":"/hub/Data-Storage/NOSQL-DB/column-database/Couchbase/","title":"Couchbase","parent":"列存储数据库","content":"Couchbase is the highest performing NoSQL distributed database.\n","description":"Couchbase is the highest performing NoSQL distributed database."},{"id":85,"href":"/hub/Data-Storage/NOSQL-DB/others/","title":"其他","parent":"NOSQL数据库","content":"","description":""},{"id":86,"href":"/hub/Data-Storage/SQL-DB/OLAP/","title":"分析型数据库","parent":"SQL数据库","content":"","description":""},{"id":87,"href":"/hub/Data-Storage/NOSQL-DB/k-v-database/","title":"键值对数据库","parent":"NOSQL数据库","content":"","description":""},{"id":88,"href":"/hub/Data-Storage/SQL-DB/OLAP/Amazon-Redshift/","title":"Amazon Redshift","parent":"分析型数据库","content":"Amazon Redshift is an [[Amazon Web Services|AWS]] cloud-based [[Data Warehouse|data warehouse]] service based on [[PostgreSQL]].\nDeveloper Documentation ","description":"Amazon Redshift is an [[Amazon Web Services|AWS]] cloud-based [[Data Warehouse|data warehouse]] service based on [[PostgreSQL]].\nDeveloper Documentation "},{"id":89,"href":"/hub/Data-Storage/NOSQL-DB/k-v-database/Redis/","title":"Redis","parent":"键值对数据库","content":" Redis Redis提供诸如字符串、哈希、列表、集合、带有范围查询的有序集合、位图、HyperLogLogs、地理空间索引和流等数据结构。Redis具有内置的复制、Lua脚本、LRU驱逐、事务以及不同级别的磁盘持久性，并通过Redis Sentinel提供高可用性，并通过Redis Cluster实现自动分区。\nRedis支持的数据结构 https://redis.io/docs/data-types/\nredis的失效策略 定时失效（TTL）：当为一个键设置了过期时间（TTL），Redis会自动在键过期后将其删除。这是最常见的失效策略，适用于需要在固定时间后使键失效的场景。\n惰性删除：在获取键时，Redis会检查该键是否过期，如果过期则删除。这种策略可能会导致过期键在一段时间内仍然存在于内存中，直到被访问时才会被删除。\n定期删除：Redis会定期随机检查一些过期键，并删除其中已过期的键。这种策略可以在不频繁检查所有过期键的情况下，保证过期键的及时删除。\n持久性失效：Redis在写入一个键时，会检查该键是否过期，如果过期则拒绝写入。这种策略可以确保写入的数据不会立即过期。\n主动清理：通过配置参数maxmemory-policy，可以设置Redis在达到内存上限时的清理策略，如LRU（最近最少使用）、LFU（最少使用频率）等，以释放内存空间。\nredis ","description":" Redis Redis提供诸如字符串、哈希、列表、集合、带有范围查询的有序集合、位图、HyperLogLogs、地理空间索引和流等数据结构。Redis具有内置的复制、Lua脚本、LRU驱逐、事务以及不同级别的磁盘持久性，并通过Redis Sentinel提供高可用性，并通过Redis Cluster实现自动分区。\nRedis支持的数据结构 https://redis.io/docs/data-types/\nredis的失效策略 定时失效（TTL）：当为一个键设置了过期时间（TTL），Redis会自动在键过期后将其删除。这是最常见的失效策略，适用于需要在固定时间后使键失效的场景。\n惰性删除：在获取键时，Redis会检查该键是否过期，如果过期则删除。这种策略可能会导致过期键在一段时间内仍然存在于内存中，直到被访问时才会被删除。\n定期删除：Redis会定期随机检查一些过期键，并删除其中已过期的键。这种策略可以在不频繁检查所有过期键的情况下，保证过期键的及时删除。\n持久性失效：Redis在写入一个键时，会检查该键是否过期，如果过期则拒绝写入。这种策略可以确保写入的数据不会立即过期。\n主动清理：通过配置参数maxmemory-policy，可以设置Redis在达到内存上限时的清理策略，如LRU（最近最少使用）、LFU（最少使用频率）等，以释放内存空间。\nredis "},{"id":90,"href":"/hub/Data-Storage/NOSQL-DB/others/ClickHouse/","title":"ClickHouse","parent":"其他","content":" ![[Assets/clickhouse_logo.png|100]]\nClickHouse is a column-oriented database management system (DBMS) for online analytical processing of queries ([[Online Analytical Processing|OLAP]]). It can process billions of rows and tens of gigabytes of data per server per second.\nClickHouse Official Documentation https://clickhouse.com/docs\nClickHouse Advantages #placeholder/description\nClickHouse Disadvantages #placeholder/description\n","description":"![[Assets/clickhouse_logo.png|100]]\nClickHouse is a column-oriented database management system (DBMS) for online analytical processing of queries ([[Online Analytical Processing|OLAP]]). It can process billions of rows and tens of gigabytes of data per server per second.\nClickHouse Official Documentation https://clickhouse.com/docs\nClickHouse Advantages #placeholder/description\nClickHouse Disadvantages #placeholder/description"},{"id":91,"href":"/hub/Data-Storage/SQL-DB/OLAP/Google-BigQuery/","title":"Google BigQuery","parent":"分析型数据库","content":"![[Assets/google_bigquery_logo.png|100]]\nGoogle BigQuery is an analytics data warehouse that Google Cloud offers. It lets you run complex queries quickly over large data sets, and there\u0026rsquo;s no need to set up or manage any infrastructure.\nGoogle BigQuery Official Documentation https://cloud.google.com/bigquery/docs\nGoogle BigQuery Learning Resources https://cloud.google.com/bigquery/docs/quickstarts https://www.youtube.com/playlist?list=PLIivdWyY5sqIZLeLzyg1B-Pd1MIOo6d-g\n","description":"![[Assets/google_bigquery_logo.png|100]]\nGoogle BigQuery is an analytics data warehouse that Google Cloud offers. It lets you run complex queries quickly over large data sets, and there\u0026rsquo;s no need to set up or manage any infrastructure.\nGoogle BigQuery Official Documentation https://cloud.google.com/bigquery/docs\nGoogle BigQuery Learning Resources https://cloud.google.com/bigquery/docs/quickstarts https://www.youtube.com/playlist?list=PLIivdWyY5sqIZLeLzyg1B-Pd1MIOo6d-g"},{"id":92,"href":"/hub/Data-Storage/SQL-DB/OLTP/Amazon-RDS/","title":"Amazon RDS","parent":"事务性数据库","content":"Amazon RDS (Relational Database Service) is a collection of AWS managed services. Featuring several different popular relational database engines, including Aurora, Postgres, and MySQL, RDS is a popular choice for spinning up databases.\nOne of the key features of RDS is that it is a cloud distributed product. This simplifies the setup of the databases and allows for easier scalability. There are a few features in particular that RDS is known for:\nRead Replicas Read replicas create multiple “replica” copies of data. This allows high-volume applications to have enhanced performance for read-heavy operations. Read replica can be turned into standalone database instances, and elastically scale.\nMulti-AZ AWS is well known for their availability zone architecture, which separates traffic from physical geographic regions. Multi-AZ settings allow for the creation of standby replicas in different regions to allow for enhanced performance.\nAmazon RDS Official Documentation https://aws.amazon.com/rds/\n","description":"Amazon RDS (Relational Database Service) is a collection of AWS managed services. Featuring several different popular relational database engines, including Aurora, Postgres, and MySQL, RDS is a popular choice for spinning up databases.\nOne of the key features of RDS is that it is a cloud distributed product. This simplifies the setup of the databases and allows for easier scalability. There are a few features in particular that RDS is known for:"},{"id":93,"href":"/hub/Data-Storage/SQL-DB/OLTP/MySQL/","title":"MySQL","parent":"事务性数据库","content":"MySQL的架构，可以分为以下几个部分：\n连接池组件（Connection Pool）：负责管理客户端与数据库之间的连接，提高连接的复用性和效率。\n查询解析器（Query Parser）：负责解析并优化SQL查询语句，生成执行计划。\n查询缓存（Query Cache）：用于缓存查询结果，提高查询性能。\n存储引擎接口（Storage Engine Interface）：负责和存储引擎进行交互，实现数据的存储和检索。\n存储引擎（Storage Engine）：负责实际的数据存储和检索操作。\nMySQL有多个存储引擎可供选择，常用的存储引擎包括：\nInnoDB：支持事务处理，行级锁定，外键约束等特性，适合于需要较高并发性能和数据完整性的场景。\nMyISAM：不支持事务处理和行级锁定，但在读写频率差异较大的场景下性能较好，适合于读密集型应用。\nMEMORY：将数据存储在内存中，读写速度非常快，但数据在数据库重启时会丢失，适合于临时数据存储和缓存。\nNDB Cluster：适用于分布式存储和高可用性需求，支持数据分片和自动故障切换。\n每种存储引擎都有自己的特点和适用场景，用户可以根据实际需求选择合适的存储引擎来优化数据库性能和功能。\n","description":"MySQL的架构，可以分为以下几个部分：\n连接池组件（Connection Pool）：负责管理客户端与数据库之间的连接，提高连接的复用性和效率。\n查询解析器（Query Parser）：负责解析并优化SQL查询语句，生成执行计划。\n查询缓存（Query Cache）：用于缓存查询结果，提高查询性能。\n存储引擎接口（Storage Engine Interface）：负责和存储引擎进行交互，实现数据的存储和检索。\n存储引擎（Storage Engine）：负责实际的数据存储和检索操作。\nMySQL有多个存储引擎可供选择，常用的存储引擎包括：\nInnoDB：支持事务处理，行级锁定，外键约束等特性，适合于需要较高并发性能和数据完整性的场景。\nMyISAM：不支持事务处理和行级锁定，但在读写频率差异较大的场景下性能较好，适合于读密集型应用。\nMEMORY：将数据存储在内存中，读写速度非常快，但数据在数据库重启时会丢失，适合于临时数据存储和缓存。\nNDB Cluster：适用于分布式存储和高可用性需求，支持数据分片和自动故障切换。\n每种存储引擎都有自己的特点和适用场景，用户可以根据实际需求选择合适的存储引擎来优化数据库性能和功能。"},{"id":94,"href":"/hub/Data-Storage/SQL-DB/OLTP/PostgreSQL/","title":"PostgreSQL","parent":"事务性数据库","content":"![[Assets/postgresql_logo.png|100]]\nPostgreSQL or Postgres as it is commonly referred to is an open-source [[Relational Database Management System]] with over 30 years of active development. It has a strong reputation for reliability, feature robustness, and performance.\nLearning Resources https://pgexercises.com/\n","description":"![[Assets/postgresql_logo.png|100]]\nPostgreSQL or Postgres as it is commonly referred to is an open-source [[Relational Database Management System]] with over 30 years of active development. It has a strong reputation for reliability, feature robustness, and performance.\nLearning Resources https://pgexercises.com/"},{"id":95,"href":"/hub/Data-Storage/SQL-DB/OLTP/Microsoft-SQL-Server/","title":"Microsoft SQL Server","parent":"事务性数据库","content":"Microsoft SQL Server is a [[Relational Database Management System]] developed by Microsoft. Microsoft currently supports several versions of SQL Server that target different business applications.\n","description":"Microsoft SQL Server is a [[Relational Database Management System]] developed by Microsoft. Microsoft currently supports several versions of SQL Server that target different business applications."},{"id":96,"href":"/hub/Data-Mining/user_profile/","title":"用户画像","parent":"数据挖掘","content":"用户画像（User Profile）是一种刻画用户需求的模型，广泛应用于推荐系统、广告系统、商业分析、数据分析、用户增长、用户研究、产品设计、数据化运营、精准营销、量化风控等领域¹²。\nSource: Conversation with Bing, 4/1/2024 (1) 用户画像的基础、原理、方法论（模型）和应用 - 知乎. https://zhuanlan.zhihu.com/p/140104236. (2) 用户画像的基础、原理、方法论（模型）和应用 - 知乎. https://bing.com/search?q=%e7%94%a8%e6%88%b7%e7%94%bb%e5%83%8f%e6%8a%80%e6%9c%af. (3) 用户画像 - 知乎. https://www.zhihu.com/topic/19647591/intro. (4) 用户画像原理、技术选型及架构实现-CSDN博客. https://blog.csdn.net/SecondLieutenant/article/details/81153565.\n","description":"用户画像（User Profile）是一种刻画用户需求的模型，广泛应用于推荐系统、广告系统、商业分析、数据分析、用户增长、用户研究、产品设计、数据化运营、精准营销、量化风控等领域¹²。\nSource: Conversation with Bing, 4/1/2024 (1) 用户画像的基础、原理、方法论（模型）和应用 - 知乎. https://zhuanlan.zhihu.com/p/140104236. (2) 用户画像的基础、原理、方法论（模型）和应用 - 知乎. https://bing.com/search?q=%e7%94%a8%e6%88%b7%e7%94%bb%e5%83%8f%e6%8a%80%e6%9c%af. (3) 用户画像 - 知乎. https://www.zhihu.com/topic/19647591/intro. (4) 用户画像原理、技术选型及架构实现-CSDN博客. https://blog.csdn.net/SecondLieutenant/article/details/81153565."},{"id":97,"href":"/hub/Data-Architecture/CAP%E4%B8%8D%E5%8F%AF%E8%83%BD%E4%B8%89%E8%A7%92%E5%8E%9F%E5%88%99/","title":"CAP不可能三角原则","parent":"数据架构","content":"CAP原则，又称CAP定理，是在分布式系统设计中必须考虑的理论。它涉及三个基本性质：\n一致性（Consistency）：\n对于客户端的每次读操作，要么读到的是最新的数据，要么读取失败。 一致性承诺是站在分布式系统的角度，对访问本系统的客户端的一种保证：要么返回错误，要么返回绝对一致的最新数据。 可用性（Availability）：\n任何客户端的请求都能得到响应数据，不会出现响应错误。 可用性承诺是站在分布式系统的角度，对访问本系统的客户的另一种保证：一定会返回数据，不会返回错误，但不保证数据最新。 分区容忍性（Partition tolerance）：\n由于分布式系统通过网络进行通信，网络是不可靠的。 即使任意数量的消息丢失或延迟到达，系统仍会继续提供服务，不会挂掉。 权衡：\nCAP理论指出，一个分布式系统不可能同时满足这三个特性。 在不存在网络失败的情况下，C和A能够同时保证。只有当网络发生分区或失败时，才会在C和A之间做出选择。 对于一个分布式系统而言，P是前提，必须保证，因为只要有网络交互就一定会有延迟和数据丢失，这种状况我们必须接受，必须保证系统不能挂掉。 所以只剩下C、A可以选择。要么保证数据一致性（保证数据绝对正确），要么保证可用性（保证系统不出错）。 当选择了C（一致性）时，如果由于网络分区而无法保证特定信息是最新的，则系统将返回错误或超时。 当选择了A（可用性）时，系统将始终处理客户端的查询并尝试返回最新的可用的信息版本，即使由于网络分区而无法保证其是最新的。 C、A、P三者之间的冲突：\n在分布式系统中，最大的问题是网络。 假设两台服务器之间的网络断开，仍要支持这种网络异常，即满足分区容错性。 这时，要么牺牲数据一致性，响应旧的数据给用户；要么牺牲可用性，阻塞等待，直到网络连接恢复，数据更新操作完成后再响应最新的数据。 总之，CAP原则在分布式系统设计中帮助我们权衡一致性、可用性和分区容忍性，根据业务需求做出合适的选择¹²³⁴.\nSource: Conversation with Bing, 3/31/2024 (1) 轻松理解CAP理论 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/50990721. (2) 分布式必备理论基础：CAP和BASE - 三分恶 - 博客园. https://bing.com/search?q=CAP%e4%b8%89%e5%8e%9f%e5%88%99. (3) CAP原则 - 百度百科. https://baike.baidu.com/item/CAP%E5%8E%9F%E5%88%99/5712863. (4) 分布式必备理论基础：CAP和BASE - 三分恶 - 博客园. https://www.cnblogs.com/three-fighter/p/15293310.html. (5) 分布式系统架构中CAP原理及案例-腾讯云开发者社区-腾讯云. https://cloud.tencent.com/developer/article/1554867.\n","description":"CAP原则，又称CAP定理，是在分布式系统设计中必须考虑的理论。它涉及三个基本性质：\n一致性（Consistency）：\n对于客户端的每次读操作，要么读到的是最新的数据，要么读取失败。 一致性承诺是站在分布式系统的角度，对访问本系统的客户端的一种保证：要么返回错误，要么返回绝对一致的最新数据。 可用性（Availability）：\n任何客户端的请求都能得到响应数据，不会出现响应错误。 可用性承诺是站在分布式系统的角度，对访问本系统的客户的另一种保证：一定会返回数据，不会返回错误，但不保证数据最新。 分区容忍性（Partition tolerance）：\n由于分布式系统通过网络进行通信，网络是不可靠的。 即使任意数量的消息丢失或延迟到达，系统仍会继续提供服务，不会挂掉。 权衡：\nCAP理论指出，一个分布式系统不可能同时满足这三个特性。 在不存在网络失败的情况下，C和A能够同时保证。只有当网络发生分区或失败时，才会在C和A之间做出选择。 对于一个分布式系统而言，P是前提，必须保证，因为只要有网络交互就一定会有延迟和数据丢失，这种状况我们必须接受，必须保证系统不能挂掉。 所以只剩下C、A可以选择。要么保证数据一致性（保证数据绝对正确），要么保证可用性（保证系统不出错）。 当选择了C（一致性）时，如果由于网络分区而无法保证特定信息是最新的，则系统将返回错误或超时。 当选择了A（可用性）时，系统将始终处理客户端的查询并尝试返回最新的可用的信息版本，即使由于网络分区而无法保证其是最新的。 C、A、P三者之间的冲突：\n在分布式系统中，最大的问题是网络。 假设两台服务器之间的网络断开，仍要支持这种网络异常，即满足分区容错性。 这时，要么牺牲数据一致性，响应旧的数据给用户；要么牺牲可用性，阻塞等待，直到网络连接恢复，数据更新操作完成后再响应最新的数据。 总之，CAP原则在分布式系统设计中帮助我们权衡一致性、可用性和分区容忍性，根据业务需求做出合适的选择¹²³⁴.\nSource: Conversation with Bing, 3/31/2024 (1) 轻松理解CAP理论 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/50990721. (2) 分布式必备理论基础：CAP和BASE - 三分恶 - 博客园. https://bing.com/search?q=CAP%e4%b8%89%e5%8e%9f%e5%88%99. (3) CAP原则 - 百度百科. https://baike.baidu.com/item/CAP%E5%8E%9F%E5%88%99/5712863. (4) 分布式必备理论基础：CAP和BASE - 三分恶 - 博客园. https://www.cnblogs.com/three-fighter/p/15293310.html. (5) 分布式系统架构中CAP原理及案例-腾讯云开发者社区-腾讯云. https://cloud.tencent.com/developer/article/1554867."},{"id":98,"href":"/hub/Draft/%E9%97%AE%E9%A2%98%E9%9B%86/","title":"问题集","parent":"草稿","content":" 关于数据库 关于数据计算 关于数据集成 关于数据挖掘 关于数据治理 关于架构 实时仓库架构的架构有哪些？ 数据仓库，数据湖，数仓一体架构各有何特点，其区别有哪些方面？ ","description":" 关于数据库 关于数据计算 关于数据集成 关于数据挖掘 关于数据治理 关于架构 实时仓库架构的架构有哪些？ 数据仓库，数据湖，数仓一体架构各有何特点，其区别有哪些方面？ "},{"id":99,"href":"/hub/Data-Architecture/kappa-%E6%9E%B6%E6%9E%84/","title":"Kappa架构","parent":"数据架构","content":"Kappa 架构是一种流式优先的部署模式，专注于实时处理数据。相较于Lambda 架构，它更简化了数据摄取流程，只使用单一的流处理引擎来处理历史和实时数据¹².\n下面是一套基于开源软件的 Kappa 架构解决方案，涵盖数据摄取、存储、计算、服务和展示：\n数据摄取：\n使用Apache Kafka作为消息系统，将来自流式、物联网、批处理或近实时（例如变更数据捕获）的数据摄取到 Kafka 中。 数据存储：\n将数据分发到服务层，例如云数据湖、云数据仓库、操作智能或警报系统，以供自助分析、机器学习、报表、仪表盘、预测和预防性维护等用例使用。 作为存储层的选择，你可以考虑以下开源工具： 云数据湖：例如 Amazon S3、Azure Data Lake Storage。 云数据仓库：例如 Amazon Redshift、Google BigQuery、Snowflake。 操作智能和警报系统：例如 Elasticsearch、Prometheus。 数据计算：\n使用流处理引擎（例如 Apache Spark、Apache Flink）从 Kafka 中读取数据，进行转换和计算。 将经过处理的数据重新发布到 Kafka，使其可用于实时分析。 数据服务：\n通过服务层提供数据服务，例如： 自助分析：用户可以查询和分析实时数据。 机器学习：训练模型并进行预测。 报表和仪表盘：展示实时指标和趋势。 预测和预防性维护：检测异常并采取措施。 数据展示：\n使用可视化工具（例如 Grafana、Kibana）创建仪表盘，展示实时数据。 你还可以使用自定义应用程序或 Web 界面来展示数据。 总之，Kappa 架构提供了一种简单且实时的方式来处理数据，适用于许多实时数据处理场景。如果你希望采用 Kappa 架构，可以考虑使用Apache Kafka作为消息系统，Apache Flink作为流处理引擎，并结合云数据湖和云数据仓库来存储和服务数据¹²³⁴.\nSource: Conversation with Bing, 3/31/2024 (1) Kappa Architecture – Easy Adoption with Informatica End-to-End \u0026hellip;. https://www.informatica.com/blogs/adopt-a-kappa-architecture-for-streaming-and-ingesting-data.html. (2) Real-Time Data Ingestion Architecture: Tools \u0026amp; Examples. https://estuary.dev/real-time-data-ingestion/. (3) Kappa Architecture: A Different Way to Process Data - 3Cloud. https://3cloudsolutions.com/resources/kappa-architecture-a-different-way-to-process-data/. (4) Kappa Architecture 1:1 - How to Build a Modern Streaming Data \u0026hellip;. https://nexocode.com/blog/posts/kappa-architecture/.\n","description":"Kappa 架构是一种流式优先的部署模式，专注于实时处理数据。相较于Lambda 架构，它更简化了数据摄取流程，只使用单一的流处理引擎来处理历史和实时数据¹².\n下面是一套基于开源软件的 Kappa 架构解决方案，涵盖数据摄取、存储、计算、服务和展示：\n数据摄取：\n使用Apache Kafka作为消息系统，将来自流式、物联网、批处理或近实时（例如变更数据捕获）的数据摄取到 Kafka 中。 数据存储：\n将数据分发到服务层，例如云数据湖、云数据仓库、操作智能或警报系统，以供自助分析、机器学习、报表、仪表盘、预测和预防性维护等用例使用。 作为存储层的选择，你可以考虑以下开源工具： 云数据湖：例如 Amazon S3、Azure Data Lake Storage。 云数据仓库：例如 Amazon Redshift、Google BigQuery、Snowflake。 操作智能和警报系统：例如 Elasticsearch、Prometheus。 数据计算：\n使用流处理引擎（例如 Apache Spark、Apache Flink）从 Kafka 中读取数据，进行转换和计算。 将经过处理的数据重新发布到 Kafka，使其可用于实时分析。 数据服务：\n通过服务层提供数据服务，例如： 自助分析：用户可以查询和分析实时数据。 机器学习：训练模型并进行预测。 报表和仪表盘：展示实时指标和趋势。 预测和预防性维护：检测异常并采取措施。 数据展示：\n使用可视化工具（例如 Grafana、Kibana）创建仪表盘，展示实时数据。 你还可以使用自定义应用程序或 Web 界面来展示数据。 总之，Kappa 架构提供了一种简单且实时的方式来处理数据，适用于许多实时数据处理场景。如果你希望采用 Kappa 架构，可以考虑使用Apache Kafka作为消息系统，Apache Flink作为流处理引擎，并结合云数据湖和云数据仓库来存储和服务数据¹²³⁴.\nSource: Conversation with Bing, 3/31/2024 (1) Kappa Architecture – Easy Adoption with Informatica End-to-End \u0026hellip;. https://www.informatica.com/blogs/adopt-a-kappa-architecture-for-streaming-and-ingesting-data.html. (2) Real-Time Data Ingestion Architecture: Tools \u0026amp; Examples."},{"id":100,"href":"/hub/Draft/","title":"草稿","parent":"数据工程知识库","content":" 问题集 ","description":" 问题集 "},{"id":101,"href":"/hub/Data-Architecture/Fan-out/","title":"Fan-out","parent":"数据架构","content":"Fan-out is a pattern where a message from a source is spread or copied to one or more destinations. In data engineering, fan-out is commonly used to send data from a microservice (publisher) to multiple subscribers. The fan-out service normally doesn\u0026rsquo;t save the message once it has been sent, so a message queue is also common to see between the fan-out service and the subscriber for catch-up/re-try scenarios.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A[Publisher] --\u0026gt;|Message 1| B(Fan-out service) B --\u0026gt;|Message 1| C[Subscriber 1] B --\u0026gt;|Message 1| D[Subscriber 2] B --\u0026gt;|Message 1| E[Subscriber 3] Fan-out Advantages Send data from one source to many destinations Fan-out Disadvantages Usually limited re-try capabilities if a subscriber is unavailable for an extended period ","description":"Fan-out is a pattern where a message from a source is spread or copied to one or more destinations. In data engineering, fan-out is commonly used to send data from a microservice (publisher) to multiple subscribers. The fan-out service normally doesn\u0026rsquo;t save the message once it has been sent, so a message queue is also common to see between the fan-out service and the subscriber for catch-up/re-try scenarios.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A[Publisher] --\u0026gt;|Message 1| B(Fan-out service) B --\u0026gt;|Message 1| C[Subscriber 1] B --\u0026gt;|Message 1| D[Subscriber 2] B --\u0026gt;|Message 1| E[Subscriber 3] Fan-out Advantages Send data from one source to many destinations Fan-out Disadvantages Usually limited re-try capabilities if a subscriber is unavailable for an extended period "},{"id":102,"href":"/hub/Data-Storage/NOSQL-DB/others/Timeseries-Database/","title":"Timeseries Database","parent":"其他","content":"A timeseries database (TSDB) is optimized to store, aggregate and analyze large amounts of continuously generated time-stamped data from sources such as IoT devices or sensors. They are used in applications that require monitoring performance changes over time or tracking sequences of events.\nTimeseries Database Advantages Generally provide built-in functions for analyzing timeseries data and lifecycle management More efficiently compresses and stores timeseries data vs general purpose databases Timeseries Database Disadvantages Not optimal for analyzing relationships between datasets Requires more storage because all timeseries data is indexed Require a greater amount of code and complexity vs general purpose databases Timeseries Database Use Cases Stock market data Trading platform Stock exchange Real-time ad bidding Popular Timeseries Databases InfluxDB Kdb+ Prometheus Graphite TimescaleDB ","description":"A timeseries database (TSDB) is optimized to store, aggregate and analyze large amounts of continuously generated time-stamped data from sources such as IoT devices or sensors. They are used in applications that require monitoring performance changes over time or tracking sequences of events.\nTimeseries Database Advantages Generally provide built-in functions for analyzing timeseries data and lifecycle management More efficiently compresses and stores timeseries data vs general purpose databases Timeseries Database Disadvantages Not optimal for analyzing relationships between datasets Requires more storage because all timeseries data is indexed Require a greater amount of code and complexity vs general purpose databases Timeseries Database Use Cases Stock market data Trading platform Stock exchange Real-time ad bidding Popular Timeseries Databases InfluxDB Kdb+ Prometheus Graphite TimescaleDB "},{"id":103,"href":"/hub/Data-Architecture/Claim-Check-Pattern/","title":"Claim Check Pattern","parent":"数据架构","content":"The claim-check pattern is used to reduce the cost and size of large messages by first storing the data in an external storage location and then sending a reference to the data/event to the consumer.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A[[Message with data]]--\u0026gt;|1.| B((Producer)) B --\u0026gt;|2. Store data, save key| C[(Storage)] D[[Smaller message with key only]] B --\u0026gt;|3.| D --\u0026gt;|4.| E((Consumer)) C --\u0026gt;|5. Get data with key| E --\u0026gt;|6.| F[[Message with data]] Send message Store message in data store Enqueue the message\u0026rsquo;s reference (i.e. key) Read the message\u0026rsquo;s reference Retrieve the message Process the message Claim Check Pattern Advantages Reduces cost of data transfer via messaging/streams. This is because storage is usually cheaper than messaging/streaming resources (memory). Helps protect the message bus and client from being overwhelmed or slowed down by large messages. Allows you to asynchronously process data which can help with scalability/performance. Claim Check Pattern Disadvantages If the external service used to store the payload fails, then the message will not be delivered. Requires additional storage space and adds additional time to store/retrieve data. Claim Check Pattern Examples Kafka client writes payload to S3/Azure Blob Storage/GCS. Then it sends a notification message. The consumer receives the message and accesses the payload from S3/Azure Blob Storage/GCS. In Airflow, you sometimes need to pass data between tasks. You can do this using XComs but there is a limit to the size of the message you can send. For passing large messages via XComs you can use the claim check pattern. Sources:\nhttps://learn.microsoft.com/en-us/azure/architecture/patterns/claim-check https://www.enterpriseintegrationpatterns.com/patterns/messaging/StoreInLibrary.html https://serverlessland.com/event-driven-architecture/visuals/claim-check-pattern https://aws.plainenglish.io/an-introduction-to-claim-check-pattern-and-its-uses-b018649a380d ","description":"The claim-check pattern is used to reduce the cost and size of large messages by first storing the data in an external storage location and then sending a reference to the data/event to the consumer.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A[[Message with data]]--\u0026gt;|1.| B((Producer)) B --\u0026gt;|2. Store data, save key| C[(Storage)] D[[Smaller message with key only]] B --\u0026gt;|3.| D --\u0026gt;|4.| E((Consumer)) C --\u0026gt;|5. Get data with key| E --\u0026gt;|6."},{"id":104,"href":"/hub/Data-Architecture/Lambda-%E6%9E%B6%E6%9E%84/","title":"Lambda Architecture","parent":"数据架构","content":"Lambda architecture is a data processing pattern designed to strike a balance between low latency, high throughput, and fault tolerance. This architecture type uses a combination of batch processing to create accurate views of large data sets and real-time stream processing to provide views of live data. The results from both sets can then be merged and presented together.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A((Data Source)) subgraph Batch Layer B(\u0026#34;Batch view(s)\u0026#34;) end subgraph Speed Layer C(\u0026#34;Real-time view(s)\u0026#34;) end A --\u0026gt; B A --\u0026gt; C subgraph Serving Layer D(\u0026#34;Combined view(s)\u0026#34;) end B --\u0026gt; D C --\u0026gt; D Lambda Architecture Advantages Efficiently serves batch and real-time workloads Lambda Architecture Disadvantages Duplicated code/logic for both batch and real-time views Lambda Architecture Learning Resources http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html ","description":"Lambda architecture is a data processing pattern designed to strike a balance between low latency, high throughput, and fault tolerance. This architecture type uses a combination of batch processing to create accurate views of large data sets and real-time stream processing to provide views of live data. The results from both sets can then be merged and presented together.\n%%{init: { \u0026#34;flowchart\u0026#34;: { \u0026#34;useMaxWidth\u0026#34;: true } } }%% graph LR A((Data Source)) subgraph Batch Layer B(\u0026#34;Batch view(s)\u0026#34;) end subgraph Speed Layer C(\u0026#34;Real-time view(s)\u0026#34;) end A --\u0026gt; B A --\u0026gt; C subgraph Serving Layer D(\u0026#34;Combined view(s)\u0026#34;) end B --\u0026gt; D C --\u0026gt; D Lambda Architecture Advantages Efficiently serves batch and real-time workloads Lambda Architecture Disadvantages Duplicated code/logic for both batch and real-time views Lambda Architecture Learning Resources http://nathanmarz."},{"id":105,"href":"/hub/Data-Storage/NOSQL-DB/graph-database/Graph-Database/","title":"Graph Database","parent":"图数据库","content":"A graph database is a type of [[Non-relational Database|NoSQL]] database that uses nodes, edges, and properties to store data about entities and the relationships between them. The main purpose of a graph database is to allow for efficiently traversing the network of nodes and edges, and for analyzing the relationships between entities.\n![[graph_database_example.png|500]]\nPopular Graph Databases Neo4j ArangoDB OrientDB Azure Cosmos DB Graph API Graph Database Use Cases Social networks Fraud detection Anti-money laundering Machine Learning ","description":"A graph database is a type of [[Non-relational Database|NoSQL]] database that uses nodes, edges, and properties to store data about entities and the relationships between them. The main purpose of a graph database is to allow for efficiently traversing the network of nodes and edges, and for analyzing the relationships between entities.\n![[graph_database_example.png|500]]\nPopular Graph Databases Neo4j ArangoDB OrientDB Azure Cosmos DB Graph API Graph Database Use Cases Social networks Fraud detection Anti-money laundering Machine Learning "},{"id":106,"href":"/hub/Data-Storage/NOSQL-DB/column-database/Column-oriented-Database/","title":"Column-oriented Database","parent":"列存储数据库","content":"![[Assets/row_oriented_vs_column_oriented_database.jpeg|500]]\nIn a column-oriented or columnar database, the data for each column in a datatable is stored together. Because of their characteristics, they are a popular option for building a [[Data Warehouse|data warehouse]].\nColumn-oriented Database Example In a datatable like this:\nEmpId Lastname FirstnameSalary 10 Smith Joe 12 Jones Mary 11 Johnson Cathy 22 Jones Bob The data would be stored like this (simplified example):\n10,12,11,22; Smith,Jones,Johnson,Jones; Joe,Mary,Cathy,Bob; 60000,80000,94000,55000; Column-oriented Database Advantages More efficient querying when querying a subset of columns because the database doesn\u0026rsquo;t need to read columns that aren\u0026rsquo;t relevant. Data can be compressed further which translates into storage and query improvements. Column-oriented Database Disadvantages Typically less efficient when inserting data. When to use a column-oriented database When you tend to only query a subset of columns in your data. When you often run analytical queries or [[Online Analytical Processing|OLAP]] workloads such as metrics and aggregations. Column-oriented Database Use Cases Reporting Big Data Analytics Business Intelligence ","description":"![[Assets/row_oriented_vs_column_oriented_database.jpeg|500]]\nIn a column-oriented or columnar database, the data for each column in a datatable is stored together. Because of their characteristics, they are a popular option for building a [[Data Warehouse|data warehouse]].\nColumn-oriented Database Example In a datatable like this:\nEmpId Lastname FirstnameSalary 10 Smith Joe 12 Jones Mary 11 Johnson Cathy 22 Jones Bob The data would be stored like this (simplified example):\n10,12,11,22; Smith,Jones,Johnson,Jones; Joe,Mary,Cathy,Bob; 60000,80000,94000,55000; Column-oriented Database Advantages More efficient querying when querying a subset of columns because the database doesn\u0026rsquo;t need to read columns that aren\u0026rsquo;t relevant."},{"id":107,"href":"/hub/Data-Warehouse/%E6%95%B0%E6%8D%AE%E6%B9%96/","title":"数据湖","parent":"数据仓库","content":" A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics - from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\nAWS, What is a data lake? ","description":" A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics - from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\nAWS, What is a data lake? "},{"id":108,"href":"/hub/Data-Architecture/%E4%BA%91%E5%8E%82%E5%95%86%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84/","title":"云厂商数据架构","parent":"数据架构","content":" Data Architecture Examples AWS Reference Architecture Examples Azure Architecture Examples GCP Architecture Center ","description":" Data Architecture Examples AWS Reference Architecture Examples Azure Architecture Examples GCP Architecture Center "},{"id":109,"href":"/hub/Data-Warehouse/%E5%A2%9E%E9%87%8F%E6%9B%B4%E6%96%B0/","title":"增量更新","parent":"数据仓库","content":"A delta load refers to extracting only the data that has changed since the last time the extract process has run. The most commonly used steps to perform a delta load are:\nEnsure there is a modified_at timestamp or incremental id column such as a primary key on the data source. On the initial run of the pipeline, do a full load of the dataset. On following runs of the pipeline, query the target dataset using MAX(column_name). Query the source dataset and filter records where values are greater than the value from step 3. Delta Load Advantages More resource efficient Easy to implement and maintain Only requires read permissions to perform Delta Load Disadvantages Does not capture deleted records Requires extra metadata on the source (commonly a unique id or updated timestamp) Does not capture multiple changes between the polling interval. If a row changes multiple times, you may only capture the latest state. Querying the database for changes may impact the database performance. ","description":"A delta load refers to extracting only the data that has changed since the last time the extract process has run. The most commonly used steps to perform a delta load are:\nEnsure there is a modified_at timestamp or incremental id column such as a primary key on the data source. On the initial run of the pipeline, do a full load of the dataset. On following runs of the pipeline, query the target dataset using MAX(column_name)."},{"id":110,"href":"/hub/Data-Governance/data-compliance/%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/","title":"数据传输","parent":"数据合规","content":"","description":""},{"id":111,"href":"/hub/Data-Governance/data-compliance/%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86/","title":"数据加密","parent":"数据合规","content":"","description":""},{"id":112,"href":"/hub/Data-Governance/data-compliance/%E6%95%B0%E6%8D%AE%E5%AE%B9%E7%81%BE/","title":"数据容灾","parent":"数据合规","content":"","description":""},{"id":113,"href":"/hub/Data-Governance/data-compliance/%E6%95%B0%E6%8D%AE%E7%9B%91%E6%8E%A7/","title":"数据监控","parent":"数据合规","content":"","description":""},{"id":114,"href":"/hub/Data-Pipeline/apache_airflow/","title":"Apache Airflow","parent":"数据管道","content":"Apache Airflow™ 是一个开源平台，用于开发、调度和监控面向批处理的工作流。Airflow 的可扩展 Python 框架使您能够构建工作流与几乎任何技术连接。Web 界面有助于管理工作流的状态。Airflow 可以通过多种方式进行部署，从笔记本电脑上的单个进程到分布式设置，甚至支持最大的工作流程。\nAirflow 的理念是将工作流定义为代码, Workflows as code\nAirflow 工作流的主要特征是所有工作流都是在 Python 代码中定义的。“工作流即代码”有多种用途：\nDynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. 动态：Airflow 管道配置为 Python 代码，允许动态生成管道。\n可扩展：Airflow™ 框架包含可连接多种技术的操作符。所有 Airflow 组件都具有可扩展性，可轻松适应您的环境。\n灵活：利用 Jinja 模板引擎，内置工作流程参数化功能。\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG(dag_id=\u0026#34;demo\u0026#34;, start_date=datetime(2022, 1, 1), schedule=\u0026#34;0 0 * * *\u0026#34;) as dag: # Tasks are represented as operators hello = BashOperator(task_id=\u0026#34;hello\u0026#34;, bash_command=\u0026#34;echo hello\u0026#34;) @task() def airflow(): print(\u0026#34;airflow\u0026#34;) # Set dependencies between tasks hello \u0026gt;\u0026gt; airflow() 如果你喜欢编码而不是点击，那么 Airflow 就是你的理想工具。工作流被定义为 Python 代码，这意味着\n工作流程可存储在版本控制中，以便回滚到以前的版本\n多人可同时开发工作流程\n可编写测试来验证功能\n组件具有可扩展性，您可以在大量现有组件的基础上进行构建\n","description":"Apache Airflow™ 是一个开源平台，用于开发、调度和监控面向批处理的工作流。Airflow 的可扩展 Python 框架使您能够构建工作流与几乎任何技术连接。Web 界面有助于管理工作流的状态。Airflow 可以通过多种方式进行部署，从笔记本电脑上的单个进程到分布式设置，甚至支持最大的工作流程。\nAirflow 的理念是将工作流定义为代码, Workflows as code\nAirflow 工作流的主要特征是所有工作流都是在 Python 代码中定义的。“工作流即代码”有多种用途：\nDynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. 动态：Airflow 管道配置为 Python 代码，允许动态生成管道。\n可扩展：Airflow™ 框架包含可连接多种技术的操作符。所有 Airflow 组件都具有可扩展性，可轻松适应您的环境。"},{"id":115,"href":"/hub/Data-on-Cloud/AWS/","title":"AWS","parent":"数据上云","content":"","description":""},{"id":116,"href":"/hub/Data-on-Cloud/Azure/","title":"Azure","parent":"数据上云","content":"","description":""},{"id":117,"href":"/hub/Data-Process/Flink-on-Yarn-Startup-procedure/","title":"Flink on yarn startup procedure","parent":"数据处理","content":" graph TB A[用户提交Flink应用程序到YARN集群] B[YARN ResourceManager分配ApplicationMaster] C[ApplicationMaster请求资源] D[ResourceManager分配资源给ApplicationMaster] E[ApplicationMaster启动JobManager和TaskManager] F[JobManager接收作业提交请求、调度任务执行和监控作业执行状态] G[TaskManager执行具体任务并返回执行结果给JobManager] H[Flink作业执行完毕，释放资源] I[ApplicationMaster通知ResourceManager释放资源] A --\u0026gt; B B --\u0026gt; C C --\u0026gt; D D --\u0026gt; E E --\u0026gt; F F --\u0026gt; G G --\u0026gt; H H --\u0026gt; I style A,B,C,D,E,F,G,H,I fill:#f9f,stroke:#333,stroke-width:2px ","description":" graph TB A[用户提交Flink应用程序到YARN集群] B[YARN ResourceManager分配ApplicationMaster] C[ApplicationMaster请求资源] D[ResourceManager分配资源给ApplicationMaster] E[ApplicationMaster启动JobManager和TaskManager] F[JobManager接收作业提交请求、调度任务执行和监控作业执行状态] G[TaskManager执行具体任务并返回执行结果给JobManager] H[Flink作业执行完毕，释放资源] I[ApplicationMaster通知ResourceManager释放资源] A --\u0026gt; B B --\u0026gt; C C --\u0026gt; D D --\u0026gt; E E --\u0026gt; F F --\u0026gt; G G --\u0026gt; H H --\u0026gt; I style A,B,C,D,E,F,G,H,I fill:#f9f,stroke:#333,stroke-width:2px "},{"id":118,"href":"/hub/Data-Process/Flink-wordcount/","title":"flink wordcount","parent":"数据处理","content":"import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; public class WindowWordCount { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; dataStream = env .socketTextStream(\u0026#34;localhost\u0026#34;, 9999) .flatMap(new Splitter()) .keyBy(value -\u0026gt; value.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1); dataStream.print(); env.execute(\u0026#34;Window WordCount\u0026#34;); } public static class Splitter implements FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String sentence, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) throws Exception { for (String word: sentence.split(\u0026#34; \u0026#34;)) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(word, 1)); } } } } ","description":"import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; public class WindowWordCount { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; dataStream = env .socketTextStream(\u0026#34;localhost\u0026#34;, 9999) .flatMap(new Splitter()) .keyBy(value -\u0026gt; value.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1); dataStream.print(); env.execute(\u0026#34;Window WordCount\u0026#34;); } public static class Splitter implements FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String sentence, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) throws Exception { for (String word: sentence.split(\u0026#34; \u0026#34;)) { out."},{"id":119,"href":"/hub/Data-on-Cloud/GCP/","title":"Google Cloud Platform","parent":"数据上云","content":"","description":""},{"id":120,"href":"/hub/Fundamentals/languages/sql/mysql%E9%AB%98%E9%98%B6%E6%8A%80%E5%B7%A7/","title":"mysql的高阶技巧","parent":"SQL","content":"MySQL 中有许多常用的高阶 SQL 写法，以下列举了一些常见的高级 SQL 技巧和语法：\n窗口函数（Window Functions）：使用 OVER 子句来执行聚合、排序等操作，如 ROW_NUMBER(), RANK(), LEAD(), LAG() 等。\n公共表表达式（Common Table Expressions，CTE）：使用 WITH 关键字创建临时结果集，可以提高查询的可读性和可维护性。\n联结（Join）：除了常见的 INNER JOIN、LEFT JOIN、RIGHT JOIN 外，还可以使用 CROSS JOIN、SELF JOIN 等。\n子查询（Subquery）：在 SELECT、FROM、WHERE 等子句中嵌套查询，用于实现复杂的逻辑。\n条件聚合（Conditional Aggregation）：使用 CASE 表达式在聚合函数中进行条件判断，如 SUM(CASE WHEN condition THEN column END)。\n动态SQL（Dynamic SQL）：使用 MySQL 变量和字符串拼接等技巧动态生成 SQL 语句，实现灵活的查询逻辑。\n分析函数（Analytic Functions）：类似于窗口函数，但可以在结果集中进行更复杂的分析，如 RANK() OVER (PARTITION BY column ORDER BY column)。\n递归查询（Recursive Query）：使用 WITH RECURSIVE 关键字实现递归查询，处理具有递归结构的数据。\n数据透视表（Pivot Table）：使用 CASE 表达式和聚合函数将行数据转换为列数据，实现数据透视的效果。\n多表更新（Multi-table Update）：使用 UPDATE 语句联结多个表进行更新操作，可以一次性更新多个表中的数据。\n","description":"MySQL 中有许多常用的高阶 SQL 写法，以下列举了一些常见的高级 SQL 技巧和语法：\n窗口函数（Window Functions）：使用 OVER 子句来执行聚合、排序等操作，如 ROW_NUMBER(), RANK(), LEAD(), LAG() 等。\n公共表表达式（Common Table Expressions，CTE）：使用 WITH 关键字创建临时结果集，可以提高查询的可读性和可维护性。\n联结（Join）：除了常见的 INNER JOIN、LEFT JOIN、RIGHT JOIN 外，还可以使用 CROSS JOIN、SELF JOIN 等。\n子查询（Subquery）：在 SELECT、FROM、WHERE 等子句中嵌套查询，用于实现复杂的逻辑。\n条件聚合（Conditional Aggregation）：使用 CASE 表达式在聚合函数中进行条件判断，如 SUM(CASE WHEN condition THEN column END)。\n动态SQL（Dynamic SQL）：使用 MySQL 变量和字符串拼接等技巧动态生成 SQL 语句，实现灵活的查询逻辑。\n分析函数（Analytic Functions）：类似于窗口函数，但可以在结果集中进行更复杂的分析，如 RANK() OVER (PARTITION BY column ORDER BY column)。\n递归查询（Recursive Query）：使用 WITH RECURSIVE 关键字实现递归查询，处理具有递归结构的数据。\n数据透视表（Pivot Table）：使用 CASE 表达式和聚合函数将行数据转换为列数据，实现数据透视的效果。\n多表更新（Multi-table Update）：使用 UPDATE 语句联结多个表进行更新操作，可以一次性更新多个表中的数据。"},{"id":121,"href":"/hub/Fundamentals/languages/sql/sql%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"mysql的高阶技巧","parent":"SQL","content":" 6个SQL查询小技巧 1、行列转换\ncreate table if not exists tb1( name varchar(255), age int, gender varchar(255) )\ninsert tb1 value(\u0026ldquo;Jone\u0026rdquo;,10,\u0026ldquo;male\u0026rdquo;); insert tb1 value(\u0026ldquo;kate\u0026rdquo;,20,\u0026ldquo;female\u0026rdquo;); insert tb1 value(\u0026ldquo;mike\u0026rdquo;,30,\u0026ldquo;male\u0026rdquo;);\nselect col1 from ( select case when rowid = 1 then col1 else null end as col1 , case when rowid = 2 then col1 else null end as col2 , case when rowid = 3 then col1 else null end as col3 from ( select name as col1 , row_number() over (order by name) as rowid from tb1 union all select age as col1,row_number() over (order by name) as rowid from tb1 union all select gender as col1, row_number() over (order by name) as rowid from tb1 ) as temp ) as temp2 where col1 is not null\nselect * from tmp;\n","description":"6个SQL查询小技巧 1、行列转换\ncreate table if not exists tb1( name varchar(255), age int, gender varchar(255) )\ninsert tb1 value(\u0026ldquo;Jone\u0026rdquo;,10,\u0026ldquo;male\u0026rdquo;); insert tb1 value(\u0026ldquo;kate\u0026rdquo;,20,\u0026ldquo;female\u0026rdquo;); insert tb1 value(\u0026ldquo;mike\u0026rdquo;,30,\u0026ldquo;male\u0026rdquo;);\nselect col1 from ( select case when rowid = 1 then col1 else null end as col1 , case when rowid = 2 then col1 else null end as col2 , case when rowid = 3 then col1 else null end as col3 from ( select name as col1 , row_number() over (order by name) as rowid from tb1 union all select age as col1,row_number() over (order by name) as rowid from tb1 union all select gender as col1, row_number() over (order by name) as rowid from tb1 ) as temp ) as temp2 where col1 is not null"},{"id":122,"href":"/hub/Fundamentals/languages/sql/","title":"SQL","parent":"编程语言","content":"SQL（Structured Query Language）是用于管理关系型数据库的标准化语言。它的发展历史可以简要总结如下：\n1970s：SQL最早由IBM的Edgar Codd提出，作为关系数据库管理系统（RDBMS）的查询语言。 1980s：ANSI（美国国家标准协会）和ISO（国际标准化组织）开始制定SQL的标准，确立了SQL的基本结构和语法。 1990s：SQL成为关系数据库管理系统的事实标准，各大数据库厂商开始支持SQL标准并进行扩展。 2000s以后：SQL标准逐渐得到统一，出现了越来越多的SQL数据库产品和工具。 现在SQL主要遵循的标准是SQL-92、SQL:1999、SQL:2003、SQL:2008和SQL:2011。这些标准规定了SQL的语法、数据类型、查询语句、事务控制等方面的规范，使得不同的数据库系统可以遵循相同的语言标准进行开发和应用。\nSQL能够长盛不衰的原因在于：\n简单易学：SQL的语法简单直观，容易学习和使用，适合各种技术背景的人员。 标准化：SQL是一种标准化语言，有着统一的语法和规范，使得不同数据库系统之间可以进行交互和迁移。 功能强大：SQL提供了丰富的数据操作和查询功能，支持复杂的数据处理需求。 广泛应用：SQL被广泛应用于各种类型的应用程序和系统中，如企业管理系统、数据分析、报表生成等领域。 持续发展：SQL标准不断更新和完善，不断适应新的数据处理需求和技术发展趋势，保持了其长盛不衰的地位。 mysql的高阶技巧 mysql的高阶技巧 SQL调优技巧 隔离级别和事务 ","description":"SQL（Structured Query Language）是用于管理关系型数据库的标准化语言。它的发展历史可以简要总结如下：\n1970s：SQL最早由IBM的Edgar Codd提出，作为关系数据库管理系统（RDBMS）的查询语言。 1980s：ANSI（美国国家标准协会）和ISO（国际标准化组织）开始制定SQL的标准，确立了SQL的基本结构和语法。 1990s：SQL成为关系数据库管理系统的事实标准，各大数据库厂商开始支持SQL标准并进行扩展。 2000s以后：SQL标准逐渐得到统一，出现了越来越多的SQL数据库产品和工具。 现在SQL主要遵循的标准是SQL-92、SQL:1999、SQL:2003、SQL:2008和SQL:2011。这些标准规定了SQL的语法、数据类型、查询语句、事务控制等方面的规范，使得不同的数据库系统可以遵循相同的语言标准进行开发和应用。\nSQL能够长盛不衰的原因在于：\n简单易学：SQL的语法简单直观，容易学习和使用，适合各种技术背景的人员。 标准化：SQL是一种标准化语言，有着统一的语法和规范，使得不同数据库系统之间可以进行交互和迁移。 功能强大：SQL提供了丰富的数据操作和查询功能，支持复杂的数据处理需求。 广泛应用：SQL被广泛应用于各种类型的应用程序和系统中，如企业管理系统、数据分析、报表生成等领域。 持续发展：SQL标准不断更新和完善，不断适应新的数据处理需求和技术发展趋势，保持了其长盛不衰的地位。 mysql的高阶技巧 mysql的高阶技巧 SQL调优技巧 隔离级别和事务 "},{"id":123,"href":"/hub/Data-Storage/SQL-DB/","title":"SQL数据库","parent":"数据存储","content":" 事务性数据库 Amazon RDS MySQL PostgreSQL Microsoft SQL Server 分析型数据库 Azure Synapse Analytics Amazon Redshift Google BigQuery ","description":" 事务性数据库 Amazon RDS MySQL PostgreSQL Microsoft SQL Server 分析型数据库 Azure Synapse Analytics Amazon Redshift Google BigQuery "},{"id":124,"href":"/hub/Fundamentals/languages/sql/SQL%E8%B0%83%E4%BC%98%E6%8A%80%E5%B7%A7/","title":"SQL调优技巧","parent":"SQL","content":"当涉及到 SQL 查询优化时，有许多技巧和策略可以帮助提高查询性能。以下是一些常用的 SQL 调优技巧：\n添加缺失的索引：分析查询中的字段，确保为经常用于过滤、连接或排序的列添加适当的索引。\n检查未使用的索引：定期审查数据库中的索引，删除不再使用的或无效的索引。\n避免在 FILTER 子句中使用多个 OR 条件：多个 OR 条件可能导致数据库放弃索引而进行全表扫描。\n仅在短语末尾使用通配符：通配符（如 %）应该仅在查询的短语末尾使用，以避免索引失效。\n减少 JOIN 操作：避免过多的 JOIN 操作，优先选择 INNER JOIN。\n避免使用 SELECT DISTINCT：只返回需要的字段，避免使用 DISTINCT。\n使用 SELECT 字段而不是 SELECT * .：明确指定要返回的字段，而不是返回所有字段。\n使用 LIMIT 或 TOP 来限制结果集大小：只返回所需的前几行数据。\n在非高峰时段运行查询：避免在数据库繁忙的时间运行查询。\n最小化查询提示的使用：避免过多的查询提示，如 FORCE INDEX。\n减少大型写操作：批量插入或更新数据时，尽量减少大型写操作。\n使用 INNER JOIN 而不是 WHERE 进行连接：INNER JOIN 更清晰且更容易优化。\n总之，理解 SQL 执行顺序、合理使用索引、避免全表扫描以及减少无效数据的查询是 SQL 查询优化的关键。如果你有其他问题或需要更详细的解释，请随时告知！🌟\n","description":"当涉及到 SQL 查询优化时，有许多技巧和策略可以帮助提高查询性能。以下是一些常用的 SQL 调优技巧：\n添加缺失的索引：分析查询中的字段，确保为经常用于过滤、连接或排序的列添加适当的索引。\n检查未使用的索引：定期审查数据库中的索引，删除不再使用的或无效的索引。\n避免在 FILTER 子句中使用多个 OR 条件：多个 OR 条件可能导致数据库放弃索引而进行全表扫描。\n仅在短语末尾使用通配符：通配符（如 %）应该仅在查询的短语末尾使用，以避免索引失效。\n减少 JOIN 操作：避免过多的 JOIN 操作，优先选择 INNER JOIN。\n避免使用 SELECT DISTINCT：只返回需要的字段，避免使用 DISTINCT。\n使用 SELECT 字段而不是 SELECT * .：明确指定要返回的字段，而不是返回所有字段。\n使用 LIMIT 或 TOP 来限制结果集大小：只返回所需的前几行数据。\n在非高峰时段运行查询：避免在数据库繁忙的时间运行查询。\n最小化查询提示的使用：避免过多的查询提示，如 FORCE INDEX。\n减少大型写操作：批量插入或更新数据时，尽量减少大型写操作。\n使用 INNER JOIN 而不是 WHERE 进行连接：INNER JOIN 更清晰且更容易优化。\n总之，理解 SQL 执行顺序、合理使用索引、避免全表扫描以及减少无效数据的查询是 SQL 查询优化的关键。如果你有其他问题或需要更详细的解释，请随时告知！🌟"},{"id":125,"href":"/tags/","title":"Tags","parent":"数据工程知识库","content":"","description":""},{"id":126,"href":"/hub/Data-Process/test/","title":"test mermaid","parent":"数据处理","content":" graph TB subgraph 16th x[Spain] y[Portugue] end subgraph 17th a[Nertherland] b[England] end subgraph 18th c[England] d[France] end subgraph 19th e[Germany] f[Japan] end subgraph 20th g[United States] h[Russia] end subgraph 21th i[China] j[India] end x --\u0026gt; y y --\u0026gt; a a --\u0026gt; b b --\u0026gt; c c --\u0026gt; d d --\u0026gt; e e --\u0026gt; f f --\u0026gt; g g --\u0026gt; h h --\u0026gt; i i --\u0026gt; j ","description":" graph TB subgraph 16th x[Spain] y[Portugue] end subgraph 17th a[Nertherland] b[England] end subgraph 18th c[England] d[France] end subgraph 19th e[Germany] f[Japan] end subgraph 20th g[United States] h[Russia] end subgraph 21th i[China] j[India] end x --\u0026gt; y y --\u0026gt; a a --\u0026gt; b b --\u0026gt; c c --\u0026gt; d d --\u0026gt; e e --\u0026gt; f f --\u0026gt; g g --\u0026gt; h h --\u0026gt; i i --\u0026gt; j "},{"id":127,"href":"/hub/Data-Storage/SQL-DB/OLTP/","title":"事务性数据库","parent":"SQL数据库","content":"","description":""},{"id":128,"href":"/hub/Data-Storage/NOSQL-DB/column-database/","title":"列存储数据库","parent":"NOSQL数据库","content":"","description":""},{"id":129,"href":"/hub/Data-Storage/NOSQL-DB/graph-database/","title":"图数据库","parent":"NOSQL数据库","content":"","description":""},{"id":130,"href":"/hub/Fundamentals/","title":"基础知识","parent":"数据工程知识库","content":" 编程语言 SQL mysql的高阶技巧 mysql的高阶技巧 SQL调优技巧 隔离级别和事务 Python Python相关工具 Java Java相关工具 Scala Scala相关工具 ","description":" 编程语言 SQL mysql的高阶技巧 mysql的高阶技巧 SQL调优技巧 隔离级别和事务 Python Python相关工具 Java Java相关工具 Scala Scala相关工具 "},{"id":131,"href":"/hub/Data-Governance/data-compliance/","title":"数据合规","parent":"数据治理","content":" 数据传输 数据加密 数据容灾 数据监控 ","description":" 数据传输 数据加密 数据容灾 数据监控 "},{"id":132,"href":"/","title":"数据工程知识库","parent":"","content":" Welcome！ 在这里，你可以一站式学习到数据工程师需要掌握的知识集合，从数据流的角度来书，数据从被创造出来一直到产生价值，其中包括数据摄取，数据存储，数据处理，数据可视化等环节，而在数据存储环节则需要利用到数据仓库设计，在数据摄取到计算环节涉及到数据管道和数据治理，通过数据挖掘可以释放数据的商业价值，所以数据工程师的知识图谱可以通过数据流和方法论这两条主线来构建。 如果喜欢的话，也可以通过 GitHub 贡献自己的知识哦。 ⭐ GitHub地址\n基础知识 介绍了与数据工程相关的常用编程语言和基本概念。\n数据摄取 数据摄取，或者是数据集成，是数据平台的第一步，无论在开源领域还是云服务商，都提供了针对不同情况下的数据集成服务。\n数据存储 介绍了关系型数据库，NOSQL数据库， 分布式文件系统和对象存储系统,其中数据库是该领域的重点。\n数据处理 数据计算对应到建设数据仓库中的各种ETL操作，这里重点介绍开源的Spark， Flink和常见的云端数据计算服务。\n数据可视化 数据分析是和数据平台的商业价值息息相关，此处重点关注的是数据分析和商业价值的关系，如何利用商业价值驱动整个数据平台的建设。\n数据管道 数据管道是一个工作流程（Workflow），代表了不同的数据工程流程和工具如何协同工作，以实现将数据从源传输到目标存储系统。\n数据仓库 数据仓库是一个战略性的数据集合，用于支持企业各级决策制定过程。它是一个单一的数据存储，专为分析性报告和决策支持而创建。\n数据治理 数据治理是个综合的领域，包含数据质量管理，元数据管理，数据合规性管理，数据安全管理，数据权限控制，数据血缘，数据标准等。\n数据架构 数据架构关注以下领域，系统的高性能，系统的高可用性，系统处理及时性，数据的可回溯性，系统的可观测性等。\n数据挖掘 数据挖掘，介绍几种常见的机器学习算法，比如聚类，分类，回归，模式匹配等。\n数据上云 本章介绍如何在AWS、阿里云等公有云上建设云端数据平台的，重点介绍在云端数据平台上所涉及到的相关服务。\n","description":"Welcome！ 在这里，你可以一站式学习到数据工程师需要掌握的知识集合，从数据流的角度来书，数据从被创造出来一直到产生价值，其中包括数据摄取，数据存储，数据处理，数据可视化等环节，而在数据存储环节则需要利用到数据仓库设计，在数据摄取到计算环节涉及到数据管道和数据治理，通过数据挖掘可以释放数据的商业价值，所以数据工程师的知识图谱可以通过数据流和方法论这两条主线来构建。 如果喜欢的话，也可以通过 GitHub 贡献自己的知识哦。 ⭐ GitHub地址\n基础知识 介绍了与数据工程相关的常用编程语言和基本概念。\n数据摄取 数据摄取，或者是数据集成，是数据平台的第一步，无论在开源领域还是云服务商，都提供了针对不同情况下的数据集成服务。\n数据存储 介绍了关系型数据库，NOSQL数据库， 分布式文件系统和对象存储系统,其中数据库是该领域的重点。\n数据处理 数据计算对应到建设数据仓库中的各种ETL操作，这里重点介绍开源的Spark， Flink和常见的云端数据计算服务。\n数据可视化 数据分析是和数据平台的商业价值息息相关，此处重点关注的是数据分析和商业价值的关系，如何利用商业价值驱动整个数据平台的建设。\n数据管道 数据管道是一个工作流程（Workflow），代表了不同的数据工程流程和工具如何协同工作，以实现将数据从源传输到目标存储系统。\n数据仓库 数据仓库是一个战略性的数据集合，用于支持企业各级决策制定过程。它是一个单一的数据存储，专为分析性报告和决策支持而创建。\n数据治理 数据治理是个综合的领域，包含数据质量管理，元数据管理，数据合规性管理，数据安全管理，数据权限控制，数据血缘，数据标准等。\n数据架构 数据架构关注以下领域，系统的高性能，系统的高可用性，系统处理及时性，数据的可回溯性，系统的可观测性等。\n数据挖掘 数据挖掘，介绍几种常见的机器学习算法，比如聚类，分类，回归，模式匹配等。\n数据上云 本章介绍如何在AWS、阿里云等公有云上建设云端数据平台的，重点介绍在云端数据平台上所涉及到的相关服务。"},{"id":133,"href":"/hub/_Index/","title":"数据工程知识库","parent":"数据工程知识库","content":" 基础知识 编程语言 SQL mysql的高阶技巧 mysql的高阶技巧 SQL调优技巧 隔离级别和事务 Python Python相关工具 Java Java相关工具 Scala Scala相关工具 数据存储 SQL数据库 事务性数据库 Amazon RDS MySQL PostgreSQL Microsoft SQL Server 分析型数据库 Azure Synapse Analytics Amazon Redshift Google BigQuery 文件格式 Avro Apache ORC Apache Parquet JSON yaml格式 CSV Delta Lake Protocol Buffers NOSQL数据库 列存储数据库 hbase Couchbase Column-oriented Database 图数据库 Graph Database 文档型数据库 MongoDB elasticsearch Amazon DynamoDB 其他 ClickHouse Timeseries Database 键值对数据库 Redis 分布式文件存储 分布式文件存储 对象存储 Amazon S3 Glacier Amazon S3 数据处理 Flink on yarn startup procedure flink wordcount test mermaid Spark Join的类型和使用 spark概念和架构 spark面试题 spark shuffle 详解 spark structured streaming案例 Flink案例 数据摄取 Apache Kafka详解 Consume all messages Kafka Connect示例 Flink CDC Flink CDC结合Debezium Data Build Tool 数据挖掘 逻辑回归案例 K_Means案例 用户画像 数据可视化 Power BI Qlikview Kibana Grafana Prometheus Prometheus和Grafana的集成 Apache Superset 数据管道 Apache Airflow 数据治理 数据合规 数据传输 数据加密 数据容灾 数据监控 数据血缘 数据质量 Soda Great Expectations Monte Carlo Deequ Choosing your optimal messaging service Cost Optimization in the Cloud SQL Guide Testing Your Data Pipeline Guides Data Governance Guide Getting Started With Data Engineering Data Pipeline Best Practices 数据仓库 什么是数据仓库 数据模型 数仓建模的方式 数据仓库分层设计 维度建模 事实表的设计 缓慢变化维 电商数据仓库设计 实时数仓架构 数据湖 增量更新 数据上云 AWS AWS DynamoDB Amazon MSK Benthos Amazon Web Services Azure Microsoft Azure Google Cloud Platform Google Cloud Platform Aliyun 数据架构 CAP不可能三角原则 Kappa架构 Fan-out Claim Check Pattern Lambda Architecture 云厂商数据架构 草稿 问题集 ","description":" 基础知识 编程语言 SQL mysql的高阶技巧 mysql的高阶技巧 SQL调优技巧 隔离级别和事务 Python Python相关工具 Java Java相关工具 Scala Scala相关工具 数据存储 SQL数据库 事务性数据库 Amazon RDS MySQL PostgreSQL Microsoft SQL Server 分析型数据库 Azure Synapse Analytics Amazon Redshift Google BigQuery 文件格式 Avro Apache ORC Apache Parquet JSON yaml格式 CSV Delta Lake Protocol Buffers NOSQL数据库 列存储数据库 hbase Couchbase Column-oriented Database 图数据库 Graph Database 文档型数据库 MongoDB elasticsearch Amazon DynamoDB 其他 ClickHouse Timeseries Database 键值对数据库 Redis 分布式文件存储 分布式文件存储 对象存储 Amazon S3 Glacier Amazon S3 数据处理 Flink on yarn startup procedure flink wordcount test mermaid Spark Join的类型和使用 spark概念和架构 spark面试题 spark shuffle 详解 spark structured streaming案例 Flink案例 数据摄取 Apache Kafka详解 Consume all messages Kafka Connect示例 Flink CDC Flink CDC结合Debezium Data Build Tool 数据挖掘 逻辑回归案例 K_Means案例 用户画像 数据可视化 Power BI Qlikview Kibana Grafana Prometheus Prometheus和Grafana的集成 Apache Superset 数据管道 Apache Airflow 数据治理 数据合规 数据传输 数据加密 数据容灾 数据监控 数据血缘 数据质量 Soda Great Expectations Monte Carlo Deequ Choosing your optimal messaging service Cost Optimization in the Cloud SQL Guide Testing Your Data Pipeline Guides Data Governance Guide Getting Started With Data Engineering Data Pipeline Best Practices 数据仓库 什么是数据仓库 数据模型 数仓建模的方式 数据仓库分层设计 维度建模 事实表的设计 缓慢变化维 电商数据仓库设计 实时数仓架构 数据湖 增量更新 数据上云 AWS AWS DynamoDB Amazon MSK Benthos Amazon Web Services Azure Microsoft Azure Google Cloud Platform Google Cloud Platform Aliyun 数据架构 CAP不可能三角原则 Kappa架构 Fan-out Claim Check Pattern Lambda Architecture 云厂商数据架构 草稿 问题集 "},{"id":134,"href":"/hub/Data-Governance/data-linage/","title":"数据血缘","parent":"数据治理","content":" ","description":" "},{"id":135,"href":"/hub/Data-Governance/data-quality/","title":"数据质量","parent":"数据治理","content":" Soda Great Expectations Monte Carlo Deequ ","description":" Soda Great Expectations Monte Carlo Deequ "},{"id":136,"href":"/hub/Data-Storage/format/","title":"文件格式","parent":"数据存储","content":"在大数据领域，常用的数据存储格式包括但不限于以下几种：\nParquet：Parquet是一种列式存储格式，能够高效地压缩和存储数据。它支持高效的列裁剪、谓词下推等操作，适合用于大规模数据存储和分析。\nORC（Optimized Row Columnar）：ORC也是一种列式存储格式，设计用于Hadoop生态系统中的数据存储和处理。它支持高效的压缩和列式存储，适合用于数据仓库和数据分析场景。\nAvro：Avro是一种数据序列化格式，旨在提供一种紧凑、快速和可序列化的数据格式。它支持动态模式定义和数据交换，适合用于数据交换和数据通信。\nJSON（JavaScript Object Notation）：JSON是一种轻量级的数据交换格式，易于阅读和编写。在大数据领域，JSON常用于数据传输和交换，但相对于其他列式存储格式，它可能不够高效。\nSequenceFile：SequenceFile是Hadoop中的一种二进制文件格式，用于存储序列化的键值对数据。它适合用于Hadoop MapReduce任务中的中间数据存储和传输。\nDelta Lake：Delta Lake是一种开源的数据湖存储格式，构建在Apache Spark之上，支持事务性ACID操作。Delta Lake结合了数据湖和数据仓库的优点，适合用于大规模数据湖存储和分析。\n以上列举的数据存储格式在大数据领域中被广泛应用，选择合适的数据存储格式取决于具体的应用场景和需求。\nAvro Apache ORC Apache Parquet JSON yaml格式 CSV Delta Lake Protocol Buffers ","description":"在大数据领域，常用的数据存储格式包括但不限于以下几种：\nParquet：Parquet是一种列式存储格式，能够高效地压缩和存储数据。它支持高效的列裁剪、谓词下推等操作，适合用于大规模数据存储和分析。\nORC（Optimized Row Columnar）：ORC也是一种列式存储格式，设计用于Hadoop生态系统中的数据存储和处理。它支持高效的压缩和列式存储，适合用于数据仓库和数据分析场景。\nAvro：Avro是一种数据序列化格式，旨在提供一种紧凑、快速和可序列化的数据格式。它支持动态模式定义和数据交换，适合用于数据交换和数据通信。\nJSON（JavaScript Object Notation）：JSON是一种轻量级的数据交换格式，易于阅读和编写。在大数据领域，JSON常用于数据传输和交换，但相对于其他列式存储格式，它可能不够高效。\nSequenceFile：SequenceFile是Hadoop中的一种二进制文件格式，用于存储序列化的键值对数据。它适合用于Hadoop MapReduce任务中的中间数据存储和传输。\nDelta Lake：Delta Lake是一种开源的数据湖存储格式，构建在Apache Spark之上，支持事务性ACID操作。Delta Lake结合了数据湖和数据仓库的优点，适合用于大规模数据湖存储和分析。\n以上列举的数据存储格式在大数据领域中被广泛应用，选择合适的数据存储格式取决于具体的应用场景和需求。\nAvro Apache ORC Apache Parquet JSON yaml格式 CSV Delta Lake Protocol Buffers "},{"id":137,"href":"/hub/Data-Storage/NOSQL-DB/document-database/","title":"文档型数据库","parent":"NOSQL数据库","content":"","description":""},{"id":138,"href":"/hub/Fundamentals/languages/","title":"编程语言","parent":"基础知识","content":" SQL mysql的高阶技巧 mysql的高阶技巧 SQL调优技巧 隔离级别和事务 Python Python相关工具 Java Java相关工具 Scala Scala相关工具 ","description":" SQL mysql的高阶技巧 mysql的高阶技巧 SQL调优技巧 隔离级别和事务 Python Python相关工具 Java Java相关工具 Scala Scala相关工具 "},{"id":139,"href":"/hub/Fundamentals/languages/sql/%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8C%E4%BA%8B%E5%8A%A1/","title":"隔离级别和事务","parent":"SQL","content":" 事务的并发问题和隔离级别 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。\n事务的四大特性(ACID) 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；\n一致性： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；\n隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；\n持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响\n事务的并发问题 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复度和幻读区别：\n不可重复读的重点是修改，针对的数据是多行。幻读的重点在于新增或者删除，针对数据是多行。\n事务的隔离级别 READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。\nMySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读），Oracle和sql server的默认隔离级别是READ-COMMITTED(读取已提交)。我们可以通过 SELECT @@tx_isolation;命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation;\nMySQL InnoDB 的 REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是 Next-Key Locks。\n因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是 InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读） 并不会有任何性能损失\n","description":"事务的并发问题和隔离级别 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。\n事务的四大特性(ACID) 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；\n一致性： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；\n隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；\n持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响\n事务的并发问题 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复度和幻读区别：\n不可重复读的重点是修改，针对的数据是多行。幻读的重点在于新增或者删除，针对数据是多行。\n事务的隔离级别 READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。\nMySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读），Oracle和sql server的默认隔离级别是READ-COMMITTED(读取已提交)。我们可以通过 SELECT @@tx_isolation;命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation;\nMySQL InnoDB 的 REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是 Next-Key Locks。\n因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是 InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读） 并不会有任何性能损失"}]